@inproceedings{10.1145/2790798.2790819,
author = {Gupta, Kunal and Sachdev, Astha and Sureka, Ashish},
title = {Pragamana: Performance Comparison and Programming Alpha-Miner Algorithm in Relational Database Query Language and NoSQL Column-Oriented Using Apache Phoenix},
year = {2015},
isbn = {9781450334198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2790798.2790819},
doi = {10.1145/2790798.2790819},
abstract = {Process-Aware Information Systems (PAIS) is an IT system that support business processes
and generate large amounts of event logs from the execution of business processes.
An event log is represented as a tuple of CaseID, Timestamp, Activity and Actor. Process
Mining is a new and emerging field that aims at analyzing the event logs to discover,
enhance and improve business processes and check conformance between run time and
design time business processes. The large volume of event logs generated are stored
in the databases. Relational databases perform well for a certain class of applications.
However, there are a certain class of applications for which relational databases
are not able to scale. To handle such class of applications, NoSQL database systems
emerged. Discovering a process model (workflow model) from event logs is one of the
most challenging and important Process Mining task. The α-miner algorithm is one of
the first and most widely used Process Discovery technique. Our objective is to investigate
which of the databases (Relational or NoSQL) performs better for a Process Discovery
application under Process Mining. We implement the α-miner algorithm on relational
(row-oriented) and NoSQL (column-oriented) databases in database query languages so
that our algorithm is tightly coupled to the database. We present a performance benchmarking
and comparison of the α-miner algorithm on row-oriented database and NoSQL column-oriented
database so that we can compare which database can efficiently store massive event
logs and analyze it in seconds to discover a process model.},
booktitle = {Proceedings of the Eighth International C* Conference on Computer Science &amp; Software Engineering},
pages = {113–118},
numpages = {6},
keywords = {Apache Hadoop, Process Mining, Apache HBase, Column-Oriented Database, Apache Phoenix, Row-Oriented Database, MySQL, Hadoop Distributed File System (HDFS)},
location = {Yokohama, Japan},
series = {C3S2E '15}
}

@inproceedings{10.1145/2944165.2944174,
author = {Yassien, Amal W. and Desouky, Amr F.},
title = {RDBMS, NoSQL, Hadoop: A Performance-Based Empirical Analysis},
year = {2016},
isbn = {9781450342933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2944165.2944174},
doi = {10.1145/2944165.2944174},
abstract = {The relational data model has been dominant and widely used since 1970. However, as
the need to deal with big data grows, new data models, such as Hadoop and NoSQL, were
developed to address the limitation of the traditional relational data model. As a
result, determining which data model is suitable for applications has become a challenge.
The purpose of this paper is to provide insight into choosing the suitable data model
by conducting a benchmark using Yahoo! Cloud Serving Benchmark (YCSB) on three different
database systems: (1) MySQL for relational data model, (2) MongoDB for NoSQL data
model, and (3) HBase for Hadoop framework. The benchmark was conducted by running
four different workloads. Each workload is executed using a different increasing operation
and thread count, while observing how their change respectively affects throughput,
latency, and runtime.},
booktitle = {Proceedings of the 2nd Africa and Middle East Conference on Software Engineering},
pages = {52–59},
numpages = {8},
keywords = {Performance, NoSQL, Hadoop, RDBMS, Databases},
location = {Cairo, Egypt},
series = {AMECSE '16}
}

@article{10.1145/2465085.2465100,
author = {Sattar, Abdul and Lorenzen, Torben and Nallamaddi, Keerthi},
title = {Incorporating NoSQL into a Database Course},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2153-2184},
url = {https://doi.org/10.1145/2465085.2465100},
doi = {10.1145/2465085.2465100},
abstract = {This article introduces the concepts of Big Data and NoSQL and describes a semester
long web-based project that uses both a relational database (Oracle 11g) and a NoSQL
(MongoDB) database for an undergraduate database course. The relational database stores
the record information of an online sales system and the NoSQL database stores three
manuals. The semester long assignment and its implementation are available as a download.},
journal = {ACM Inroads},
month = jun,
pages = {50–53},
numpages = {4},
keywords = {query languages, MongoDB, SQL, NoSQL, Oracle, database design, Java, RDBMS}
}

@inproceedings{10.1145/2602622.2602624,
author = {Mior, Michael J.},
title = {Automated Schema Design for NoSQL Databases},
year = {2014},
isbn = {9781450329248},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602622.2602624},
doi = {10.1145/2602622.2602624},
abstract = {Selecting appropriate indices and materialized views is critical for high performance
in relational databases. By example, we show that the problem of schema optimization
is also highly relevant for NoSQL databases. We explore the problem of schema design
in NoSQL databases with a goal of optimizing query performance while minimizing storage
overhead. Our suggested approach uses the cost of executing a given workload for a
given schema to guide the mapping from the application data model to a physical schema.
We propose a cost-driven approach for optimization and discuss its usefulness as part
of an automated schema design tool.},
booktitle = {Proceedings of the 2014 SIGMOD PhD Symposium},
pages = {41–45},
numpages = {5},
keywords = {schema optimization, nosql, workload modeling},
location = {Snowbird, Utah, USA},
series = {SIGMOD'14 PhD Symposium}
}

@inproceedings{10.5555/2814058.2814097,
author = {de Souza, Vanessa Cristina Oliveira and dos Santos, Marcus Vinicius Carli},
title = {Maturing, Consolidation and Performance of NoSQL Databases: Comparative Study},
year = {2015},
publisher = {Brazilian Computer Society},
address = {Porto Alegre, BRA},
abstract = {This paper presents a study on the NoSQL (Not Only SQL), a new dimension in databases.
Lately witnesses an expansion in the number of generated data, and the relational
model cannot deal with this data growth. The NoSQL comes as a solution to this problem.
This paper aims to conduct a study on the architecture of DBMS, demonstrate why they
can deal with this large volume of data and show some properties that NoSQL is based
to make your data management. Therefore, DBMSs Redis and Cassandra were used and compared
to the relational database MySQL. The maturity and consolidation of NoSQL DBMSs were
qualitatively assessed on the parameters online documentation, software help, support
virtual community and academic articles, so this work presents a perception of a user
already familiar with relational approach taking their first steps with NoSQL. The
results showed recognition of the relational database, but also showed a strong consolidation
of NoSQL technology market and academic research environments, especially with Cassandra.
The horizontal scale was tested with the DBMS Cassandra, who proved to be an excellent
tool in this regard.},
booktitle = {Proceedings of the Annual Conference on Brazilian Symposium on Information Systems: Information Systems: A Computer Socio-Technical Perspective - Volume 1},
pages = {235–242},
numpages = {8},
keywords = {performance, consolidation, horinzontal scale, NoSQL},
location = {Goiania, Goias, Brazil},
series = {SBSI 2015}
}

@inproceedings{10.1145/3341525.3387399,
author = {Kim, Suneuy},
title = {Seamless Integration of NoSQL Class into the Database Curriculum},
year = {2020},
isbn = {9781450368742},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341525.3387399},
doi = {10.1145/3341525.3387399},
abstract = {Since NoSQL emerged a decade ago, it has rapidly gained popularity and has been actively
incorporated into data management solutions for big data. This phenomenon brings positive
challenges to accommodate NoSQL topics in the database curriculum. This paper presents
our experience of teaching a NoSQL class over the last three years. The course uses
a comprehensive teaching methodology that combines lectures, hands-on assignments,
projects, and research-based approaches. The methodology aims at both students' in-depth
learning and seamless integration of NoSQL topics into the database curriculum. The
teaching methodology and course contents are detailed. Student evaluations of teaching,
assessment results, success stories, and challenges and lessons learned are presented.},
booktitle = {Proceedings of the 2020 ACM Conference on Innovation and Technology in Computer Science Education},
pages = {314–320},
numpages = {7},
keywords = {education, curriculum, databases, NoSQL, big data},
location = {Trondheim, Norway},
series = {ITiCSE '20}
}

@inproceedings{10.1145/2588555.2595631,
author = {Yang, Fangjin and Tschetter, Eric and L\'{e}aut\'{e}, Xavier and Ray, Nelson and Merlino, Gian and Ganguli, Deep},
title = {Druid: A Real-Time Analytical Data Store},
year = {2014},
isbn = {9781450323765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2588555.2595631},
doi = {10.1145/2588555.2595631},
abstract = {Druid is an open source data store designed for real-time exploratory analytics on
large data sets. The system combines a column-oriented storage layout, a distributed,
shared-nothing architecture, and an advanced indexing structure to allow for the arbitrary
exploration of billion-row tables with sub-second latencies. In this paper, we describe
Druid's architecture, and detail how it supports fast aggregations, flexible filters,
and low latency data ingestion.},
booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
pages = {157–168},
numpages = {12},
keywords = {distributed, real-time, open source, fault-tolerant, column-oriented, analytics, highly available, OLAP},
location = {Snowbird, Utah, USA},
series = {SIGMOD '14}
}

@inproceedings{10.1145/3428757.3429141,
author = {Sellami, Amal and Nabli, Ahlem and Gargouri, Faiez},
title = {Graph NoSQL Data Warehouse Creation},
year = {2020},
isbn = {9781450389228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428757.3429141},
doi = {10.1145/3428757.3429141},
abstract = {Over the last few years, NoSQL systems are gaining strong popularity and a number
of decision makers are using it to implement their warehouses. In the recent years,
many web applications are moving towards the use of data in the form of graphs. For
example, social media and the emergence of Facebook, LinkedIn and Twitter have accelerated
the emergence of the NoSQL database and in particular graph-oriented databases that
represent the basic format with which data in these media is stored. Based on these
findings and in addition to the absence of a clear approach which allows the creation
of data warehouse under NoSQL model, we propose, in this paper, an approach to create
a Graph-oriented Data warehouse. We propose the transformation of Dimensional Fact
Model into Graph Dimensional Model. Then, we implement the Graph Dimensional Model
using java routines in Talend Data Integration tool (TOS).},
booktitle = {Proceedings of the 22nd International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {34–38},
numpages = {5},
keywords = {Graph-oriented Data Warehouse, NoSQL Data Warehouse, Graph-oriented NoSQL model},
location = {Chiang Mai, Thailand},
series = {iiWAS '20}
}

@inproceedings{10.1145/3372938.3372954,
author = {Amghar, Souad and Cherdal, Safae and Mouline, Salma},
title = {Data Integration and NoSQL Systems: A State of the Art},
year = {2019},
isbn = {9781450372404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372938.3372954},
doi = {10.1145/3372938.3372954},
abstract = {Data Integration is one of the older research problems in database area. It aims to
combine data stored in different data sources and provide a unified view of data to
the user.With the spread of a new generation of database systems, called NoSQL systems,
data integration becomes more challenging, since we have to integrate data stored
in systems that implement different data models and query languages. Inspired by these
motivations, we provide in this paper an overview of data integration challenges and
solutions in the context of NoSQL systems.},
booktitle = {Proceedings of the 4th International Conference on Big Data and Internet of Things},
articleno = {16},
numpages = {6},
keywords = {Big Data, Data Integration, NoSQL database systems},
location = {Rabat, Morocco},
series = {BDIoT'19}
}

@inproceedings{10.1145/2095536.2095583,
author = {Pokorny, Jaroslav},
title = {NoSQL Databases: A Step to Database Scalability in Web Environment},
year = {2011},
isbn = {9781450307840},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2095536.2095583},
doi = {10.1145/2095536.2095583},
abstract = {The paper is focused on so called NoSQL databases. In context of cloud computing,
architectures and basic features of these databases are studied, particularly their
horizontal scalability and concurrency model, that is mostly weaker than ACID transactions
in relational SQL-like database systems. Some characteristics like a data model and
querying capabilities are discussed in more detail. The paper also contains an overview
of some representatives of NoSQL databases.},
booktitle = {Proceedings of the 13th International Conference on Information Integration and Web-Based Applications and Services},
pages = {278–283},
numpages = {6},
keywords = {weak consistency, horizontal data distribution, cloud computing, NoSQL database, CAP theorem, vertical scaling, horizontal scaling},
location = {Ho Chi Minh City, Vietnam},
series = {iiWAS '11}
}

@inproceedings{10.1145/3053600.3053622,
author = {Reniers, Vincent and Van Landuyt, Dimitri and Rafique, Ansar and Joosen, Wouter},
title = {On the State of NoSQL Benchmarks},
year = {2017},
isbn = {9781450348997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3053600.3053622},
doi = {10.1145/3053600.3053622},
abstract = {The proliferation of Big Data systems and namely NoSQL databases has resulted in a
tremendous heterogeneity in its offerings. It has become increasingly difficult to
compare and select the most optimal NoSQL storage technology. Current benchmark efforts,
such as the Yahoo! Cloud Serving Benchmark (YCSB), evaluate simple read and write
operations on a primary key. However, while YCSB has become the de-facto benchmark
solution for practitioners and NoSQL vendors, it is lacking in capabilities to extensively
evaluate specific NoSQL solutions.In this paper, we present a systematic survey of
current NoSQL benchmarks, in which we identify a clear gap in benchmarking more advanced
workloads (e.g. nested document search) for features specific to NoSQL database families
(e.g. document stores). Secondly, based on our survey, we discuss the strengths and
weaknesses of the different benchmark design approaches, and argue in favor of a benchmark
suite that targets specific families of NoSQL databases yet still allows overall comparison
of databases in terms of their commonalities.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
pages = {107–112},
numpages = {6},
keywords = {performance benchmarks, nosql benchmarks, ycsb},
location = {L'Aquila, Italy},
series = {ICPE '17 Companion}
}

@inproceedings{10.1145/2837185.2837224,
author = {Schreiner, Geomar A. and Duarte, Denio and dos Santos Mello, Ronaldo},
title = {SQLtoKeyNoSQL: A Layer for Relational to Key-Based NoSQL Database Mapping},
year = {2015},
isbn = {9781450334914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2837185.2837224},
doi = {10.1145/2837185.2837224},
abstract = {Today, many applications produce and manipulate a large volume of data, the so-called
Big Data. Traditional databases (DB), like relational databases, are not suitable
to Big Data management. In order to solve this problem, a new category of DB has been
proposed, the so-called NoSQL DB. NoSQL DB have different data models, as well as
different access methods which are not usually compatible with the SQL language. In
this context, approaches have been proposed for providing mapping of relational DB
schemata and operations to equivalent ones in NoSQL DB to deal with large relational
data sets in the cloud, focusing on scalability and availability. However, these approaches
map relational DB only to a single NoSQL data model and, sometimes, to a specific
NoSQL DB product. This paper presents SQLtoKeyNoSQL, a layer able to translate relational
schemata as well as SQL commands to equivalent schemata and access methods to any
key-oriented NoSQL DB (document-oriented, key-value and column-oriented). We present
the architecture of our layer focusing on our mapping strategies, as well as some
preliminary experiments that evaluate the impact of SQLtoKeyNoSQL as a solution for
transparent mapping of relational DB to key-based NoSQL DB.},
booktitle = {Proceedings of the 17th International Conference on Information Integration and Web-Based Applications &amp; Services},
articleno = {74},
numpages = {9},
keywords = {data management, SQL-NoSQL mapping, NoSQL, database interoperability},
location = {Brussels, Belgium},
series = {iiWAS '15}
}

@article{10.1145/3457608,
author = {Vera-Olivera, Harley and Guo, Ruizhe and Huacarpuma, Ruben Cruz and Da Silva, Ana Paula Bernardi and Mariano, Ari Melo and Holanda, Maristela},
title = {Data Modeling and NoSQL Databases - A Systematic Mapping Review},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3457608},
doi = {10.1145/3457608},
abstract = {Modeling is one of the most important steps in developing a database. In traditional
databases, the Entity Relationship (ER) and Unified Modeling Language (UML) models
are widely used. But how are NoSQL databases being modeled? We performed a systematic
mapping review to answer three research questions to identify and analyze the levels
of representation, models used, and contexts where the modeling process occurred in
the main categories of NoSQL databases. We found 54 primary studies where we identified
that conceptual and logical levels received more attention than the physical level
of representation. The UML, ER, and new notation based on ER and UML were adapted
to model NoSQL databases, in the same way, formats such as JSON, XML, and XMI were
used to generate schemas through the three levels of representation. New contexts
such as benchmark, evaluations, migration, and schema generation were identified,
as well as new features to be considered for modeling NoSQL databases, such as the
number of records by entities, CRUD operations, and system requirements (availability,
consistency, or scalability). Additionally, a coupling and co-citation analysis was
carried out to identify relevant works and researchers.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {116},
numpages = {26},
keywords = {systematic mapping, Data modeling, NoSQL databases}
}

@inproceedings{10.1145/3468791.3468820,
author = {Ton That, Dai Hai and Gharehdaghi, Mohammadsaleh and Rasin, Alexander and Malik, Tanu},
title = {On Lowering Merge Costs of an LSM Tree},
year = {2021},
isbn = {9781450384131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468791.3468820},
doi = {10.1145/3468791.3468820},
abstract = { In column stores, which ingest large amounts of data into multiple column groups,
query performance deteriorates. Commercial column stores use log-structured merge
(LSM) tree on projections to ingest data rapidly. LSM tree improves ingestion performance,
but for column stores the sort-merge maintenance phase in an LSM tree is I/O-intensive,
which slows concurrent queries and reduces overall throughput. In this paper, we present
a simple heuristic approach to reduce the sorting and merging cost that arise when
data is ingested in column stores. We demonstrate how a Min-Max heuristic can construct
buckets and identify the level of sortedness in each range of data. Filled and relatively-sorted
buckets are written out to disk; unfilled buckets are retained to achieve a better
level of sortedness, thus avoiding the expensive sort-merge phase. We compare our
Min-Max approach with LSM tree and production columnar stores using real and synthetic
datasets. },
booktitle = {33rd International Conference on Scientific and Statistical Database Management},
pages = {253–258},
numpages = {6},
keywords = {Min-Max, column-oriented, write-optimized},
location = {Tampa, FL, USA},
series = {SSDBM 2021}
}

@inproceedings{10.1145/2938503.2938518,
author = {Oliveira, F\'{a}bio Roberto and del Val Cura, Luis},
title = {Performance Evaluation of NoSQL Multi-Model Data Stores in Polyglot Persistence Applications},
year = {2016},
isbn = {9781450341189},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2938503.2938518},
doi = {10.1145/2938503.2938518},
abstract = {NoSQL data store systems have recently been introduced as alternatives to traditional
relational database management systems. These data stores systems implement simpler
and scalable data models that increase the performance and efficiency of a new kind
of emerging complex database application. Applications that model their data using
two or more simple NoSQL models are known as applications with polyglot persistence.
Usually, their implementations are complex because they must manage and store their
data using several data store systems simultaneously. Recently, a new family of multi-model
data stores was introduced, integrating simple NoSQL data models into a single unique
system. This paper presents a performance evaluation of multi-model data stores used
by an application with polyglot persistence. In this research, multi-- model datasets
were synthesized in order to simulate that application. We evaluate the performance
of benchmarks based on a set of basic database operations on single model and multimodel
data store systems. Experimental results show that in some scenarios multi-model data
stores have similar or better performance than simple model data stores.},
booktitle = {Proceedings of the 20th International Database Engineering &amp; Applications Symposium},
pages = {230–235},
numpages = {6},
keywords = {Database Performance, NoSQL Data Stores, Polyglot Persistence},
location = {Montreal, QC, Canada},
series = {IDEAS '16}
}

@article{10.14778/2367502.2367511,
author = {Floratou, Avrilia and Teletia, Nikhil and DeWitt, David J. and Patel, Jignesh M. and Zhang, Donghui},
title = {Can the Elephants Handle the NoSQL Onslaught?},
year = {2012},
issue_date = {August 2012},
publisher = {VLDB Endowment},
volume = {5},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2367502.2367511},
doi = {10.14778/2367502.2367511},
abstract = {In this new era of "big data", traditional DBMSs are under attack from two sides.
At one end of the spectrum, the use of document store NoSQL systems (e.g. MongoDB)
threatens to move modern Web 2.0 applications away from traditional RDBMSs. At the
other end of the spectrum, big data DSS analytics that used to be the domain of parallel
RDBMSs is now under attack by another class of NoSQL data analytics systems, such
as Hive on Hadoop. So, are the traditional RDBMSs, aka "big elephants", doomed as
they are challenged from both ends of this "big data" spectrum? In this paper, we
compare one representative NoSQL system from each end of this spectrum with SQL Server,
and analyze the performance and scalability aspects of each of these approaches (NoSQL
vs. SQL) on two workloads (decision support analysis and interactive data-serving)
that represent the two ends of the application spectrum. We present insights from
this evaluation and speculate on potential trends for the future.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1712–1723},
numpages = {12}
}

@inproceedings{10.1145/2509908.2509914,
author = {Cuzzocrea, Alfredo and Di Stefano, Marcello and Fosci, Paolo and Psaila, Giuseppe},
title = {Effectively and Efficiently Supporting Crowd-Enabled Databases via NoSQL Paradigms},
year = {2013},
isbn = {9781450324830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509908.2509914},
doi = {10.1145/2509908.2509914},
abstract = {In this paper we provide an overview of the Hints From the Crowd (HFC) project, whose
main goal is to build a NoSQL database system for large collections of product reviews;
the database is queried by expressing a natural language sentence; the result is a
list of products ranked based on the relevance of reviews w.r.t. the natural language
sentence. The best ranked products in the result list can be seen as the best hints
for the user based on crowd opinions (the reviews).The HFC prototype has been developed
as a web application, independent of the particular application domain of the collected
product reviews. Queries are performed by evaluating a text-based ranking metric for
sets of reviews, specifically devised for this system; the metric evaluates the relevance
of product reviews w.r.t. a natural language sentence.},
booktitle = {Proceedings of the 3rd International Workshop on Semantic Search Over the Web},
articleno = {7},
numpages = {5},
location = {Riva del Garda, Italy},
series = {SSW '13}
}

@inproceedings{10.1145/2452376.2452378,
author = {Mohan, C.},
title = {History Repeats Itself: Sensible and NonsenSQL Aspects of the NoSQL Hoopla},
year = {2013},
isbn = {9781450315975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2452376.2452378},
doi = {10.1145/2452376.2452378},
abstract = {In this paper, I describe some of the recent developments in the database management
area, in particular the NoSQL phenomenon and the hoopla associated with it. The goal
of the paper is not to do an exhaustive survey of NoSQL systems. The aim is to do
a broad brush analysis of what these developments mean - the good and the bad aspects!
Based on my more than three decades of database systems work in the research and product
arenas, I will outline what are many of the pitfalls to avoid since there is currently
a mad rush to develop and adopt a plethora of NoSQL systems in a segment of the IT
population, including the research community. In rushing to develop these systems
to overcome some of the shortcomings of the relational systems, many good principles
of the latter, which go beyond the relational model and the SQL language, have been
left by the wayside. Now many of the features that were initially discarded as unnecessary
in the NoSQL systems are being brought in, but unfortunately in ad hoc ways. Hopefully,
the lessons learnt over three decades with relational and other systems would not
go to waste and we wouldn't let history repeat itself with respect to simple minded
approaches leading to enormous pain later on for developers as well as users of the
NoSQL systems!Caveat: What I express in this paper are my personal opinions and they
do not necessarily reflect the opinions of my employer.},
booktitle = {Proceedings of the 16th International Conference on Extending Database Technology},
pages = {11–16},
numpages = {6},
keywords = {MongoDB, optimization, SQL, hype, RDBMS, data models, DBMS, in-memory, APIs, NoSQL, JSON, HBase},
location = {Genoa, Italy},
series = {EDBT '13}
}

@inproceedings{10.1145/2812428.2812429,
author = {Pokorn\'{y}, Jaroslav},
title = {Database Technologies in the World of Big Data},
year = {2015},
isbn = {9781450333573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2812428.2812429},
doi = {10.1145/2812428.2812429},
abstract = {Now we have a number of database technologies called usually NoSQL, like key-value,
column-oriented, and document stores as well as search engines and graph databases.
Whereas SQL software vendors offer advanced products with the capability to handle
highly complex queries and transactions, NoSQL databases share rather characteristics
concerning scaling and performance, as e.g. auto-sharding, distributed query support,
and integrated caching. Their drawbacks can be a lack of schema or data consistency,
difficulty in testing and maintaining, and absence of a higher query language. Complex
data modelling and the SQL language as the only access tool to data are missing here.
On the other hand, last studies show that both SQL and NoSQL databases have value
for both for transactional and analytical Big Data. Top databases providers offer
rearchitected database technologies combining row data stores with columnar in-memory
compression enabling processing large data sets and analytical querying, often over
massive, continuous data streams. The technological progress led to development of
massively parallel processing analytic databases. The paper presents some details
of current database technologies, their pros and cons in different application environments,
and emerging trends in this area.},
booktitle = {Proceedings of the 16th International Conference on Computer Systems and Technologies},
pages = {1–12},
numpages = {12},
keywords = {big data, database technologies, NoSQL databases, data distribution, NewSQL databases, transaction processing, big analytics},
location = {Dublin, Ireland},
series = {CompSysTech '15}
}

@inproceedings{10.1145/3297663.3309668,
author = {Talreja, Disha and Lahiri, Kanishka and Kalambur, Subramaniam and Raghavendra, Prakash},
title = {Performance Scaling of Cassandra on High-Thread Count Servers},
year = {2019},
isbn = {9781450362399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297663.3309668},
doi = {10.1145/3297663.3309668},
abstract = {NoSQL databases are commonly used today in cloud deployments due to their ability
to "scale-out" and effectively use distributed computing resources in a data center.
At the same time, cloud servers are also witnessing rapid growth in CPU core counts,
memory bandwidth, and memory capacity. Hence, apart from scaling out effectively,
it's important to consider how such workloads "scale-up" within a single system, so
that they can make the best use of available resources. In this paper, we describe
our experiences studying the performance scaling characteristics of Cassandra, a popular
open-source, column-oriented database, on a single high-thread count dual socket server.
We demonstrate that using commonly used benchmarking practices, Cassandra does not
scale well on such systems. Next, we show how by taking into account specific knowledge
of the underlying topology of the server architecture, we can achieve substantial
improvements in performance scalability. We report on how, during the course of our
work, we uncovered an area for performance improvement in the official open-source
implementation of the Java platform with respect to NUMA awareness. We show how optimizing
this resulted in 27% throughput gain for Cassandra under studied configurations. As
a result of these optimizations, using standard workload generators, we obtained up
to 1.44x and 2.55x improvements in Cassandra throughput over baseline single and dual-socket
performance measurements respectively. On wider testing across a variety of workloads,
we achieved excellent performance scaling, averaging 98% efficiency within a socket
and 90% efficiency at the system-level.},
booktitle = {Proceedings of the 2019 ACM/SPEC International Conference on Performance Engineering},
pages = {179–187},
numpages = {9},
keywords = {nosql databases, cassandra, performance benchmarking, performance scalability},
location = {Mumbai, India},
series = {ICPE '19}
}

@inproceedings{10.1145/3167132.3167191,
author = {Soransso, R. A. S. N. and Cavalcanti, M. C.},
title = {Data Modeling for Analytical Queries on Document-Oriented DBMS},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167191},
doi = {10.1145/3167132.3167191},
abstract = {NoSQL database management systems have emerged as an alternative to increase performance
and decrease the hardware costs of applications that use traditional relational databases.
However, there are not many works on how to guide data modeling for such DBMS to gain
query performance. Specially, in the context of Business Intelligence (BI) applications,
data modeling should take into account analytical queries performance. This work highlights
the importance of data modeling for this kind of application on NoSQL DBMS. It shows
how much alternative modelings can significantly impact on the query performance.
Experiments were performed on MongoDB, a popular document-oriented NoSQL DBMS, and
show some significant results. In addition, a modeling heuristic is presented for
this DBMS, and suggests that more than one document collection, based on alternative
data modelings, should be maintained in order to improve query performance. Future
work points to query redirection mechanisms for such systems.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {541–548},
numpages = {8},
keywords = {document-store, NoSQL, query performance, heuristic, data modeling},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/2361999.2362039,
author = {Begoli, Edmon},
title = {A Short Survey on the State of the Art in Architectures and Platforms for Large Scale Data Analysis and Knowledge Discovery from Data},
year = {2012},
isbn = {9781450315685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361999.2362039},
doi = {10.1145/2361999.2362039},
abstract = {Intended as a survey for practicing architects and researchers seeking an overview
of the state-of-the-art architectures for data analysis, this paper provides an overview
of the emerging data management and analytic platforms including parallel databases,
Hadoop-based systems, High Performance Computing (HPC) platforms and platforms popularly
referred to as NoSQL platforms. Platforms are presented based on their relevance,
analysis they support and the data organization model they support.},
booktitle = {Proceedings of the WICSA/ECSA 2012 Companion Volume},
pages = {177–183},
numpages = {7},
keywords = {big data, knowledge discovery from data, large scale data analysis, software architecture, massively parallel processing, NoSQL},
location = {Helsinki, Finland},
series = {WICSA/ECSA '12}
}

@inproceedings{10.1145/3007818.3007844,
author = {Nguyen, Trong-Dat and Lee, Sang-Won},
title = {I/O Characteristics of MongoDB and Trim-Based Optimization in Flash SSDs},
year = {2016},
isbn = {9781450347549},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3007818.3007844},
doi = {10.1145/3007818.3007844},
abstract = {NoSQL solutions become emerging for large scaled, high performance, schema-flexible
applications. WiredTiger is cost effective, non-locking, no-overwrite storage used
as default storage engine in MongoDB. Understanding I/O characteristics of storage
engine is important not only for choosing suitable solution with an application but
also opening opportunities for researchers optimizing current working system, especially
building more flash-awareness NoSQL DBMS. This paper explores background of MongoDB
internals then analyze I/O characteristics of WiredTiger storage engine in detail.
We also exploit space management mechanism in WiredTiger by using TRIM command.},
booktitle = {Proceedings of the Sixth International Conference on Emerging Databases: Technologies, Applications, and Theory},
pages = {139–144},
numpages = {6},
keywords = {NoSQL, I/O pattern, SSD, MongoDB, TRIM command, YCSB, WiredTiger, I/O characteristics},
location = {Jeju, Republic of Korea},
series = {EDB '16}
}

@inproceedings{10.1145/2465848.2465849,
author = {Dede, Elif and Govindaraju, Madhusudhan and Gunter, Daniel and Canon, Richard Shane and Ramakrishnan, Lavanya},
title = {Performance Evaluation of a MongoDB and Hadoop Platform for Scientific Data Analysis},
year = {2013},
isbn = {9781450319799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2465848.2465849},
doi = {10.1145/2465848.2465849},
abstract = {Scientific facilities such as the Advanced Light Source (ALS) and Joint Genome Institute
and projects such as the Materials Project have an increasing need to capture, store,
and analyze dynamic semi-structured data and metadata. A similar growth of semi-structured
data within large Internet service providers has led to the creation of NoSQL data
stores for scalable indexing and MapReduce for scalable parallel analysis. MapReduce
and NoSQL stores have been applied to scientific data. Hadoop, the most popular open
source implementation of MapReduce, has been evaluated, utilized and modified for
addressing the needs of different scientific analysis problems. ALS and the Materials
Project are using MongoDB, a document oriented NoSQL store. However, there is a limited
understanding of the performance trade-offs of using these two technologies together.In
this paper we evaluate the performance, scalability and fault-tolerance of using MongoDB
with Hadoop, towards the goal of identifying the right software environment for scientific
data analysis.},
booktitle = {Proceedings of the 4th ACM Workshop on Scientific Cloud Computing},
pages = {13–20},
numpages = {8},
keywords = {MapReduce, distributed computing, MongoDB, scientific computing, NoSQL, Hadoop},
location = {New York, New York, USA},
series = {Science Cloud '13}
}

@inproceedings{10.1145/2588555.2612183,
author = {Tahara, Daniel and Diamond, Thaddeus and Abadi, Daniel J.},
title = {Sinew: A SQL System for Multi-Structured Data},
year = {2014},
isbn = {9781450323765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2588555.2612183},
doi = {10.1145/2588555.2612183},
abstract = {As applications are becoming increasingly dynamic, the notion that a schema can be
created in advance for an application and remain relatively stable is becoming increasingly
unrealistic. This has pushed application developers away from traditional relational
database systems and away from the SQL interface, despite their many well-established
benefits. Instead, developers often prefer self-describing data models such as JSON,
and NoSQL systems designed specifically for their relaxed semantics.In this paper,
we discuss the design of a system that enables developers to continue to represent
their data using self-describing formats without moving away from SQL and traditional
relational database systems. Our system stores arbitrary documents of key-value pairs
inside physical and virtual columns of a traditional relational database system, and
adds a layer above the database system that automatically provides a dynamic relational
view to the user against which fully standard SQL queries can be issued. We demonstrate
that our design can achieve an order of magnitude improvement in performance over
alternative solutions, including existing relational database JSON extensions, MongoDB,
and shredding systems that store flattened key-value data inside a relational database.},
booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
pages = {815–826},
numpages = {12},
keywords = {RDBMS, JSON, NoSQL, SQL, MongoDB, dynamic schema},
location = {Snowbird, Utah, USA},
series = {SIGMOD '14}
}

@inproceedings{10.1145/3030207.3044529,
author = {Zagieboylo, Drew and Zaman, Kazi A.},
title = {Cost-Efficient and Reliable Reporting of Highly Bursty Video Game Crash Data},
year = {2017},
isbn = {9781450344043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3030207.3044529},
doi = {10.1145/3030207.3044529},
abstract = {Video game crash events are characterized primarily by large media payloads and by
highly bursty traffic patterns, with hundreds of thousands or millions of reports
being issued in only a few minutes. These events are invaluable in quickly responding
to game breaking issues that directly impact user experience. Even the slightest delay
in capturing, processing and reporting these events can lead to user abandonment and
significant financial cost.A traditional standalone RESTful service, backed by a vertically
scaled SQL database is neither a reliable nor cost-effective solution to this problem.
An architecture that decouples capture and persistence and uses a horizontally scalable
NoSQL database is not only easier to provision, but also uses fewer cpu and memory
resources to provide the same end to end latency and throughput.By replacing our RESTful
implementation with one that takes advantage both of the aforementioned design and
multi-tenant provisioning, we have reduced our dedicated cpu footprint by 63% and
memory footprint by 59%. Additionally, we have decreased our data loss during spikes
to essentially 0, maintained sub-second persistence latency and improved query latency
in the average case by 54% with only a 3% sacrifice for worst case queries.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
pages = {201–212},
numpages = {12},
keywords = {cloud infrastructure, reliability, cost efficiency, crash reporting, nosql},
location = {L'Aquila, Italy},
series = {ICPE '17}
}

@inproceedings{10.5555/2694443.2694458,
author = {Awasthi, Anurag and Nandini, Avani and Bhattacharya, Arnab and Sehgal, Priya},
title = {Hybrid HBase: Leveraging Flash SSDs to Improve Cost per Throughput of HBase},
year = {2012},
publisher = {Computer Society of India},
address = {Mumbai, Maharashtra, IND},
abstract = {Column-oriented data stores, such as BigTable and HBase, have successfully paved the
way for managing large key-value datasets with random accesses. At the same time,
the declining cost of flash SSDs have enabled their use in several applications including
large databases. In this paper, we explore the feasibility of introducing flash SSDs
for HBase. Since storing the entire user data is infeasible due to impractically large
costs, we perform a qualitative and supporting quantitative assessment of the implications
of storing the system components of HBase in flash SSDs. Our proposed Hybrid HBase
system performs 1.5-2 times better than a complete disk-based system on the YCSB benchmark
workloads. This increase in performance comes at a relatively low cost overhead. Consequently,
Hybrid HBase exhibits the best performance in terms of cost per throughput when compared
to either a complete HDD-based or a complete flash SSD-based system.},
booktitle = {Proceedings of the 18th International Conference on Management of Data},
pages = {68–79},
numpages = {12},
keywords = {cost per throughput, big data, HBase, flash SSD},
location = {Pune, India},
series = {COMAD '12}
}

@inproceedings{10.1145/3141128.3141139,
author = {Pti\v{c}ek, Marina and Vrdoljak, Boris},
title = {Big Data and New Data Warehousing Approaches},
year = {2017},
isbn = {9781450353434},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3141128.3141139},
doi = {10.1145/3141128.3141139},
abstract = {Big data are a data trend present around us mainly through Internet -- social networks
and smart devices and meters -- mostly without us being aware of them. Also they are
a fact that both industry and scientific research needs to deal with. They are interesting
from analytical point of view, for they contain knowledge that cannot be ignored and
left unused. Traditional system that supports the advanced analytics and knowledge
extraction -- data warehouse -- is not able to cope with large amounts of fast incoming
various and unstructured data, and may be facing a paradigm shift in terms of utilized
concepts, technologies and methodologies, which have become a very active research
area in the last few years. This paper provides an overview of research trends important
for the big data warehousing, concepts and technologies used for data storage and
(ETL) processing, and research approaches done in attempts to empower traditional
data warehouses for handling big data.},
booktitle = {Proceedings of the 2017 International Conference on Cloud and Big Data Computing},
pages = {6–10},
numpages = {5},
keywords = {databases, MapReduce, NoSQL, data warehouse, NewSQL, big data},
location = {London, United Kingdom},
series = {ICCBDC 2017}
}

@inproceedings{10.1109/WI-IAT.2013.62,
author = {Pe\~{n}as, Paula and del Hoyo, Rafael and Vea-Murgu\'{\i}a, Jorge and Gonz\'{a}lez, Carlos and Mayo, Sergio},
title = {Collective Knowledge Ontology User Profiling for Twitter -- Automatic User Profiling},
year = {2013},
isbn = {9780769551456},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2013.62},
doi = {10.1109/WI-IAT.2013.62},
abstract = {How to model user interests and intentions through user profiling is an important
key for providing personalized service on Internet. User profiling can be seen as
the inference of user interests, intentions, characteristics, behaviors and preferences.
This paper introduces a scalable and automated technique for user profiling by extracting
his URLs from publicly available tweets information and using a semantic ontology
in which user interests and intentions are characterized. In order to enhance the
performance of our method, categorization of websites offered by OpenDNS and DBpedia
collective knowledge databases are used to find the interests and intention categories
of the user profile ontology. In this context, user profile ontology is populated
taking these collective categories and with assertions of individuals, and relationships
of interest and intention. As new concepts and relationships are defined and inferred,
user profile ontology evolves continuously. Experimental results based on user's tweets
confirm strongly that the proposed method improves the automatic acquisition of interests
and intentions of a user profile.},
booktitle = {Proceedings of the 2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT) - Volume 01},
pages = {439–444},
numpages = {6},
keywords = {Ontology, Social Networks, Twitter, NoSQL, OWL, RDF},
series = {WI-IAT '13}
}

@article{10.1145/2094114.2094126,
author = {F\"{a}rber, Franz and Cha, Sang Kyun and Primsch, J\"{u}rgen and Bornh\"{o}vd, Christof and Sigg, Stefan and Lehner, Wolfgang},
title = {SAP HANA Database: Data Management for Modern Business Applications},
year = {2012},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/2094114.2094126},
doi = {10.1145/2094114.2094126},
abstract = {The SAP HANA database is positioned as the core of the SAP HANA Appliance to support
complex business analytical processes in combination with transactionally consistent
operational workloads. Within this paper, we outline the basic characteristics of
the SAP HANA database, emphasizing the distinctive features that differentiate the
SAP HANA database from other classical relational database management systems. On
the technical side, the SAP HANA database consists of multiple data processing engines
with a distributed query processing environment to provide the full spectrum of data
processing -- from classical relational data supporting both row- and column-oriented
physical representations in a hybrid engine, to graph and text processing for semi-
and unstructured data management within the same system.From a more application-oriented
perspective, we outline the specific support provided by the SAP HANA database of
multiple domain-specific languages with a built-in set of natively implemented business
functions. SQL -- as the lingua franca for relational database systems -- can no longer
be considered to meet all requirements of modern applications, which demand the tight
interaction with the data management layer. Therefore, the SAP HANA database permits
the exchange of application semantics with the underlying data management platform
that can be exploited to increase query expressiveness and to reduce the number of
individual application-to-database round trips.},
journal = {SIGMOD Rec.},
month = jan,
pages = {45–51},
numpages = {7}
}

@inproceedings{10.1109/CCGRID.2017.17,
author = {Weintraub, Grisha and Gudes, Ehud},
title = {Crowdsourced Data Integrity Verification for Key-Value Stores in the Cloud},
year = {2017},
isbn = {9781509066100},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2017.17},
doi = {10.1109/CCGRID.2017.17},
abstract = {Thanks to their high availability, scalability, and usability, cloud databases have
become one of the dominant cloud services. However, since cloud users do not physically
possess their data, data integrity may be at risk. In this paper, we present a novel
protocol that utilizes crowdsourcing paradigm to provide practical data integrity
assurance in key-value cloud databases. The main advantage of our protocol over previous
work is its high applicability - as opposed to existing approaches, our scheme does
not require any system changes on the cloud side and thus can be applied directly
to any existing system. We demonstrate the feasibility of our scheme by a prototype
implementation and its evaluation.},
booktitle = {Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {498–503},
numpages = {6},
keywords = {key-value stores, data integrity, secure storage, cloud, NoSQL},
location = {Madrid, Spain},
series = {CCGrid '17}
}

@inproceedings{10.1145/2588555.2588891,
author = {Kaoudi, Zoi and Manolescu, Ioana},
title = {Cloud-Based RDF Data Management},
year = {2014},
isbn = {9781450323765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2588555.2588891},
doi = {10.1145/2588555.2588891},
abstract = {The W3C's Resource Description Framework (or RDF, in short) is set to deliver many
of the original semi-structured data promises: flexible structure, optional schema,
and rich, flexible URIs as a basis for information sharing. Moreover, RDF is uniquely
positioned to benefit from the efforts of scientific communities studying databases,
knowledge representation, and Web technologies. As a consequence, numerous collections
of RDF data are published, going from scientific data to general-purpose ontologies
to open government data, in particular published as part of the Linked Data movement.
Managing such large volumes of RDF data is challenging, due to the sheer size, the
heterogeneity, and the further complexity brought by RDF reasoning. To tackle the
size challenge, distributed storage architectures are required. Cloud computing is
an emerging distributed paradigm massively adopted in many applications for the scalability,
fault-tolerance and elasticity features it provides. This tutorial presents the challenges
faced in order to efficiently handle massive amounts of RDF data in a cloud environment.
We provide the necessary background, analyze and classify existing solutions, and
discuss open problems and perspectives.},
booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
pages = {725–729},
numpages = {5},
keywords = {cloud, noSQL, data management, RDF, mapreduce},
location = {Snowbird, Utah, USA},
series = {SIGMOD '14}
}

@inproceedings{10.1145/3297663.3310303,
author = {Seybold, Daniel and Keppler, Moritz and Gr\"{u}ndler, Daniel and Domaschka, J\"{o}rg},
title = {Mowgli: Finding Your Way in the DBMS Jungle},
year = {2019},
isbn = {9781450362399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297663.3310303},
doi = {10.1145/3297663.3310303},
abstract = {Big Data and IoT applications require highly-scalable database management system (DBMS),
preferably operated in the cloud to ensure scalability also on the resource level.
As the number of existing distributed DBMS is extensive, the selection and operation
of a distributed DBMS in the cloud is a challenging task. While DBMS benchmarking
is a supportive approach, existing frameworks do not cope with the runtime constraints
of distributed DBMS and the volatility of cloud environments. Hence, DBMS evaluation
frameworks need to consider DBMS runtime and cloud resource constraints to enable
portable and reproducible results. In this paper we present Mowgli, a novel evaluation
framework that enables the evaluation of non-functional DBMS features in correlation
with DBMS runtime and cloud resource constraints. Mowgli fully automates the execution
of cloud and DBMS agnostic evaluation scenarios, including DBMS cluster adaptations.
The evaluation of Mowgli is based on two IoT-driven scenarios, comprising the DBMSs
Apache Cassandra and Couchbase, nine DBMS runtime configurations, two cloud providers
with two different storage backends. Mowgli automates the execution of the resulting
102 evaluation scenarios, verifying its support for portable and reproducible DBMS
evaluations. The results provide extensive insights into the DBMS scalability and
the impact of different cloud resources. The significance of the results is validated
by the correlation with existing DBMS evaluation results.},
booktitle = {Proceedings of the 2019 ACM/SPEC International Conference on Performance Engineering},
pages = {321–332},
numpages = {12},
keywords = {benchmarking, nosql, cloud, scalability, distributed database},
location = {Mumbai, India},
series = {ICPE '19}
}

@inproceedings{10.1145/3105831.3105841,
author = {Costa, Carlos and Santos, Maribel Yasmina},
title = {The SusCity Big Data Warehousing Approach for Smart Cities},
year = {2017},
isbn = {9781450352208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3105831.3105841},
doi = {10.1145/3105831.3105841},
abstract = {Nowadays, the concept of Smart City provides a rich analytical context, highlighting
the need to store and process vast amounts of heterogeneous data flowing at different
velocities. This data is defined as Big Data, which imposes significant difficulties
in traditional data techniques and technologies. Data Warehouses (DWs) have long been
recognized as a fundamental enterprise asset, providing fact-based decision support
for several organizations. The concept of DW is evolving. Traditionally, Relational
Database Management Systems (RDBMSs) are used to store historical data, providing
different analytical perspectives regarding several business processes. With the current
advancements in Big Data techniques and technologies, the concept of Big Data Warehouse
(BDW) emerges to surpass several limitations of traditional DWs. This paper presents
a novel approach for designing and implementing BDWs, which has been supporting the
SusCity data visualization platform. The BDW is a crucial component of the SusCity
research project in the context of Smart Cities, supporting analytical tasks based
on data collected in the city of Lisbon.},
booktitle = {Proceedings of the 21st International Database Engineering &amp; Applications Symposium},
pages = {264–273},
numpages = {10},
keywords = {Big Data Warehousing, Hadoop, NoSQL, Big Data, Smart Cities, Data Warehouse},
location = {Bristol, United Kingdom},
series = {IDEAS 2017}
}

@article{10.14778/3297753.3297756,
author = {Kara, Kaan and Eguro, Ken and Zhang, Ce and Alonso, Gustavo},
title = {ColumnML: Column-Store Machine Learning with on-the-Fly Data Transformation},
year = {2018},
issue_date = {December 2018},
publisher = {VLDB Endowment},
volume = {12},
number = {4},
issn = {2150-8097},
url = {https://doi.org/10.14778/3297753.3297756},
doi = {10.14778/3297753.3297756},
abstract = {The ability to perform machine learning (ML) tasks in a database management system
(DBMS) provides the data analyst with a powerful tool. Unfortunately, integration
of ML into a DBMS is challenging for reasons varying from differences in execution
model to data layout requirements. In this paper, we assume a column-store main-memory
DBMS, optimized for online analytical processing, as our initial system. On this system,
we explore the integration of coordinate-descent based methods working natively on
columnar format to train generalized linear models. We use a cache-efficient, partitioned
stochastic coordinate descent algorithm providing linear throughput scalability with
the number of cores while preserving convergence quality, up to 14 cores in our experiments.Existing
column oriented DBMS rely on compression and even encryption to store data in memory.
When those features are considered, the performance of a CPU based solution suffers.
Thus, in the paper we also show how to exploit hardware acceleration as part of a
hybrid CPU+FPGA system to provide on-the-fly data transformation combined with an
FPGA-based coordinate-descent engine. The resulting system is a column-store DBMS
with its important features preserved (e.g., data compression) that offers high performance
machine learning capabilities.},
journal = {Proc. VLDB Endow.},
month = dec,
pages = {348–361},
numpages = {14}
}

@inproceedings{10.1145/2755644.2755650,
author = {Li, Tonglin and Keahey, Kate and Wang, Ke and Zhao, Dongfang and Raicu, Ioan},
title = {A Dynamically Scalable Cloud Data Infrastructure for Sensor Networks},
year = {2015},
isbn = {9781450335706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2755644.2755650},
doi = {10.1145/2755644.2755650},
abstract = {As small, specialized sensor devices become more ubiquitous, reliable, and cheap,
increasingly more domain sciences are creating "instruments at large" - dynamic, often
self-organizing, groups of sensors whose outputs are capable of being aggregated and
correlated to support experiments organized around specific questions. This calls
for an infrastructure able to collect, store, query, and process data set from sensor
networks. The design and development of such infrastructure faces several challenges.
The challenges reflect the need to interact with and administer the sensors remotely.
The sensors may be deployed in inaccessible places and have only intermittent network
connectivity due to power conservation and other factors. This requires communication
protocols that can withstand unreliable networks as well as an administrative interface
to sensor controller. Further, the system has to be scalable, i.e., capable of ultimately
dealing with potentially large numbers of data producing sensors. It also needs to
be able to organize many different data types efficiently. And finally, it also needs
to scale in the number of queries and processing requests. In this work we present
a set of protocols and a cloud-based data streaming infrastructure called WaggleDB
that address those challenges. The system efficiently aggregates and stores data from
sensor networks and enables the users to query those data sets. It address the challenges
above with a scalable multi-tier architecture, which is designed in such way that
each tier can be scaled by adding more independent resources provisioned on-demand
in the cloud.},
booktitle = {Proceedings of the 6th Workshop on Scientific Cloud Computing},
pages = {25–28},
numpages = {4},
keywords = {data streaming, cloud, sensor networks, nosql databases},
location = {Portland, Oregon, USA},
series = {ScienceCloud '15}
}

@article{10.1145/3323214,
author = {Lu, Jiaheng and Holubov\'{a}, Irena},
title = {Multi-Model Databases: A New Journey to Handle the Variety of Data},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3323214},
doi = {10.1145/3323214},
abstract = {The variety of data is one of the most challenging issues for the research and practice
in data management systems. The data are naturally organized in different formats
and models, including structured data, semi-structured data, and unstructured data.
In this survey, we introduce the area of multi-model DBMSs that build a single database
platform to manage multi-model data. Even though multi-model databases are a newly
emerging area, in recent years, we have witnessed many database systems to embrace
this category. We provide a general classification and multi-dimensional comparisons
for the most popular multi-model databases. This comprehensive introduction on existing
approaches and open problems, from the technique and application perspective, make
this survey useful for motivating new multi-model database approaches, as well as
serving as a technical reference for developing multi-model database applications.},
journal = {ACM Comput. Surv.},
month = jun,
articleno = {55},
numpages = {38},
keywords = {NoSQL database management systems, multi-model databases, Big data management}
}

@inproceedings{10.1145/2837060.2837062,
author = {Cho, Wonhee and Choi, Eunmi},
title = {A GPS Trajectory Map-Matching Mechanism with DTG Big Data on the HBase System},
year = {2015},
isbn = {9781450338462},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2837060.2837062},
doi = {10.1145/2837060.2837062},
abstract = {Since smartphones equipped with GPS have been produced, the need to conduct an analysis
by matching the mass of GPS trajectory data on a digital map has increased. However,
the study of the existing map-matching algorithm technique is mainly for navigation.
In order to analyze large amounts of GPS trajectories on a server, issues of the speed
and performance of the system exist. The purpose of this study is to utilize a map-matching
system using HBase, which is a distributed NoSQL DB in a Hadoop ecosystem. We defined
the table specification of HBase for mounting the digital map and proposed and implemented
the method for analysis with a map-matching algorithm. In this paper, we present the
map-matching methodology using the NoSQL DB of Hadoop ecosystem for analyzing GPS
trajectory.},
booktitle = {Proceedings of the 2015 International Conference on Big Data Applications and Services},
pages = {22–29},
numpages = {8},
keywords = {Big data, map matching, spatial analysis, HBase, Hadoop},
location = {Jeju Island, Republic of Korea},
series = {BigDAS '15}
}

@inproceedings{10.1145/3297280.3297506,
author = {Ravat, Franck and Song, Jiefu and Teste, Olivier and Trojahn, Cassia},
title = {Improving the Performance of Querying Multidimensional RDF Data Using Aggregates},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297506},
doi = {10.1145/3297280.3297506},
abstract = {In this paper, we propose a novel approach to tackle the problem of querying large
volume of statistical RDF cubes. Our approach relies on combining pre-aggregation
strategies and the performance of NoSQL engines to represent and manage statistical
RDF data. Specifically, we define a conceptual modeling solution to represent original
RDF data with aggregates in a multidimensional structure. We complete the conceptual
modeling with a logical design process based on well-known multidimensional RDF graph
and property-graph representations. We implement our proposed model in RDF triple
stores and a property-graph NoSQL database, and we compare the querying performance,
with and without aggregates. Experimental results, on real-world datasets containing
81.92 million triplets, show that pre-aggregation allows reducing query runtime in
both RDF triple stores and property-graph NoSQL databases. Neo4j NoSQL database with
aggregates outperforms RDF Jena TDB2 and Virtuoso triple stores, speeding up to 99%
query runtime.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {2275–2284},
numpages = {10},
keywords = {multidimensional graph data, querying performance, pre-computed aggregates},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/1900008.1900067,
author = {Vicknair, Chad and Macias, Michael and Zhao, Zhendong and Nan, Xiaofei and Chen, Yixin and Wilkins, Dawn},
title = {A Comparison of a Graph Database and a Relational Database: A Data Provenance Perspective},
year = {2010},
isbn = {9781450300643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1900008.1900067},
doi = {10.1145/1900008.1900067},
abstract = {Relational databases have been around for many decades and are the database technology
of choice for most traditional data-intensive storage and retrieval applications.
Retrievals are usually accomplished using SQL, a declarative query language. Relational
database systems are generally efficient unless the data contains many relationships
requiring joins of large tables. Recently there has been much interest in data stores
that do not use SQL exclusively, the so-called NoSQL movement. Examples are Google's
BigTable and Facebook's Cassandra. This paper reports on a comparison of one such
NoSQL graph database called Neo4j with a common relational database system, MySQL,
for use as the underlying technology in the development of a software system to record
and query data provenance information.},
booktitle = {Proceedings of the 48th Annual Southeast Regional Conference},
articleno = {42},
numpages = {6},
location = {Oxford, Mississippi},
series = {ACM SE '10}
}

@inproceedings{10.1145/2076623.2076626,
author = {McGlothlin, James P. and Khan, Latifur},
title = {Scalable Queries for Large Datasets Using Cloud Computing: A Case Study},
year = {2011},
isbn = {9781450306270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2076623.2076626},
doi = {10.1145/2076623.2076626},
abstract = {Cloud computing is rapidly growing in popularity as a solution for processing and
retrieving huge amounts of data over clusters of inexpensive commodity hardware. The
most common data model utilized by cloud computing software is the NoSQL data model.
While this data model is extremely scalable, it is much more efficient for simple
retrievals and scans than for the complex analytical queries typical in a relational
database model. In this paper, we evaluate emerging cloud computing technologies using
a representative use case. Our use case involves analyzing telecommunications logs
for performance monitoring and quality assurance. Clearly, the size of such logs is
growing exponentially as more devices communicate more frequently and the amount of
data being transferred steadily increases. We analyze potential solutions to provide
a scalable database which supports both retrieval and analysis. We will investigate
and analyze all the major open source cloud computing solutions and designs. We then
choose the most applicable subset of these technologies for experimentation. We provide
a performance evaluation of these products, and we analyze our results and make recommendations.
This paper provides a comprehensive survey of technologies for scalable data processing
and an in-depth performance evaluation of these technologies.},
booktitle = {Proceedings of the 15th Symposium on International Database Engineering &amp; Applications},
pages = {8–16},
numpages = {9},
keywords = {cloud computing},
location = {Lisboa, Portugal},
series = {IDEAS '11}
}

@inproceedings{10.1145/3035918.3054784,
author = {\"{O}zcan, Fatma and Tian, Yuanyuan and T\"{o}z\"{u}n, Pinar},
title = {Hybrid Transactional/Analytical Processing: A Survey},
year = {2017},
isbn = {9781450341974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3035918.3054784},
doi = {10.1145/3035918.3054784},
abstract = {The popularity of large-scale real-time analytics applications (real-time inventory/pricing,
recommendations from mobile apps, fraud detection, risk analysis, IoT, etc.) keeps
rising. These applications require distributed data management systems that can handle
fast concurrent transactions (OLTP) and analytics on the recent data. Some of them
even need running analytical queries (OLAP) as part of transactions. Efficient processing
of individual transactional and analytical requests, however, leads to different optimizations
and architectural decisions while building a data management system.For the kind of
data processing that requires both analytics and transactions, Gartner recently coined
the term Hybrid Transactional/Analytical Processing (HTAP). Many HTAP solutions are
emerging both from the industry as well as academia that target these new applications.
While some of these are single system solutions, others are a looser coupling of OLTP
databases or NoSQL systems with analytical big data platforms, like Spark. The goal
of this tutorial is to 1-) quickly review the historical progression of OLTP and OLAP
systems, 2-) discuss the driving factors for HTAP, and finally 3-) provide a deep
technical analysis of existing and emerging HTAP solutions, detailing their key architectural
differences and trade-offs.},
booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
pages = {1771–1775},
numpages = {5},
keywords = {transactions, htap, analytics, oltp, hybrid transaction and analytics processing, olap},
location = {Chicago, Illinois, USA},
series = {SIGMOD '17}
}

@article{10.1145/2687880,
author = {Richardson, Rick},
title = {Disambiguating Databases},
year = {2014},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {58},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/2687880},
doi = {10.1145/2687880},
abstract = {Use the database built for your access model.},
journal = {Commun. ACM},
month = dec,
pages = {54–61},
numpages = {8}
}

@inproceedings{10.1145/2882903.2912569,
author = {Athanassoulis, Manos and Idreos, Stratos},
title = {Design Tradeoffs of Data Access Methods},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2912569},
doi = {10.1145/2882903.2912569},
abstract = {Database researchers and practitioners have been building methods to store, access,
and update data for more than five decades. Designing access methods has been a constant
effort to adapt to the ever changing underlying hardware and workload requirements.
The recent explosion in data system designs - including, in addition to traditional
SQL systems, NoSQL, NewSQL, and other relational and non-relational systems - makes
understanding the tradeoffs of designing access methods more important than ever.
Access methods are at the core of any new data system. In this tutorial we survey
recent developments in access method design and we place them in the design space
where each approach focuses primarily on one or a subset of read performance, update
performance, and memory utilization. We discuss how to utilize designs and lessons-learned
from past research. In addition, we discuss new ideas on how to build access methods
that have tunable behavior, as well as, what is the scenery of open research problems.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {2195–2200},
numpages = {6},
keywords = {design tradeoffs, log-structure design, logarithmic structure, space-efficient indexing, access methods, rum tradeoffs, data skipping, read-optimized indexing, continuous reorganization, differential updates, update-optimized indexing, approximate indexing, cache optimizations},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@inproceedings{10.1145/2741948.2741975,
author = {Viennot, Nicolas and L\'{e}cuyer, Mathias and Bell, Jonathan and Geambasu, Roxana and Nieh, Jason},
title = {Synapse: A Microservices Architecture for Heterogeneous-Database Web Applications},
year = {2015},
isbn = {9781450332385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2741948.2741975},
doi = {10.1145/2741948.2741975},
abstract = {The growing demand for data-driven features in today's Web applications -- such as
targeting, recommendations, or predictions -- has transformed those applications into
complex conglomerates of services operating on each others' data without a coherent,
manageable architecture. We present Synapse, an easy-to-use, strong-semantic system
for large-scale, data-driven Web service integration. Synapse lets independent services
cleanly share data with each other in an isolated and scalable way. The services run
on top of their own databases, whose layouts and engines can be completely different,
and incorporate read-only views of each others' shared data. Synapse synchronizes
these views in real-time using a new scalable, consistent replication mechanism that
leverages the high-level data models in popular MVC-based Web applications to replicate
data across heterogeneous databases. We have developed Synapse on top of the popular
Web framework Ruby-on-Rails. It supports data replication among a wide variety of
SQL and NoSQL databases, including MySQL, Oracle, PostgreSQL, MongoDB, Cassandra,
Neo4j, and Elasticsearch. We and others have built over a dozen microservices using
Synapse with great ease, some of which are running in production with over 450,000
users.},
booktitle = {Proceedings of the Tenth European Conference on Computer Systems},
articleno = {21},
numpages = {16},
location = {Bordeaux, France},
series = {EuroSys '15}
}

@inproceedings{10.1145/3035918.3035938,
author = {Xu, Lianghong and Pavlo, Andrew and Sengupta, Sudipta and Ganger, Gregory R.},
title = {Online Deduplication for Databases},
year = {2017},
isbn = {9781450341974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3035918.3035938},
doi = {10.1145/3035918.3035938},
abstract = {dbDedup is a similarity-based deduplication scheme for on-line database management
systems (DBMSs). Beyond block-level compression of individual database pages or operation
log (oplog) messages, as used in today's DBMSs, dbDedup uses byte-level delta encoding
of individual records within the database to achieve greater savings. dbDedup's single-pass
encoding method can be integrated into the storage and logging components of a DBMS
to provide two benefits: (1) reduced size of data stored on disk beyond what traditional
compression schemes provide, and (2) reduced amount of data transmitted over the network
for replication services. To evaluate our work, we implemented dbDedup in a distributed
NoSQL DBMS and analyzed its properties using four real datasets. Our results show
that dbDedup achieves up to 37x reduction in the storage size and replication traffic
of the database on its own and up to 61x reduction when paired with the DBMS's block-level
compression. dbDedup provides both benefits with negligible effect on DBMS throughput
or client latency (average and tail).},
booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
pages = {1355–1368},
numpages = {14},
keywords = {databases, deduplication, delta compression, replication},
location = {Chicago, Illinois, USA},
series = {SIGMOD '17}
}

@inproceedings{10.1145/3234698.3234723,
author = {El Bousty, Hicham and krit, Salah-ddine and Elasikri, Mohamed and Dani, Hassan and Karimi, Khaoula and Bendaoud, Kaoutar and Kabrane, Mustapha},
title = {Investigating Business Intelligence in the Era of Big Data: Concepts, Benefits and Challenges},
year = {2018},
isbn = {9781450363921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234698.3234723},
doi = {10.1145/3234698.3234723},
abstract = {Business intelligence suppose retrieving value from data floating in the organization
environment. It provides methods and tools for collecting, storing, formatting and
analyzing data for the purpose of helping managers in decision-making. At the start,
only data from enterprise internal activities were examined. Now and in this turbulent
business environment, organizations should incorporate analysis of the huge amount
of external data gathered from multifarious sources. It is argued that BI systems
accuracy depends on quantity of data at their disposal, yet some storage and analysis
methods are phased out and should be reviewed by academics and practitioners.This
paper presents an overview of BI challenges in the context of Big Data (BD) and some
available solutions provided, either by using Cloud Computing (CC) or improving Data
Warehouse (DW) efficiency.},
booktitle = {Proceedings of the Fourth International Conference on Engineering &amp; MIS 2018},
articleno = {25},
numpages = {9},
keywords = {Big Data, Business Intelligence, Cloud Computing, Data Warehouse},
location = {Istanbul, Turkey},
series = {ICEMIS '18}
}

@inproceedings{10.1145/3297280.3297354,
author = {Satti, Fahad Ahmed and Khan, Wajahat Ali and Lee, Ganghun and Khattak, Asad Masood and Lee, Sungyoung},
title = {Resolving Data Interoperability in Ubiquitous Health Profile Using Semi-Structured Storage and Processing},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297354},
doi = {10.1145/3297280.3297354},
abstract = {Advancements in the field of healthcare information management have led to the development
of a plethora of software, medical devices and standards. As a consequence, the rapid
growth in quantity and quality of medical data has compounded the problem of heterogeneity;
thereby decreasing the effectiveness and increasing the cost of diagnostics, treatment
and follow-up. However, this problem can be resolved by using a semi-structured data
storage and processing engine, which can extract semantic value from a large volume
of patient data, produced by a variety of data sources, at variable rates and conforming
to different abstraction levels. Going beyond the traditional relational model and
by re-purposing state-of-the-art tools and technologies, we present, the Ubiquitous
Health Profile (UHPr), which enables a semantic solution to the data interoperability
problem, in the domain of healthcare1.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {762–770},
numpages = {9},
keywords = {text tagging, ACM proceedings},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@article{10.14778/3352063.3352145,
author = {Abouzied, Azza and Abadi, Daniel J. and Bajda-Pawlikowski, Kamil and Silberschatz, Avi},
title = {Integration of Large-Scale Data Processing Systems and Traditional Parallel Database Technology},
year = {2019},
issue_date = {August 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3352063.3352145},
doi = {10.14778/3352063.3352145},
abstract = {In 2009 we explored the feasibility of building a hybrid SQL data analysis system
that takes the best features from two competing technologies: large-scale data processing
systems (such as Google MapReduce and Apache Hadoop) and parallel database management
systems (such as Greenplum and Vertica). We built a prototype, HadoopDB, and demonstrated
that it can deliver the high SQL query performance and efficiency of parallel database
management systems while still providing the scalability, fault tolerance, and flexibility
of large-scale data processing systems. Subsequently, HadoopDB grew into a commercial
product, Hadapt, whose technology was eventually acquired by Teradata. In this paper,
we provide an overview of HadoopDB's original design, and its evolution during the
subsequent ten years of research and development effort. We describe how the project
innovated both in the research lab, and as a commercial product at Hadapt and Teradata.
We then discuss the current vibrant ecosystem of software projects (most of which
are open source) that continued HadoopDB's legacy of implementing a systems level
integration of large-scale data processing systems and parallel database technology.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {2290–2299},
numpages = {10}
}

@inproceedings{10.1145/2790755.2790762,
author = {Ceci, Michelangelo and Corizzo, Roberto and Fumarola, Fabio and Ianni, Michele and Malerba, Donato and Maria, Gaspare and Masciari, Elio and Oliverio, Marco and Rashkovska, Aleksandra},
title = {Big Data Techniques For Supporting Accurate Predictions of Energy Production From Renewable Sources},
year = {2015},
isbn = {9781450334143},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2790755.2790762},
doi = {10.1145/2790755.2790762},
abstract = {Predicting the output power of renewable energy production plants distributed on a
wide territory is a really valuable goal, both for marketing and energy management
purposes. Vi-POC (Virtual Power Operating Center) project aims at designing and implementing
a prototype which is able to achieve this goal. Due to the heterogeneity and the high
volume of data, it is necessary to exploit suitable Big Data analysis techniques in
order to perform a quick and secure access to data that cannot be obtained with traditional
approaches for data management. In this paper, we describe Vi-POC -- a distributed
system for storing huge amounts of data, gathered from energy production plants and
weather prediction services. We use HBase over Hadoop framework on a cluster of commodity
servers in order to provide a system that can be used as a basis for running machine
learning algorithms. Indeed, we perform one-day ahead forecast of PV energy production
based on Artificial Neural Networks in two learning settings, that is, structured
and non-structured output prediction. Preliminary experimental results confirm the
validity of the approach, also when compared with a baseline approach.},
booktitle = {Proceedings of the 19th International Database Engineering &amp; Applications Symposium},
pages = {62–71},
numpages = {10},
location = {Yokohama, Japan},
series = {IDEAS '15}
}

@article{10.1007/s00779-016-0980-2,
author = {Liu, Yingjian and Qiu, Meng and Liu, Chao and Guo, Zhongwen},
title = {Big Data Challenges in Ocean Observation: A Survey},
year = {2017},
issue_date = {February  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {1},
issn = {1617-4909},
url = {https://doi.org/10.1007/s00779-016-0980-2},
doi = {10.1007/s00779-016-0980-2},
abstract = {Ocean observation plays an essential role in ocean exploration. Ocean science is entering
into big data era with the exponentially growth of information technology and advances
in ocean observatories. Ocean observatories are collections of platforms capable of
carrying sensors to sample the ocean over appropriate spatiotemporal scales. Data
collected by these platforms help answer a range of fundamental and applied research
questions. Many countries are spending considerable amount of resources on ocean observing
programs for various purposes. Given the huge volume, diverse types, sustained measurement,
and potential uses of ocean observing data, it is a typical kind of big data, namely
marine big data. The traditional data-centric infrastructure is insufficient to deal
with new challenges arising in ocean science. New distributed, large-scale modern
infrastructure backbone is urgently required. This paper discusses some possible strategies
to solve marine big data challenges in the phases of data storage, data computing,
and analysis. Some applications in physics, chemistry, geology, and biology illustrate
the significant uses of marine big data. Finally, we highlight some challenges and
key issues in marine big data.},
journal = {Personal Ubiquitous Comput.},
month = feb,
pages = {55–65},
numpages = {11},
keywords = {Data analysis, Ocean observation, Marine big data, Data storage, Data computing}
}

@inproceedings{10.1145/2897845.2897852,
author = {Yuan, Xingliang and Wang, Xinyu and Wang, Cong and Qian, Chen and Lin, Jianxiong},
title = {Building an Encrypted, Distributed, and Searchable Key-Value Store},
year = {2016},
isbn = {9781450342339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897845.2897852},
doi = {10.1145/2897845.2897852},
abstract = {Modern distributed key-value stores are offering superior performance, incremental
scalability, and fine availability for data-intensive computing and cloud-based applications.
Among those distributed data stores, the designs that ensure the confidentiality of
sensitive data, however, have not been fully explored yet. In this paper, we focus
on designing and implementing an encrypted, distributed, and searchable key-value
store. It achieves strong protection on data privacy while preserving all the above
prominent features of plaintext systems. We first design a secure data partition algorithm
that distributes encrypted data evenly across a cluster of nodes. Based on this algorithm,
we propose a secure transformation layer that supports multiple data models in a privacy-preserving
way, and implement two basic APIs for the proposed encrypted key-value store. To enable
secure search queries for secondary attributes of data, we leverage searchable symmetric
encryption to design the encrypted secondary indexes which consider security, efficiency,
and data locality simultaneously, and further enable secure query processing in parallel.
For completeness, we present formal security analysis to demonstrate the strong security
strength of the proposed designs. We implement the system prototype and deploy it
to a cluster at Microsoft Azure. Comprehensive performance evaluation is conducted
in terms of Put/Get throughput, Put/Get latency under different workloads, system
scaling cost, and secure query performance. The comparison with Redis shows that our
prototype can function in a practical manner.},
booktitle = {Proceedings of the 11th ACM on Asia Conference on Computer and Communications Security},
pages = {547–558},
numpages = {12},
keywords = {searchable encryption, key-value store},
location = {Xi'an, China},
series = {ASIA CCS '16}
}

@inproceedings{10.1145/2457317.2457323,
author = {Ji, Yuanzhen},
title = {Database Support for Processing Complex Aggregate Queries over Data Streams},
year = {2013},
isbn = {9781450315999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2457317.2457323},
doi = {10.1145/2457317.2457323},
abstract = {Over the last few years, the increasing demand on processing streaming data with high
throughput and low latency has led to the development of specialized stream processing
engines (SPE). Although existing SPEs show high performance in evaluating stateless
operations and stateful operations with small windows, their performance degrades
significantly when calculating exact answers for complex aggregate queries with huge
windows. Examples include correlated aggregations, quantile and ordering statistic
computation. Meanwhile, modern database systems have demonstrated the ability of processing
complex analytical tasks efficiently over very large datasets, using technologies
such as vertical storage, vectorized query execution, etc. This suggests the feasibility
of leveraging database systems to assist SPEs to process complex aggregate queries
to reduce their evaluation latency.The goal of this thesis is to investigate the potential
of combining database systems with SPEs in the context of stream processing so as
to improve the overall query evaluation performance. To this end, the following two
major topics will be addressed in this thesis: (1) dynamic migration of complex aggregate
operations between the SPE and the database in response to varying system load and
(2) efficient evaluation of continuous queries over streaming data that is migrated
to the database.},
booktitle = {Proceedings of the Joint EDBT/ICDT 2013 Workshops},
pages = {31–37},
numpages = {7},
location = {Genoa, Italy},
series = {EDBT '13}
}

@inproceedings{10.1145/3216122.3216152,
author = {Leclercq, \'{E}ric and Savonnet, Marinette},
title = {A Tensor Based Data Model for Polystore: An Application to Social Networks Data},
year = {2018},
isbn = {9781450365277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3216122.3216152},
doi = {10.1145/3216122.3216152},
abstract = {In this article, we show how the mathematical object tensor can be used to build a
multi-paradigm model for the storage of social data in data warehouses. From an architectural
point of view, our approach allows to link different storage systems (polystore) and
limits the impact of ETL tools performing model transformations required to feed different
analysis algorithms. Therefore, systems can take advantage of multiple data models
both in terms of query execution performance and the semantic expressiveness of data
representation. The proposed model allows to reach the logical independence between
data and programs implementing analysis algorithms. With a concrete case study on
message virality on Twitter during the French presidential election of 2017, we highlight
some of the contributions of our model.},
booktitle = {Proceedings of the 22nd International Database Engineering &amp; Applications Symposium},
pages = {110–118},
numpages = {9},
keywords = {Tensor, Polystore, OLAP, Multi-relational Networks, Multi-paradigm Storage, Associative Array},
location = {Villa San Giovanni, Italy},
series = {IDEAS 2018}
}

@article{10.1007/s00779-014-0784-1,
author = {Tang, Yu and Fan, Aihua and Wang, Yingjie and Yao, Yuanzhe},
title = {MDHT: A Multi-Level-Indexed DHT Algorithm to Extra-Large-Scale Data Retrieval on HDFS/Hadoop Architecture},
year = {2014},
issue_date = {December  2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {8},
issn = {1617-4909},
url = {https://doi.org/10.1007/s00779-014-0784-1},
doi = {10.1007/s00779-014-0784-1},
abstract = {Corresponding to the storing and fast searching needs of an extra-large scale of energy
monitoring and statistics data, we propose a multi-level-indexed distributed hash
table (mDHT) algorithm and complete a MapReduce implementation of the algorithm on
the open-standard HDFS/Hbase platform. Such an approach uses a columnar storage structure
for energy consumption data storage and creates a hashed index table to provide a
quick search and retrieval method for extra-large-scale data processing systems. Such
a hashed indexing scheme is implemented on a 3-node Hadoop cluster, and the simulation
experiments at a scale up to 48 million data records indicate that, when the data
volume reaches the scale of 12 million to 48 millions, the proposed mDHT algorithm
presents an outstanding performance in data writing operation, compared to that of
traditional SQL Server implementation. Even compared to the single-indexed DHT (sDHT)
application, the mDHT solution outperforms by reducing the data retrieval time by
24.5---48.6 %. The multi-level-indexed DHT algorithm presented in this paper contributes
a key technique to developing a fast search engine to the extra-large scale of data
on the cloud storage architecture.},
journal = {Personal Ubiquitous Comput.},
month = dec,
pages = {1835–1844},
numpages = {10},
keywords = {Distributed hash table, Data retrieval, Algorithm, Cloud storage, Multi-level index}
}

@inproceedings{10.1145/3372454.3372455,
author = {Gaoyu, Jiang and Jingchang, Pan and Bo, Zhang},
title = {Storage Design and Implementation of Information Reconstruction System},
year = {2019},
isbn = {9781450372015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372454.3372455},
doi = {10.1145/3372454.3372455},
abstract = {With the development and popularization of the Internet and mobile devices, the production
of information has grown into blowouts. The proliferation of data in different content
provides sufficient conditions for big data analysis, and it also puts forward new
requirements. The information reconstruction system aims to use natural language processing
tools and information extraction algorithms to reconstruct the unstructured original
information into structured information entities by extracting event elements according
to the reconstruction model, and form a complete entity information database and connections
database, which provides users structured data reconstruction, storage and query processing
results. This paper proposes a reconstruction model of information objects. The model
includes the characteristics of information objects evolved over time. It also includes
the correlation and interaction between different but related information objects.
A set of distributed storage based on Hbase is designed in the paper. The information
reconstruction system uses the big data framework to provide efficient and stable
storage for extracting entity information.},
booktitle = {Proceedings of the 2019 3rd International Conference on Big Data Research},
pages = {1–5},
numpages = {5},
keywords = {Time evolution, Big data, Hbase, Information reconstruction, distributed system},
location = {Cergy-Pontoise, France},
series = {ICBDR 2019}
}

@inproceedings{10.1145/2948992.2949024,
author = {Santos, Maribel Yasmina and Costa, Carlos},
title = {Data Warehousing in Big Data: From Multidimensional to Tabular Data Models},
year = {2016},
isbn = {9781450340755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2948992.2949024},
doi = {10.1145/2948992.2949024},
abstract = {Data warehouses are central pieces in business intelligence and analytics as these
repositories ensure proper data storage and querying, being supported by data models
that allow the analysis of data by different perspectives. Those perspectives support
users and organizations in the decision-making process. In Big Data environments,
Hive is used as a distributed storage mechanism that provides data warehousing capabilities.
Its data schemas are defined attending to the analytical requirements specified by
the users. In this work, multidimensional data models are used as the source of those
requirements, allowing the automatic transformation of a multidimensional schema into
a tabular schema suited to be implemented in Hive. To achieve this objective, a set
of rules is proposed and tested in a demonstration case, showing the applicability
and usefulness of the proposed approach.},
booktitle = {Proceedings of the Ninth International C* Conference on Computer Science &amp; Software Engineering},
pages = {51–60},
numpages = {10},
keywords = {Big Data, Hive, Analytical Data Model, Data Warehousing},
location = {Porto, Portugal},
series = {C3S2E '16}
}

@inproceedings{10.5555/2555523.2555559,
author = {Statchuk, Craig and Iles, Michael and Thomas, Fenny},
title = {Big Data and Analytics},
year = {2013},
publisher = {IBM Corp.},
address = {USA},
abstract = {Business Analytics is maturing and moving towards mass adoption. The emergence of
big data increases the need for innovative tools and methodologies. Of particular
interest is the established Business Intelligence market segment, built on structured
data and reporting. How does big data affect methodologies like ETL, modeling and
report authoring? Business Intelligence is at a crossroads between less formal data
analysis at scale and business imperatives like regulatory reporting that runs an
enterprise. This paper highlights new technologies and services that move the methodologies
of old into the data-centric world of high volume and velocity that defines the modern
information landscape.},
booktitle = {Proceedings of the 2013 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {341–343},
numpages = {3},
location = {Ontario, Canada},
series = {CASCON '13}
}

@inproceedings{10.1145/2882903.2903739,
author = {Antova, Lyublena and Baldwin, Rhonda and Bryant, Derrick and Cao, Tuan and Duller, Michael and Eshleman, John and Gu, Zhongxian and Shen, Entong and Soliman, Mohamed A. and Waas, F. Michael},
title = {Datometry Hyper-Q: Bridging the Gap Between Real-Time and Historical Analytics},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2903739},
doi = {10.1145/2882903.2903739},
abstract = {Wall Street's trading engines are complex database applications written for time series
databases like kdb+ that uses the query language Q to perform real-time analysis.
Extending the models to include other data sources, e.g., historic data, is critical
for backtesting and compliance. However, Q applications cannot run directly on SQL
databases. Therefore, financial institutions face the dilemma of either maintaining
two separate application stacks, one written in Q and the other in SQL, which means
increased IT cost and increased risk, or migrating all Q applications to SQL, which
results in losing the inherent competitive advantage on Q real-time processing. Neither
solution is desirable as both alternatives are costly, disruptive, and suboptimal.
In this paper we present Hyper-Q, a data virtualization plat- form that overcomes
the chasm. Hyper-Q enables Q applications to run natively on PostgreSQL-compatible
databases by translating queries and results on the fly. We outline the basic concepts,
detail specific difficulties, and demonstrate the viability of the approach with a
case study.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {1405–1416},
numpages = {12},
keywords = {data virtualization, query processing, financial services, data analytics, big data},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@article{10.1145/2963143,
author = {Khalifa, Shadi and Elshater, Yehia and Sundaravarathan, Kiran and Bhat, Aparna and Martin, Patrick and Imam, Fahim and Rope, Dan and Mcroberts, Mike and Statchuk, Craig},
title = {The Six Pillars for Building Big Data Analytics Ecosystems},
year = {2016},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/2963143},
doi = {10.1145/2963143},
abstract = {With almost everything now online, organizations look at the Big Data collected to
gain insights for improving their services. In the analytics process, derivation of
such insights requires experimenting-with and integrating different analytics techniques,
while handling the Big Data high arrival velocity and large volumes. Existing solutions
cover bits-and-pieces of the analytics process, leaving it to organizations to assemble
their own ecosystem or buy an off-the-shelf ecosystem that can have unnecessary components
to them. We build on this point by dividing the Big Data Analytics problem into six
main pillars. We characterize and show examples of solutions designed for each of
these pillars. We then integrate these six pillars into a taxonomy to provide an overview
of the possible state-of-the-art analytics ecosystems. In the process, we highlight
a number of ecosystems to meet organizations different needs. Finally, we identify
possible areas of research for building future Big Data Analytics Ecosystems.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {33},
numpages = {36},
keywords = {consumable analytics, Orchestration, analytics talent gap}
}

@inproceedings{10.1145/2723372.2735380,
author = {Yuan, Li-Yan and Wu, Lengdong and You, Jia-Huai and Chi, Yan},
title = {A Demonstration of Rubato DB: A Highly Scalable NewSQL Database System for OLTP and Big Data Applications},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723372.2735380},
doi = {10.1145/2723372.2735380},
abstract = {We propose to demonstrate Rubato DB, a highly scalable NewSQL system, supporting various
consistency levels from ACID to BASE for OLTP and big data applications. Rubato DB
employs the staged grid architecture with a novel formula based protocol for distributed
concurrency control. Our demonstration will present Rubato DB as one NewSQL database
management system running on a collection of commodity servers against two of benchmark
sets.The demo attendees can modify the configuration of system size, fine-tune the
query workload, and visualize the performance on the fly by the graphical user interface.
Attendees can experiment with various system scales, and thus grasp the potential
scalability of Rubato DB, whose performance, with the increase of the number of servers
used, can achieve a linear growth for both OLTP application with the strong consistency
properties and key-value storage applications with the weak consistency properties.},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
pages = {907–912},
numpages = {6},
keywords = {scalability, base, acid, concurrency control, architecture},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15}
}

@article{10.1145/3003665.3003674,
author = {Pavlo, Andrew and Aslett, Matthew},
title = {What's Really New with NewSQL?},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/3003665.3003674},
doi = {10.1145/3003665.3003674},
abstract = {A new class of database management systems (DBMSs) called NewSQL tout their ability
to scale modern on-line transaction processing (OLTP) workloads in a way that is not
possible with legacy systems. The term NewSQL was first used by one of the authors
of this article in a 2011 business analysis report discussing the rise of new database
systems as challengers to these established vendors (Oracle, IBM, Microsoft). The
other author was working on what became one of the first examples of a NewSQL DBMS.
Since then several companies and research projects have used this term (rightly and
wrongly) to describe their systems.Given that relational DBMSs have been around for
over four decades, it is justifiable to ask whether the claim of NewSQL's superiority
is actually true or whether it is simply marketing. If they are indeed able to get
better performance, then the next question is whether there is anything scientifically
new about them that enables them to achieve these gains or is it just that hardware
has advanced so much that now the bottlenecks from earlier years are no longer a problem.To
do this, we first discuss the history of databases to understand how NewSQL systems
came about. We then provide a detailed explanation of what the term NewSQL means and
the different categories of systems that fall under this definition.},
journal = {SIGMOD Rec.},
month = sep,
pages = {45–55},
numpages = {11}
}

@article{10.14778/2732286.2732290,
author = {Wang, Sheng and Maier, David and Ooi, Beng Chin},
title = {Lightweight Indexing of Observational Data in Log-Structured Storage},
year = {2014},
issue_date = {March 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {7},
issn = {2150-8097},
url = {https://doi.org/10.14778/2732286.2732290},
doi = {10.14778/2732286.2732290},
abstract = {Huge amounts of data are being generated by sensing devices every day, recording the
status of objects and the environment. Such observational data is widely used in scientific
research. As the capabilities of sensors keep improving, the data produced are drastically
expanding in precision and quantity, making it a write-intensive domain. Log-structured
storage is capable of providing high write throughput, and hence is a natural choice
for managing large-scale observational data.In this paper, we propose an approach
to indexing and querying observational data in log-structured storage. Based on key
traits of observational data, we design a novel index approach called the CR-index
(Continuous Range Index), which provides fast query performance without compromising
write throughput. It is a lightweight structure that is fast to construct and often
small enough to reside in RAM. Our experimental results show that the CR-index is
superior in handling observational data compared to other indexing techniques. While
our focus is scientific data, we believe our index will be effective for other applications
with similar properties, such as process monitoring in manufacturing.},
journal = {Proc. VLDB Endow.},
month = mar,
pages = {529–540},
numpages = {12}
}

@article{10.14778/3415478.3415538,
author = {Lee, Juchang and Kim, Kyu Hwan and Lee, Hyejeong and Andrei, Mihnea and Ko, Seongyun and Keller, Friedrich and Han, Wook-Shin},
title = {Asymmetric-Partition Replication for Highly Scalable Distributed Transaction Processing in Practice},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3415478.3415538},
doi = {10.14778/3415478.3415538},
abstract = {Database replication is widely known and used for high availability or load balancing
in many practical database systems. In this paper, we show how a replication engine
can be used for three important practical cases that have not previously been studied
very well. The three practical use cases include: 1) scaling out OLTP/OLAP-mixed workloads
with partitioned replicas, 2) efficiently maintaining a distributed secondary index
for a partitioned table, and 3) efficiently implementing an online re-partitioning
operation. All three use cases are crucial for enabling a high-performance shared-nothing
distributed database system. To support the three use cases more efficiently, we propose
the concept of asymmetric-partition replication, so that replicas of a table can be
independently partitioned regardless of whether or how its primary copy is partitioned.
In addition, we propose the optimistic synchronous commit protocol which avoids the
expensive two-phase commit without sacrificing transactional consistency. The proposed
asymmetric-partition replication and its optimized commit protocol are incorporated
in the production versions of the SAP HANA in-memory database system. Through extensive
experiments, we demonstrate the significant benefits that the proposed replication
engine brings to the three use cases.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3112–3124},
numpages = {13}
}

@article{10.1145/3337065,
author = {Dai, Hong-Ning and Wong, Raymond Chi-Wing and Wang, Hao and Zheng, Zibin and Vasilakos, Athanasios V.},
title = {Big Data Analytics for Large-Scale Wireless Networks: Challenges and Opportunities},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3337065},
doi = {10.1145/3337065},
abstract = {The wide proliferation of various wireless communication systems and wireless devices
has led to the arrival of big data era in large-scale wireless networks. Big data
of large-scale wireless networks has the key features of wide variety, high volume,
real-time velocity, and huge value leading to the unique research challenges that
are different from existing computing systems. In this article, we present a survey
of the state-of-art big data analytics (BDA) approaches for large-scale wireless networks.
In particular, we categorize the life cycle of BDA into four consecutive stages: Data
Acquisition, Data Preprocessing, Data Storage, and Data Analytics. We then present
a detailed survey of the technical solutions to the challenges in BDA for large-scale
wireless networks according to each stage in the life cycle of BDA. Moreover, we discuss
the open research issues and outline the future directions in this promising area.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {99},
numpages = {36},
keywords = {machine learning, Big data, wireless networks}
}

@inproceedings{10.1145/2428736.2428764,
author = {Hu, Bo and Carvalho, Nuno and Laera, Loredana and Matsutsuka, Takahide},
title = {Towards Big Linked Data: A Large-Scale, Distributed Semantic Data Storage},
year = {2012},
isbn = {9781450313063},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2428736.2428764},
doi = {10.1145/2428736.2428764},
abstract = {In light of the challenges of effectively managing Big Data, we are witnessing a gradual
shift towards the increasingly popular Linked Open Data (LOD) paradigm. LOD aims to
impose a machine-readable semantic layer over structured as well as unstructured data
and hence automate some data analysis tasks that are not designed for computers. The
convergence of Big Data and LOD is, however, not straightforward: the semantic layer
of LOD and the Big Data large scale storage do not get along easily. Meanwhile, the
sheer data size envisioned by Big Data denies certain computationally expensive semantic
technologies, rendering the latter much less efficient than their performance on relatively
small data sets.In this paper, we propose a mechanism allowing LOD to take advantage
of existing large-scale data stores while sustaining its "semantic" nature. We demonstrate
how RDF-based semantic models can be distributed across multiple storage servers and
we examine how a fundamental semantic operation can be tuned to meet the requirements
on distributed and parallel data processing. Our future work will focus on stress
test of the platform in the magnitude of tens of billions of triples, as well as comparative
studies in usability and performance against similar offerings.},
booktitle = {Proceedings of the 14th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {167–176},
numpages = {10},
keywords = {distributed data reconciliation, fault tolerance, graph storage, key-value stores, RDF store},
location = {Bali, Indonesia},
series = {IIWAS '12}
}

@inproceedings{10.1145/2367589.2367594,
author = {Nakamura, Shunsuke and Shudo, Kazuyuki},
title = {MyCassandra: A Cloud Storage Supporting Both Read Heavy and Write Heavy Workloads},
year = {2012},
isbn = {9781450314480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2367589.2367594},
doi = {10.1145/2367589.2367594},
abstract = {A cloud storage with persistence shows solid performance only with a read heavy or
write heavy workload. There is a trade-off between the read-optimized and write-optimized
design of a cloud storage. This is dominated by its storage engine, which is a software
component for managing data stored on memory and disk. A storage engine can be pluggable
with an adequate software design though today's cloud storages are not always modular.
We developed a modular cloud storage called MyCassandra to demonstrate that such a
cloud storage can be read-optimized and write-optimized with a modular design. Various
storage engines can be introduced into MyCassandra and they determine with what workload
the cloud storage can perform well. With MyCassandra we proved that such a modular
design enables a cloud storage to adapt to workloads.We propose a method to build
a cloud storage that performs well with both read heavy and write heavy workloads.
A heterogeneous cluster is built from MyCassandra nodes with different storage engines,
read-optimized, write-optimized, and on-memory (read-and-write-optimized). A query
is routed to nodes that efficiently process it while the cluster maintains consistency
between data replicas with a quorum protocol. The cluster showed comparable performance
with the original Cassandra for write heavy workloads, and it showed considerably
better performance for read heavy workloads. With read-only workload, read latency
was 90.4% lower than and throughput was 11.00 times as high as Cassandra.},
booktitle = {Proceedings of the 5th Annual International Systems and Storage Conference},
articleno = {14},
numpages = {9},
keywords = {distributed systems, performance, cloud storage},
location = {Haifa, Israel},
series = {SYSTOR '12}
}

@inproceedings{10.1145/3358505.3358512,
author = {Demchenko, Yuri},
title = {Big Data Platforms and Tools for Data Analytics in the Data Science Engineering Curriculum},
year = {2019},
isbn = {9781450371650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358505.3358512},
doi = {10.1145/3358505.3358512},
abstract = {This paper presents experiences of development and teaching courses on Big Data Infrastructure
Technologies for Data Analytics (BDIT4DA) as a part of the general Data Science curricula.
The authors built the discussed course based on the EDISON Data Science Framework
(EDSF), in particular, Data Science Body of Knowledge (DS-BoK) related to Data Science
Engineering knowledge area group (KAG-DSENG). The paper provides overview of the cloud
based platforms and tools for Big Data Analytics and stresses importance of including
into curriculum the practical work with clouds for future graduates or specialists
workplace adaptability. The paper discusses a relationship between the DSENG BoK and
Big Data technologies and platforms, in particular Hadoop based applications and tools
for data analytics that should be promoted through all course activities: lectures,
practical activities and self-study.},
booktitle = {Proceedings of the 2019 3rd International Conference on Cloud and Big Data Computing},
pages = {60–64},
numpages = {5},
keywords = {Data Science Engineering, Big Data Infrastructure Technologies, Cloud Computing, EDISON Data Science Framework (EDSF), Data Science Body of Knowledge (DS-BoK), Hadoop ecosystem},
location = {Oxford, United Kingdom},
series = {ICCBDC 2019}
}

@article{10.1145/2590989.2591001,
author = {Darmont, J\'{e}r\^{o}me and Pedersen, Torben Bach},
title = {Report on the First International Workshop on Cloud Intelligence (Cloud-I 2012)},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/2590989.2591001},
doi = {10.1145/2590989.2591001},
journal = {SIGMOD Rec.},
month = feb,
pages = {67–69},
numpages = {3}
}

@inproceedings{10.1145/2513591.2527071,
author = {Cuzzocrea, Alfredo and Sacc\`{a}, Domenico and Ullman, Jeffrey D.},
title = {Big Data: A Research Agenda},
year = {2013},
isbn = {9781450320252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2513591.2527071},
doi = {10.1145/2513591.2527071},
abstract = {Recently, a great deal of interest for Big Data has risen, mainly driven from a widespread
number of research problems strongly related to real-life applications and systems,
such as representing, modeling, processing, querying and mining massive, distributed,
large-scale repositories (mostly being of unstructured nature). Inspired by this main
trend, in this paper we discuss three important aspects of Big Data research, namely
OLAP over Big Data, Big Data Posting, and Privacy of Big Data. We also depict future
research directions, hence implicitly defining a research agenda aiming at leading
future challenges in this research field.},
booktitle = {Proceedings of the 17th International Database Engineering &amp; Applications Symposium},
pages = {198–203},
numpages = {6},
keywords = {OLAP over big data, big data, big data posting, privacy of big data},
location = {Barcelona, Spain},
series = {IDEAS '13}
}

@inproceedings{10.1145/2628194.2628231,
author = {Ceci, Michelangelo and Cassavia, Nunziato and Corizzo, Roberto and Dicosta, Pietro and Malerba, Donato and Maria, Gaspare and Masciari, Elio and Pastura, Camillo},
title = {Innovative Power Operating Center Management Exploiting Big Data Techniques},
year = {2014},
isbn = {9781450326278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2628194.2628231},
doi = {10.1145/2628194.2628231},
abstract = {The problem of accurately predicting the energy production from renewable sources
has recently received an increasing attention from both the industrial and the research
communities. It presents several challenges, such as facing with the rate data are
provided by sensors, the heterogeneity of the data collected, power plants efficiency,
as well as uncontrollable factors, such as weather conditions and user consumption
profiles. In this paper we describe Vi-POC (Virtual Power Operating Center), a project
conceived to assist energy producers and decision makers in the energy market. In
this paper we present the Vi-POC project and how we face with challenges posed by
the specific application. The solutions we propose have roots both in big data management
and in stream data mining.},
booktitle = {Proceedings of the 18th International Database Engineering &amp; Applications Symposium},
pages = {326–329},
numpages = {4},
location = {Porto, Portugal},
series = {IDEAS '14}
}

@inproceedings{10.1145/2125636.2125646,
author = {Gao, Xiaoming and Nachankar, Vaibhav and Qiu, Judy},
title = {Experimenting Lucene Index on HBase in an HPC Environment},
year = {2011},
isbn = {9781450311571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2125636.2125646},
doi = {10.1145/2125636.2125646},
abstract = {Data intensive computing has been a major focus of scientific computing communities
in the past several years, and many technologies and systems have been developed to
efficiently store and serve terabytes or even petabytes of data. One important effort
in this direction is the HBase system. Modeled after Google's BigTable, HBase supports
reliable storage and efficient access to billions of rows of structured data. However,
it does not provide an efficient searching mechanism based on column values. To achieve
efficient search on text data, this paper proposes a searching framework based on
Lucene full-text indices implemented as HBase tables. Leveraging the distributed architecture
of HBase, we expect to get high performance and availability, and excellent scalability
and flexibility for our searching system. Our experiments are based on data from a
real digital library application and carried out on a dynamically constructed HBase
deployment in a high-performance computing (HPC) environment. We have completed system
design and data loading tasks of this project, and will cover index building and performance
tests in future work.},
booktitle = {Proceedings of the First Annual Workshop on High Performance Computing Meets Databases},
pages = {25–28},
numpages = {4},
keywords = {HBase, HPC cluster, Lucene index},
location = {Seattle, Washington, USA},
series = {HPCDB '11}
}

@article{10.1145/2070736.2070750,
author = {Badia, Antonio and Lemire, Daniel},
title = {A Call to Arms: Revisiting Database Design},
year = {2011},
issue_date = {September 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/2070736.2070750},
doi = {10.1145/2070736.2070750},
journal = {SIGMOD Rec.},
month = nov,
pages = {61–69},
numpages = {9}
}

@inbook{10.1145/3448016.3457562,
author = {Lyu, Zhenghua and Zhang, Huan Hubert and Xiong, Gang and Guo, Gang and Wang, Haozhou and Chen, Jinbao and Praveen, Asim and Yang, Yu and Gao, Xiaoming and Wang, Alexandra and Lin, Wen and Agrawal, Ashwin and Yang, Junfeng and Wu, Hao and Li, Xiaoliang and Guo, Feng and Wu, Jiang and Zhang, Jesse and Raghavan, Venkatesh},
title = {Greenplum: A Hybrid Database for Transactional and Analytical Workloads},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457562},
abstract = {Demand for enterprise data warehouse solutions to support real-time Online Transaction
Processing (OLTP) queries as well as long-running Online Analytical Processing (OLAP)
workloads is growing. Greenplum database is traditionally known as an OLAP data warehouse
system with limited ability to process OLTP workloads. In this paper, we augment Greenplum
into a hybrid system to serve both OLTP and OLAP workloads. The challenge we address
here is to achieve this goal while maintaining the ACID properties with minimal performance
overhead. In this effort, we identify the engineering and performance bottlenecks
such as the under-performing restrictive locking and the two-phase commit protocol.
Next we solve the resource contention issues between transactional and analytical
queries. We propose a global deadlock detector to increase the concurrency of query
processing. When transactions that update data are guaranteed to reside on exactly
one segment we introduce one-phase commit to speed up query processing. Our resource
group model introduces the capability to separate OLAP and OLTP workloads into more
suitable query processing mode. Our experimental evaluation on the TPC-B and CH-benCHmark
benchmarks demonstrates the effectiveness of our approach in boosting the OLTP performance
without sacrificing the OLAP performance.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {2530–2542},
numpages = {13}
}

@inproceedings{10.1145/3176258.3176944,
author = {Niculaescu, Oana-Georgiana and Ghinita, Gabriel},
title = {An Empirical Study of Differentially-Private Analytics for High-Speed Network Data},
year = {2018},
isbn = {9781450356329},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3176258.3176944},
doi = {10.1145/3176258.3176944},
booktitle = {Proceedings of the Eighth ACM Conference on Data and Application Security and Privacy},
pages = {149–151},
numpages = {3},
keywords = {high-speed networks, differential privacy},
location = {Tempe, AZ, USA},
series = {CODASPY '18}
}

@inproceedings{10.1145/3077584.3077587,
author = {Li, Chengming and Wu, Zheng and Yin, Jie},
title = {Research on Oracle-Based Integrative Storage and Management of Spatial Data},
year = {2017},
isbn = {9781450348331},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077584.3077587},
doi = {10.1145/3077584.3077587},
abstract = {An Oracle-based object-oriented vector-raster-integrated spatial database management
system was proposed in this paper based upon the combination of mature relational
spatial database storage and spatial data engine technologies in order to solve the
problem of low transferability and efficiency that exists in the storage and management
of multi-source heterogeneous spatial data. By rearranging the data transfer flow,
establishing an integrated vector and raster data model, and optimizing the spatial
data retrieval mechanism, this system enabled united storage and efficient management
of spatial data. A comparison with ArcSDE, a piece of international leading similar
software, revealed this technology's higher data transfer performance and better query
and retrieval efficiency},
booktitle = {Proceedings of the 2017 International Conference on Information System and Data Mining},
pages = {16–22},
numpages = {7},
keywords = {data transfer, Spatial data, retrieval optimization, vector and raster integration management},
location = {Charleston, SC, USA},
series = {ICISDM '17}
}

@inproceedings{10.1145/3090354.3090397,
author = {Imane, Labdaoui and Youness, Tabii},
title = {State of the Art in MapReduce: Issues and Approaches},
year = {2017},
isbn = {9781450348522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3090354.3090397},
doi = {10.1145/3090354.3090397},
abstract = {In the last years, new data sources appeared: social networks, mobile, internet of
things, open Data, etc., and therefore data are rapidly increasing. These data is
voluminous, various, and difficult to measure and analyze, which appears the concept
of Big Data. The vast amount of data makes the ETL (Extract-Transform-Load) process
heavy in data warehousing, renders the data mining process more complex, and makes
the slow loading of data in database management systems. The solution to make these
process more efficient is the use of parallelization technologies, many researchers
opt for the use of MapReduce paradigm for its flexibility and powerful. In this paper,
we provide an overview of state of the art in MapReduce research and we present its
various axis.},
booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
articleno = {42},
numpages = {5},
keywords = {DBMS, ETL, Big Data, Data mining, MapReduce},
location = {Tetouan, Morocco},
series = {BDCA'17}
}

@inproceedings{10.1145/2064959.2064962,
author = {Kazemitabar, Seyed Jalal and Banaei-Kashani, Farnoush and McLeod, Dennis},
title = {Geostreaming in Cloud},
year = {2011},
isbn = {9781450310369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2064959.2064962},
doi = {10.1145/2064959.2064962},
abstract = {In recent years, geospatial databases have been commercialized and widely exposed
to mass users. Current exponential growth in data generation and querying rates for
these data highlights the importance of efficient techniques for streaming. Traditional
database technology, which operates on persistent and less dynamic data objects does
not meet the requirements for efficient geospatial data streaming. Geostreaming, the
intersection of data stream processing and geospatial querying, is an ongoing research
focus in this area. In this paper, we describe why cloud is the most appropriate infrastructure
in which to support geospatial stream data processing. First, we argue that cloud
best fits the requirements of a large-scale geostreaming application. Second, we propose
ElaStream, a general cloud-based streaming infrastructure that enables huge parallelism
by means of the divide, conquer, and combine paradigm. Third, we examine key related
work in the data streaming and (geo)spatial database fields, and describe the challenges
ahead to build scalable cloud-based geostreaming applications.},
booktitle = {Proceedings of the 2nd ACM SIGSPATIAL International Workshop on GeoStreaming},
pages = {3–9},
numpages = {7},
keywords = {spatial databases, geostreaming, data stream processing, cloud computing},
location = {Chicago, Illinois},
series = {IWGS '11}
}

@inproceedings{10.1109/CCGRID.2017.107,
author = {Haroun, Amir and Mostefaoui, Ahmed and Dessables, Fran\c{c}ois},
title = {A Big Data Architecture for Automotive Applications: PSA Group Deployment Experience},
year = {2017},
isbn = {9781509066100},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2017.107},
doi = {10.1109/CCGRID.2017.107},
abstract = {Vehicles have become moving sensor platforms collecting huge volumes of data from
their various embedded sensors. This data has a great value for automotive manufacturers
and vehicles owners. Indeed, connected vehicles data can be used in a large broad
of automotive services ranging from safety services to well-being services (e.g. fatigue
detection). However, vehicle fleets send big volumes of data that traditional computing
and storage approaches are not able to manage efficiently. In this paper, we present
the experience of the PSA Group1 on leveraging big data in automotive context. We
describe in depth the big data architecture deployed within the PSA Group and the
underlaying technologies/products used in each component.},
booktitle = {Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {921–928},
numpages = {8},
keywords = {Big Data, Reference Architecture, Connected Vehicles},
location = {Madrid, Spain},
series = {CCGrid '17}
}

@inproceedings{10.1145/2790755.2790777,
author = {Shioi, Takamitsu and Hatano, Kenji},
title = {Query Processing Optimization Using Two Types of Storage Devices},
year = {2015},
isbn = {9781450334143},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2790755.2790777},
doi = {10.1145/2790755.2790777},
abstract = {Currently, OLAP systems capable of handling a huge amount of data tend to use two
types of storage device based on NSM and DSM, respectively.Conventional approaches
for query optimization in OLAP systems usually classify issued queries according to
their operation type, e.g., insert, delete, or update, and thereby select the data
storage device to be used. Briefly, focusing on the type of query issued to the OLAP
systems makes is possible to find another approach to improve query processing time.In
this paper, we propose a method for optimizing query processing in an OLAP system
with two types of storage device that considers the type of each query.},
booktitle = {Proceedings of the 19th International Database Engineering &amp; Applications Symposium},
pages = {154–157},
numpages = {4},
keywords = {Query Processing and Optimization, DSM, NSM},
location = {Yokohama, Japan},
series = {IDEAS '15}
}

@inproceedings{10.1145/2905055.2905099,
author = {Misra, Rachita and Panda, Bijayalaxmi and Tiwary, Mayank},
title = {Big Data and ICT Applications: A Study},
year = {2016},
isbn = {9781450339629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2905055.2905099},
doi = {10.1145/2905055.2905099},
abstract = {Big Data is used to manage the data due to their large size and complexity, because
it can't be handled with the traditional methods and the current technology or tools
used for that. Big Data mining is populated with 5 V's volume, variability, velocity,
variety, value which has the ability of retrieving important information from the
huge data storage. Now the challenge of Big Data is becoming the opportunities of
research for the next few years. Throughout the world researchers and developers are
trying to make use of the Big Data technology to extend the ICT applications from
the traditional LAN, WAN environment to Internet on cloud with Big Data. In this scenario
this paper provides and an overview of some of the ICT applications which take advantage
of data mining and analytics for big data. The paper tries to establish the wide range
of applications of big data in ICT with the currently available data mining &amp; data
analytics platforms, languages and tools. An effort has been made to analyze the challenges
faced in the different application fields. Some of the advances in the Big Data technology
research that can help solve some of these challenges in ICT applications have been
discussed in brief.},
booktitle = {Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies},
articleno = {41},
numpages = {6},
keywords = {Big data analytics, HDFS, Big data, ICT, Hadoop},
location = {Udaipur, India},
series = {ICTCS '16}
}

@inproceedings{10.1145/3141128.3141143,
author = {Fathi, F. and Abghour, N. and Ouzzif, M.},
title = {From Big Data Platforms to Smarter Solution, with Intelligent Learning: [PAV] 4 - Pave the Way for Intelligence},
year = {2017},
isbn = {9781450353434},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3141128.3141143},
doi = {10.1145/3141128.3141143},
abstract = {In today's time when data is generating by everyone at every moment, and the word
is moving so fast with exponential growth of new technologies and innovations in all
science and engineering domains, the age of big data is coming, and the potential
of learning from this huge amount of data and from different sources is undoubtedly
significant to uncover underlying structure and facilitate the development of more
intelligent solution. Intelligence is around us, and the concept of big data and learning
from it has existed since the emergence of the human being. In this article we focus
on data from; sensors, images, and text, and we incorporate the principles of human
intelligence; brain - body - environment, as a source of inspiration that allows us
to put a new concept based on big data - machine learning--domain and pave the way
for intelligent platform.},
booktitle = {Proceedings of the 2017 International Conference on Cloud and Big Data Computing},
pages = {11–16},
numpages = {6},
keywords = {Big data, intelligent solution, smart city, Hadoop, machine learning},
location = {London, United Kingdom},
series = {ICCBDC 2017}
}

@inproceedings{10.5555/2819289.2819302,
author = {Chen, Hong-Mei and Kazman, Rick and Haziyev, Serge and Hrytsay, Olha},
title = {Big Data System Development: An Embedded Case Study with a Global Outsourcing Firm},
year = {2015},
publisher = {IEEE Press},
abstract = {Big data system development is dramatically different from small (traditional, structured)
data system development. At the end of 2014, big data deployment is still scarce and
failures abound. Outsourcing has become a main strategy for many enterprises. We therefore
selected an outsourcing company who has successfully deployed big data projects for
our study. Our research results from analyzing 10 outsourced big data projects provide
a glimpse into early adopters of big data, illuminates the challenges for system development
that stem from the 5Vs of big data and crystallizes the importance of architecture
design choices and technology selection. We followed a collaborative practice research
(CPR) method to develop and validate a new method, called BDD. BDD is the first attempt
to systematically combine architecture design with data modeling approaches to address
big data system development challenges. The use of reference architectures and a technology
catalog are advancements to architecture design methods and are proving to be well-suited
for big data system architecture design and system development.},
booktitle = {Proceedings of the First International Workshop on BIG Data Software Engineering},
pages = {44–50},
numpages = {7},
keywords = {collaborative practice research, system engineering, big data, software architecture, embedded case study methodology, data system design methods},
location = {Florence, Italy},
series = {BIGDSE '15}
}

@inproceedings{10.1145/3265007.3265015,
author = {Zhang, Bin and Zhu, Guobin and Yu, Riji and Wei, Shaoyan and Peng, Ling and Fei, Dingzhou and Yu, Xuesong and Pan, Peiwen},
title = {Research on the Innovation of Trajectory Big Data in Social Governance},
year = {2018},
isbn = {9781450365741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3265007.3265015},
doi = {10.1145/3265007.3265015},
abstract = {With the development of modern society. The unprecedented prosperity of science &amp;
technology and finance. Objects formed a huge amount of track data in its movement.
The large amount of track data contains rich spatio-temporal characteristics information,
it exposes the privacy information such as the behavior characteristics, interests
and social habits of mobile objects. Through trajectory data processing technology.
It can excavate information such as human activity pattern and behavior characteristic,
urban vehicle movement characteristic, atmospheric environment change law and so on.
The large amount of track data also reveals the privacy information, such as the behavior
characteristics, interests and social habits of mobile objects, which is rich in spatio-temporal
characteristics information. This paper begins with the significance of the study
of trajectory big data. Introducing track big data acquisition mode and social application
in various fields, In the specific application With the development of modern society.
The unprecedented prosperity of science &amp; technology and finance. Objects formed a
huge amount of track data in its movement. The large amount of track data contains
rich spatio-temporal characteristics information, it exposes the privacy information
such as the behavior characteristics, interests and social habits of mobile objects.
Through trajectory data processing technology. It can excavate information such as
human activity pattern and behavior characteristic, urban vehicle movement characteristic,
atmospheric environment change law and so on. The large amount of track data also
reveals the privacy information, such as the behavior characteristics, interests and
social habits of mobile objects, which is rich in spatio-temporal characteristics
information. This paper begins with the significance of the study of trajectory big
data. Introducing track big data acquisition mode and social application in various
fields, In the specific application, we pay more attention to the object's trajectory
privacy protection. Applying the big data of trajectory to social governance; In addition,
the application of big data in social governance is summarized and the future work
prospect is discussed. We pay more attention to the object's trajectory privacy protection.
Applying the big data of trajectory to social governance; In addition, the application
of big data in social governance is summarized and the future work prospect is discussed.},
booktitle = {Proceedings of the 6th ACM/ACIS International Conference on Applied Computing and Information Technology},
pages = {38–42},
numpages = {5},
keywords = {Trajectory Big Data, Social Computing, Social Governance, Privacy Protection},
location = {Kunming, China},
series = {ACIT 2018}
}

@inproceedings{10.1145/2536146.2536158,
author = {de Oliveira, Maxwell Guimar\~{a}es and Alves, Andr\'{e} Luiz Firmino and Leite, Daniel Farias Batista and Rocha, J\'{u}lio Henrique and Filho, Jos\'{e} Amilton Moura Acioli and de Souza Baptista, Cl\'{a}udio},
title = {Introducing Spatial Context in Comparative Pricing and Product Search},
year = {2013},
isbn = {9781450320047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2536146.2536158},
doi = {10.1145/2536146.2536158},
abstract = {Price survey services assist consumers interested in purchasing a new product online.
Nevertheless, even being efficient in consulting the suppliers and returning up-to-date
information about price and availability of products, current services still provide
trivial and incomplete information concerning the price dispersion and the price for
the final consumer. Usually, only instant prices are provided. These prices change
rapidly and do not include shipping costs, which are usually paid by the consumers.
In this context, this work proposes an approach based on Big Data to introduce the
spatial context supplier-consumer in comparative price surveys and also to store and
present the price variation history of the products as a function of their spatial
and temporal features. A case study is carried out in order to validate the proposal
and to highlight the advantages of this approach for the users of such services.},
booktitle = {Proceedings of the Fifth International Conference on Management of Emergent Digital EcoSystems},
pages = {127–134},
numpages = {8},
keywords = {big spatial data, product search, e-commerce, shopbots},
location = {Luxembourg, Luxembourg},
series = {MEDES '13}
}

@inproceedings{10.1145/2628194.2628251,
author = {Liu, Xiufeng and Iftikhar, Nadeem and Xie, Xike},
title = {Survey of Real-Time Processing Systems for Big Data},
year = {2014},
isbn = {9781450326278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2628194.2628251},
doi = {10.1145/2628194.2628251},
abstract = {In recent years, real-time processing and analytics systems for big data--in the context
of Business Intelligence (BI)--have received a growing attention. The traditional
BI platforms that perform regular updates on daily, weekly or monthly basis are no
longer adequate to satisfy the fast-changing business environments. However, due to
the nature of big data, it has become a challenge to achieve the real-time capability
using the traditional technologies. The recent distributed computing technology, MapReduce,
provides off-the-shelf high scalability that can significantly shorten the processing
time for big data; Its open-source implementation such as Hadoop has become the de-facto
standard for processing big data, however, Hadoop has the limitation of supporting
real-time updates. The improvements in Hadoop for the real-time capability, and the
other alternative real-time frameworks have been emerging in recent years. This paper
presents a survey of the open source technologies that support big data processing
in a real-time/near real-time fashion, including their system architectures and platforms.},
booktitle = {Proceedings of the 18th International Database Engineering &amp; Applications Symposium},
pages = {356–361},
numpages = {6},
keywords = {systems, real-time, architectures, survey, big data},
location = {Porto, Portugal},
series = {IDEAS '14}
}

@inbook{10.1145/3448016.3452752,
author = {Chatziantoniou, Damianos and Kantere, Verena},
title = {DataMingler: A Novel Approach to Data Virtualization},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3452752},
abstract = {A Data Virtual Machine (DVM) is a novel graph-based conceptual model, similar to the
entity-relationship model, representing existing data (persistent, transient, derived)
of an organization. A DVM can be built quickly, agilely, offering schematic flexibility
to data engineers. Data scientists can visually define complex dataframe queries in
an intuitive and simple manner, which are evaluated within an algebraic framework.
A DVM can be easily materialized in any logical data model and can be "reoriented''
around any node, offering a "single view of any entity''. In this paper we demonstrate
DataMingler, a tool implementing DVMs. We argue that DVMs can have a significant practical
impact in analytics environments.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {2681–2685},
numpages = {5}
}

@inproceedings{10.1145/2896825.2896830,
author = {Zhang, Yang and Xu, Fangzhou and Frise, Erwin and Wu, Siqi and Yu, Bin and Xu, Wei},
title = {DataLab: A Version Data Management and Analytics System},
year = {2016},
isbn = {9781450341523},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896825.2896830},
doi = {10.1145/2896825.2896830},
abstract = {One challenge in big data analytics is the lack of tools to manage the complex interactions
among code, data and parameters, especially in the common situation where all these
factors can change a lot. We present our preliminary experience with DataLab, a system
we build to manage the big data workflow. DataLab improves big data analytical workflow
in several novel ways. 1) DataLab manages the revision of both code and data in a
coherent system, and includes a distributed code execution engine to run users' code;
2) DataLab keeps track of all the data analytics results in a data work flow graph,
and is able to compare the code / results between any two versions, making it easier
for users to intuitively see the results of their code change; 3) DataLab provides
an efficient data management system to separate data from their metadata, allowing
efficient preprocessing filters; and 4) DataLab provides a common API so people can
build different applications on top of it. We also present our experience of applying
a DataLab prototype in a real bioinformatics application.},
booktitle = {Proceedings of the 2nd International Workshop on BIG Data Software Engineering},
pages = {12–18},
numpages = {7},
location = {Austin, Texas},
series = {BIGDSE '16}
}

@inproceedings{10.1145/3341620.3341628,
author = {Bi, Haixia and Xue, Yong and Merritt, Patrick and Windmill, Chris and Davis, Bradley},
title = {A Heterogeneous and Interactive Big Earth Data Framework},
year = {2019},
isbn = {9781450360913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341620.3341628},
doi = {10.1145/3341620.3341628},
abstract = {The dramatic development of Earth observation techniques leads to an explosion of
Earth data. However, the increase of the Earth data size and their heterogeneity bring
significant challenges to the storage, processing and visualization of the big Earth
data. To address the problems caused by the huge Earth data-sets, a heterogeneous
and interactive big Earth data framework is proposed in this paper, integrating raster-vector
data cloud storage, data processing based on workflow and machine learning techniques
and real-time rendering and interactive visualization. The framework provides a theoretical
reference for future implementations of the system.},
booktitle = {Proceedings of the 2019 International Conference on Big Data Engineering},
pages = {85–91},
numpages = {7},
keywords = {interactive data visualization, machine learning, workow, Big Earth data, heterogeneous data storage},
location = {Hong Kong, Hong Kong},
series = {BDE 2019}
}

@inproceedings{10.1145/3291064.3291068,
author = {Hu, Xiaodong and Xu, Huanli and Jia, Jinfang and Wang, Xiaoying},
title = {Research on Distributed Storage and Query Optimization of Multi-Source Heterogeneous Meteorological Data},
year = {2018},
isbn = {9781450365765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291064.3291068},
doi = {10.1145/3291064.3291068},
abstract = {The growth of massive data makes the real-time data service of meteorological forecast
and climate analysis facing severe challenge. Distributed database is a good solution
to meet the needs for massive multi-source heterogeneous meteorological data storage.
Since the current mainstream HBase database fails to support the non-Rowkey query,
the poor performance of the real-time query of meteorological data is unsatisfactory.
To address this issue, three kinds of distributed data query optimization strategies
are proposed in this paper, including query optimization based on secondary index,
secondary index query optimization based on hotscore, and query optimization based
on the Redis hot data caching strategy. The corresponding experimental results indicate
that the search scheme based on the Redis hot data caching strategy has the best performance
among the three schemes, not only can meet the needs of meteorological service query,
but also can achieve 3-8 times efficiency enhancement than standard HBase.},
booktitle = {Proceedings of the 2018 International Conference on Cloud Computing and Internet of Things},
pages = {12–18},
numpages = {7},
keywords = {Hot data, Non-Rowkey query, Multi-source heterogeneous meteorological data, Distributed database, Redis cache},
location = {Singapore, Singapore},
series = {CCIOT 2018}
}

@book{10.1145/3226595,
editor = {Brodie, Michael L.},
title = {Making Databases Work: The Pragmatic Wisdom of Michael Stonebraker},
year = {2018},
isbn = {9781947487192},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
volume = {22},
abstract = {At the ACM Awards banquet in June 2017, during the 50th anniversary celebration of
the A.M. Turing Award, ACM announced the launch of the ACM A.M. Turing Book Series,
a sub-series of ACM Books, to celebrate the winners of the A.M. Turing Award, computing's
highest honor, the "Nobel Prize" for computing. This series aims to highlight the
accomplishments of awardees, explaining their major contributions of lasting importance
in computing."Making Databases Work: The Pragmatic Wisdom of Michael Stonebraker,"
the first book in the series, celebrates Mike's contributions and impact. What accomplishments
warranted computing's highest honor? How did Stonebraker do it? Who is Mike Stonebraker---researcher,
professor, CTO, lecturer, innovative product developer, serial entrepreneur, and decades-long
leader, and research evangelist for the database community. This book describes Mike's
many contributions and evaluates them in light of the Turing Award.The book describes,
in 36 chapters, the unique nature, significance, and impact of Mike's achievements
in advancing modern database systems over more than 40 years. The stories involve
technical concepts, projects, people, prototype systems, failures, lucky accidents,
crazy risks, startups, products, venture capital, and lots of applications that drove
Mike Stonebraker's achievements and career. Even if you have no interest in databases
at all, you'll gain insights into the birth and evolution of Turing Award-worthy achievements
from the perspectives of 39 remarkable computer scientists and professionals.Today,
data is considered the world's most valuable resource ("The Economist," May 6, 2017),
whether it is in the tens of millions of databases used to manage the world's businesses
and governments, in the billions of databases in our smartphones and watches, or residing
elsewhere, as yet unmanaged, awaiting the elusive next generation of database systems.
Every one of the millions or billions of databases includes features that are celebrated
by the 2014 A.M. Turing Award and are described in this book.}
}

@article{10.1145/3177850,
author = {Wylot, Marcin and Hauswirth, Manfred and Cudr\'{e}-Mauroux, Philippe and Sakr, Sherif},
title = {RDF Data Storage and Query Processing Schemes: A Survey},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3177850},
doi = {10.1145/3177850},
abstract = {The Resource Description Framework (RDF) represents a main ingredient and data representation
format for Linked Data and the Semantic Web. It supports a generic graph-based data
model and data representation format for describing things, including their relationships
with other things. As the size of RDF datasets is growing fast, RDF data management
systems must be able to cope with growing amounts of data. Even though physically
handling RDF data using a relational table is possible, querying a giant triple table
becomes very expensive because of the multiple nested joins required for answering
graph queries. In addition, the heterogeneity of RDF Data poses entirely new challenges
to database systems. This article provides a comprehensive study of the state of the
art in handling and querying RDF data. In particular, we focus on data storage techniques,
indexing strategies, and query execution mechanisms. Moreover, we provide a classification
of existing systems and approaches. We also provide an overview of the various benchmarking
efforts in this context and discuss some of the open problems in this domain.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {84},
numpages = {36},
keywords = {semi-structured data, RDF, SPARQL}
}

@inproceedings{10.1145/2541583.2541585,
author = {Desair, Tom and Joosen, Wouter and Lagaisse, Bert and Rafique, Ansar and Walraven, Stefan},
title = {Policy-Driven Middleware for Heterogeneous, Hybrid Cloud Platforms},
year = {2013},
isbn = {9781450325530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541583.2541585},
doi = {10.1145/2541583.2541585},
abstract = {The cloud computing paradigm promises increased flexibility and scalability. However,
in private cloud environments this flexibility and scalability is constrained by the
limited capacity. On the other hand, organizations are reluctant to migrate to public
clouds because they lose control over their applications and data. The concept of
a hybrid cloud tries to combine the benefits of private and public clouds, while also
decreasing vendor lock-in.This paper presents PaaSHopper, a middleware platform for
hybrid cloud applications that enables organizations to keep fine-grained control
over the execution of their applications. Driven by policies, the middleware dynamically
decides which requests and tasks are executed in a particular part of the hybrid cloud.
We validated this work by means of a prototype on top of a hybrid cloud consisting
of a local JBoss AS cluster, Google App Engine, and Red Hat OpenShift.},
booktitle = {Proceedings of the 12th International Workshop on Adaptive and Reflective Middleware},
articleno = {2},
numpages = {6},
keywords = {hybrid cloud, platform as a service, dynamic and context-aware adaptation},
location = {Beijing, China},
series = {ARM '13}
}

@article{10.1109/TCBB.2019.2915811,
author = {Liu, Jian and Liu, Qiuru and Zhang, Lei and Su, Shuhui and Liu, Yongzhuang},
title = {Enabling Massive XML-Based Biological Data Management in HBase},
year = {2020},
issue_date = {Nov.-Dec. 2020},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {17},
number = {6},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2019.2915811},
doi = {10.1109/TCBB.2019.2915811},
abstract = {Publishing biological data in XML formats is attractive for organizations who would
like to provide their bioinformatics resources in an extensible and machine-readable
format. In the era of big data, massive XML-based biological data management is emerged
as a challengeable issue. With the continuous growth of the XML-based biological data
sets, it is usually frustrating to use traditional declarative query languages to
provide efficient query capabilities in terms of processing speed and scale. In this
study, we report a novel platform to store and query massive XML-based biological
data collections. A prototype tool for constructing HBase tables from XML-based biological
data collections is first developed, and then a formal approach to transform the XML
query model into the MapReduce query model is proposed. Finally, an evaluation of
the query performance of the proposed approach on the existing XML-based biological
databases is presented, showing that the performance advantages of the proposed solution.
The source code of the massive XML-based biological data management platform is freely
available at <uri>https://github.com/lyotvincent/X2H</uri>.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = nov,
pages = {1994–2004},
numpages = {11}
}

@inproceedings{10.1145/3288599.3296015,
author = {Vyas, Utsav and Panchal, Parth and Patel, Mayank and Bhise, Minal},
title = {STSDB: Spatio-Temporal Sensor Database for Smart City Query Processing},
year = {2019},
isbn = {9781450360944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3288599.3296015},
doi = {10.1145/3288599.3296015},
abstract = {Modern world smart devices are equipped with several sensors which continuously generate
the data. Managing and analyzing these data efficiently is a key need of the current
sensor world. Present applications require real-time analysis of past sensor data
for decision making. The goal of this work is to efficiently process the spatio-temporal
queries for sensor data. Spatio-Temporal Sensor Index STSI helps in managing the sensor
details and leads to faster query processing. The types of queries that have been
considered are; 1) Spatio-Time Travel, 2) Temporal Aggregation and 3) Time Travel,
4) Spatio-temporal Aggregation. Spatio-Temporal Sensor Database STSDB is built by
including STSI index in HBase. The STSDB performance is compared with HBase on two
parameters Data Insertion Time DIT, and Query Execution Time QET. The DIT of STSDB
is almost identical as compared to HBase. While the QET averaged over all four types
of queries show 49% improvement for STSDB over HBase. Both the performance parameters
continue to show similar trends for scaled data in HBase and STSDB. STSDB is demonstrated
in this work using smart city data.},
booktitle = {Proceedings of the 20th International Conference on Distributed Computing and Networking},
pages = {433–438},
numpages = {6},
keywords = {sensors data, smart city applications, query processing, spatio-temporal sensor index STSI, data scaling, spatio-temporal sensor database STSDB},
location = {Bangalore, India},
series = {ICDCN '19}
}

@inproceedings{10.1145/2723372.2742795,
author = {Gupta, Anurag and Agarwal, Deepak and Tan, Derek and Kulesza, Jakub and Pathak, Rahul and Stefani, Stefano and Srinivasan, Vidhya},
title = {Amazon Redshift and the Case for Simpler Data Warehouses},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723372.2742795},
doi = {10.1145/2723372.2742795},
abstract = {Amazon Redshift is a fast, fully managed, petabyte-scale data warehouse solution that
makes it simple and cost-effective to efficiently analyze large volumes of data using
existing business intelligence tools. Since launching in February 2013, it has been
Amazon Web Service's (AWS) fastest growing service, with many thousands of customers
and many petabytes of data under management. Amazon Redshift's pace of adoption has
been a surprise to many participants in the data warehousing community. While Amazon
Redshift was priced disruptively at launch, available for as little as $1000/TB/year,
there are many open-source data warehousing technologies and many commercial data
warehousing engines that provide free editions for development or under some usage
limit. While Amazon Redshift provides a modern MPP, columnar, scale-out architecture,
so too do many other data warehousing engines. And, while Amazon Redshift is available
in the AWS cloud, one can build data warehouses using EC2 instances and the database
engine of one's choice with either local or network-attached storage.In this paper,
we discuss an oft-overlooked differentiating characteristic of Amazon Redshift --
simplicity. Our goal with Amazon Redshift was not to compete with other data warehousing
engines, but to compete with non-consumption. We believe the vast majority of data
is collected but not analyzed. We believe, while most database vendors target larger
enterprises, there is little correlation in today's economy between data set size
and company size. And, we believe the models used to procure and consume analytics
technology need to support experimentation and evaluation. Amazon Redshift was designed
to bring data warehousing to a mass market by making it easy to buy, easy to tune
and easy to manage while also being fast and cost-effective.},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
pages = {1917–1923},
numpages = {7},
keywords = {columnar database:redshift:data warehousing:mpp, amazon redshift},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15}
}

@inbook{10.1109/ASE.2019.00051,
author = {Bao, Liang and Liu, Xin and Wang, Fangzheng and Fang, Baoyin},
title = {ACTGAN: Automatic Configuration Tuning for Software Systems with Generative Adversarial Networks},
year = {2019},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00051},
abstract = {Complex software systems often provide a large number of parameters so that users
can configure them for their specific application scenarios. However, configuration
tuning requires a deep understanding of the software system, far beyond the abilities
of typical system users. To address this issue, many existing approaches focus on
exploring and learning good performance estimation models. The accuracy of such models
often suffers when the number of available samples is small, a thorny challenge under
a given tuning-time constraint. By contrast, we hypothesize that good configurations
often share certain hidden structures. Therefore, instead of trying to improve the
performance estimation of a given configuration, we focus on capturing the hidden
structures of good configurations and utilizing such learned structure to generate
potentially better configurations. We propose ACTGAN to achieve this goal. We have
implemented and evaluated ACTGAN using 17 workloads with eight different software
systems. Experimental results show that ACTGAN outperforms default configurations
by 76.22% on average, and six state-of-the-art configuration tuning algorithms by
6.58%-64.56%. Furthermore, the ACTGAN-generated configurations are often better than
those used in training and show certain features consisting with domain knowledge,
both of which supports our hypothesis.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {465–476},
numpages = {12}
}

@inproceedings{10.1145/2484425.2484441,
author = {Rudolf, Michael and Paradies, Marcus and Bornh\"{o}vd, Christof and Lehner, Wolfgang},
title = {SynopSys: Large Graph Analytics in the SAP HANA Database through Summarization},
year = {2013},
isbn = {9781450321884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2484425.2484441},
doi = {10.1145/2484425.2484441},
abstract = {Graph-structured data is ubiquitous and with the advent of social networking platforms
has recently seen a significant increase in popularity amongst researchers. However,
also many business applications deal with this kind of data and can therefore benefit
greatly from graph processing functionality offered directly by the underlying database.
This paper summarizes the current state of graph data processing capabilities in the
SAP HANA database and describes our efforts to enable large graph analytics in the
context of our research project SynopSys. With powerful graph pattern matching support
at the core, we envision OLAP-like evaluation functionality exposed to the user in
the form of easy-to-apply graph summarization templates. By combining them, the user
is able to produce concise summaries of large graph-structured datasets. We also point
out open questions and challenges that we plan to tackle in the future developments
on our way towards large graph analytics.},
booktitle = {First International Workshop on Graph Data Management Experiences and Systems},
articleno = {16},
numpages = {6},
keywords = {graph transformation, graph matching, graph summarization, SAP HANA database system},
location = {New York, New York},
series = {GRADES '13}
}

@article{10.14778/3137628.3137659,
author = {Pilman, Markus and Bocksrocker, Kevin and Braun, Lucas and Marroqu\'{\i}n, Renato and Kossmann, Donald},
title = {Fast Scans on Key-Value Stores},
year = {2017},
issue_date = {August 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3137628.3137659},
doi = {10.14778/3137628.3137659},
abstract = {Key-Value Stores (KVS) are becoming increasingly popular because they scale up and
down elastically, sustain high throughputs for get/put workloads and have low latencies.
KVS owe these advantages to their simplicity. This simplicity, however, comes at a
cost: It is expensive to process complex, analytical queries on top of a KVS because
today's generation of KVS does not support an efficient way to scan the data. The
problem is that there are conflicting goals when designing a KVS for analytical queries
and for simple get/put workloads: Analytical queries require high locality and a compact
representation of data whereas elastic get/put workloads require sparse indexes. This
paper shows that it is possible to have it all, with reasonable compromises. We studied
the KVS design space and built TellStore, a distributed KVS, that performs almost
as well as state-of-the-art KVS for get/put workloads and orders of magnitude better
for analytical and mixed workloads. This paper presents the results of comprehensive
experiments with an extended version of the YCSB benchmark and a workload from the
telecommunication industry.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1526–1537},
numpages = {12}
}

@article{10.14778/3415478.3415568,
author = {Melnik, Sergey and Gubarev, Andrey and Long, Jing Jing and Romer, Geoffrey and Shivakumar, Shiva and Tolton, Matt and Vassilakis, Theo and Ahmadi, Hossein and Delorey, Dan and Min, Slava and Pasumansky, Mosha and Shute, Jeff},
title = {Dremel: A Decade of Interactive SQL Analysis at Web Scale},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3415478.3415568},
doi = {10.14778/3415478.3415568},
abstract = {Google's Dremel was one of the first systems that combined a set of architectural
principles that have become a common practice in today's cloud-native analytics tools,
including disaggregated storage and compute, in situ analysis, and columnar storage
for semistructured data. In this paper, we discuss how these ideas evolved in the
past decade and became the foundation for Google BigQuery.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3461–3472},
numpages = {12}
}

@inproceedings{10.5555/3049877.3049888,
author = {Hemmings, Matthew and McGeer, Rick and Ricart, Glenn and Stege, Ulrike},
title = {Base64Geo: An Efficient Data Structure and Transmission Format for Large, Dense, Scalar GIS Datasets},
year = {2016},
publisher = {IBM Corp.},
address = {USA},
abstract = {We describe Base64Geo, a data structure and transmission format for large-scale, dense,
scalar GIS datasets. Base64Geo encodes a rectangular grid of scalar GIS values as
an array of strings, where each character is in the range [0 − 9A − Z a − z + /].
Each string represents the values on a specific latitude value, read west to east;
the strings themselves are arranged south to north. The resulting structure gives
a wire format for data transmission that is two orders of magnitude more efficient
than standard GIS and a compact database structure that is searched with simple string
operations. Disk dataset size is reduced by an order of magnitude over a corresponding
CSV structure, and by two orders of magnitude over an indexed GIS database. Search
times on the string-based Base64Geo dataset are an order of magnitude smaller than
search times from a quad-tree based searcher on the same dataset.},
booktitle = {Proceedings of the 26th Annual International Conference on Computer Science and Software Engineering},
pages = {106–115},
numpages = {10},
location = {Toronto, Ontario, Canada},
series = {CASCON '16}
}

@article{10.1145/3157734,
author = {Sanders, Peter and Lamm, Sebastian and H\"{u}bschle-Schneider, Lorenz and Schrade, Emanuel and Dachsbacher, Carsten},
title = {Efficient Parallel Random Sampling—Vectorized, Cache-Efficient, and Online},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {3},
issn = {0098-3500},
url = {https://doi.org/10.1145/3157734},
doi = {10.1145/3157734},
abstract = {We consider the problem of sampling n numbers from the range { 1,… ,N} without replacement
on modern architectures. The main result is a simple divide-and-conquer scheme that
makes sequential algorithms more cache efficient and leads to a parallel algorithm
running in expected time O(n/p+log p) on p processors, i.e., scales to massively parallel
machines even for moderate values of n. The amount of communication between the processors
is very small (at most O(log p)) and independent of the sample size. We also discuss
modifications needed for load balancing, online sampling, sampling with replacement,
Bernoulli sampling, and vectorization on SIMD units or GPUs.},
journal = {ACM Trans. Math. Softw.},
month = jan,
articleno = {29},
numpages = {14},
keywords = {parallel algorithms, communication efficient algorithms, Hypergeometric random deviates}
}

@article{10.1007/s00778-013-0319-9,
author = {Doulkeridis, Christos and N\O{}rv\r{a}g, Kjetil},
title = {A Survey of Large-Scale Analytical Query Processing in MapReduce},
year = {2014},
issue_date = {June      2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {3},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-013-0319-9},
doi = {10.1007/s00778-013-0319-9},
abstract = {Enterprises today acquire vast volumes of data from different sources and leverage
this information by means of data analysis to support effective decision-making and
provide new functionality and services. The key requirement of data analytics is scalability,
simply due to the immense volume of data that need to be extracted, processed, and
analyzed in a timely fashion. Arguably the most popular framework for contemporary
large-scale data analytics is MapReduce, mainly due to its salient features that include
scalability, fault-tolerance, ease of programming, and flexibility. However, despite
its merits, MapReduce has evident performance limitations in miscellaneous analytical
tasks, and this has given rise to a significant body of research that aim at improving
its efficiency, while maintaining its desirable properties. This survey aims to review
the state of the art in improving the performance of parallel query processing using
MapReduce. A set of the most significant weaknesses and limitations of MapReduce is
discussed at a high level, along with solving techniques. A taxonomy is presented
for categorizing existing research on MapReduce improvements according to the specific
problem they target. Based on the proposed taxonomy, a classification of existing
research is provided focusing on the optimization objective. Concluding, we outline
interesting directions for future parallel data processing systems.},
journal = {The VLDB Journal},
month = jun,
pages = {355–380},
numpages = {26},
keywords = {Survey, Large-scale, Query processing, Data analysis, Big Data, MapReduce}
}

@article{10.1145/3385658.3385668,
author = {Abadi, Daniel and Ailamaki, Anastasia and Andersen, David and Bailis, Peter and Balazinska, Magdalena and Bernstein, Philip and Boncz, Peter and Chaudhuri, Surajit and Cheung, Alvin and Doan, AnHai and Dong, Luna and Franklin, Michael J. and Freire, Juliana and Halevy, Alon and Hellerstein, Joseph M. and Idreos, Stratos and Kossmann, Donald and Kraska, Tim and Krishnamurthy, Sailesh and Markl, Volker and Melnik, Sergey and Milo, Tova and Mohan, C. and Neumann, Thomas and Chin Ooi, Beng and Ozcan, Fatma and Patel, Jignesh and Pavlo, Andrew and Popa, Raluca and Ramakrishnan, Raghu and R\'{e}, Christopher and Stonebraker, Michael and Suciu, Dan},
title = {The Seattle Report on Database Research},
year = {2020},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/3385658.3385668},
doi = {10.1145/3385658.3385668},
abstract = {Approximately every five years, a group of database researchers meet to do a self-assessment
of our community, including reflections on our impact on the industry as well as challenges
facing our research community. This report summarizes the discussion and conclusions
of the 9th such meeting, held during October 9-10, 2018 in Seattle.},
journal = {SIGMOD Rec.},
month = feb,
pages = {44–53},
numpages = {10}
}

@inproceedings{10.1145/3139958.3140019,
author = {Baig, Furqan and Vo, Hoang and Kurc, Tahsin and Saltz, Joel and Wang, Fusheng},
title = {SparkGIS: Resource Aware Efficient In-Memory Spatial Query Processing},
year = {2017},
isbn = {9781450354905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139958.3140019},
doi = {10.1145/3139958.3140019},
abstract = {Much effort has been devoted to support high performance spatial queries on large
volumes of spatial data in distributed spatial computing systems, especially in the
MapReduce paradigm. Recent works have focused on extending spatial MapReduce frameworks
to leverage high performance in-memory distributed processing capabilities of systems
such as Spark. However, the performance advantage comes with the requirement of having
enough memory and comprehensive configuration. Failing to fulfill this falls back
to disk IO, defeating the purpose of such systems or in worst case gets out of memory
and fails the job. The problem is aggravated further for spatial processing since
the underlying in-memory systems are oblivious of spatial data features and characteristics.
In this paper we present SparkGIS - an in-memory oriented spatial data querying system
for high throughput and low latency spatial query handling by adapting Apache Spark's
distributed processing capabilities. It supports basic spatial queries including containment,
spatial join and k-nearest neighbor and allows extending these to complex query pipelines.
SparkGIS mitigates skew in distributed processing by supporting several dynamic partitioning
algorithms suitable for a rich set of contemporary application scenarios. Multilevel
global and local, pre-generated and on-demand in-memory indexes, allow SparkGIS to
prune input data and apply compute intensive operations on a subset of relevant spatial
objects only. Finally, SparkGIS employs dynamic query rewriting to gracefully manage
large spatial query workflows that exceed available distributed resources. Our comparative
evaluation has shown that the performance of SparkGIS is on par with contemporary
Spark based platforms for relatively smaller queries and outperforms them for larger
data and memory intensive workflows by dynamic query rewriting and efficient spatial
data management.},
booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
articleno = {28},
numpages = {10},
keywords = {Spark, In-Memory processing, Spatial processing, MapReduce},
location = {Redondo Beach, CA, USA},
series = {SIGSPATIAL '17}
}

@inproceedings{10.1145/3127479.3129248,
author = {Sikdar, Sourav and Teymourian, Kia and Jermaine, Chris},
title = {An Experimental Comparison of Complex Object Implementations for Big Data Systems},
year = {2017},
isbn = {9781450350280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127479.3129248},
doi = {10.1145/3127479.3129248},
abstract = {Many cloud-based data management and analytics systems support complex objects. Dataflow
platforms such as Spark and Flink allow programmers to manipulate sets consisting
of objects from a host programming language (often Java). Document databases such
as MongoDB make use of hierarchical interchange formats---most popularly JSON---which
embody a data model where individual records can themselves contain sets of records.
Systems such as Dremel and AsterixDB allow complex nesting of data structures.Clearly,
no system designer would expect a system that stores JSON objects as text to perform
at the same level as a system based upon a custom-built physical data model. The question
we ask is: How significant is the performance hit associated with choosing a particular
physical implementation? Is the choice going to result in a negligible performance
cost, or one that is debilitating? Unfortunately, there does not exist a scientific
study of the effect of physical complex model implementation on system performance
in the literature. Hence it is difficult for a system designer to fully understand
performance implications of such choices. This paper is an attempt to remedy that.},
booktitle = {Proceedings of the 2017 Symposium on Cloud Computing},
pages = {432–444},
numpages = {13},
keywords = {complex objects implementation, experimental comparison, data serialization, big data management},
location = {Santa Clara, California},
series = {SoCC '17}
}

@inproceedings{10.1145/3423211.3425684,
author = {Zhang, Shungeng and Wang, Qingyang and Kanemasa, Yasuhiko and Liu, Jianshu and Pu, Calton},
title = {DoubleFaceAD: A New Datastore Driver Architecture to Optimize Fanout Query Performance},
year = {2020},
isbn = {9781450381536},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423211.3425684},
doi = {10.1145/3423211.3425684},
abstract = {The broad adoption of fanout queries on distributed datastores has made asynchronous
event-driven datastore drivers a natural choice due to reduced multithreading overhead.
However, through extensive experiments using the latest datastore drivers (e.g., MongoDB,
HBase, DynamoDB) and YCSB benchmark, we show that an asynchronous datastore driver
can cause unexpected performance degradation especially in fanout-query scenarios.
For example, the default MongoDB asynchronous driver adopts the latest Java asynchronous
I/O library, which uses a hidden on-demand JVM level thread pool to process fanout
query responses, causing a surprising multithreading overhead when the query response
size is large. A second instance is the traditional wisdom of modular design of an
application server and the embedded asynchronous datastore driver can cause an im-balanced
workload between the two components due to lack of coordination, incurring frequent
unnecessary system calls. To address the revealed problems, we introduce DoubleFaceAD--a
new asynchronous datastore driver architecture that integrates the management of both
upstream and downstream workload traffic through a few shared reactor threads, with
fanout-query-aware priority-based scheduling to reduce the overall query waiting time.
Our experimental results on two representative application scenarios (YCSB and DBLP)
show DoubleFaceAD outperforms all other types of datastore drivers up to 34% on throughput
and 1.9\texttimes{} faster on 99th percentile response time.},
booktitle = {Proceedings of the 21st International Middleware Conference},
pages = {430–444},
numpages = {15},
keywords = {performance, fanout queries, distributed datastores, asynchronous},
location = {Delft, Netherlands},
series = {Middleware '20}
}

@inproceedings{10.1109/MICRO.2014.44,
author = {Volos, Stavros and Picorel, Javier and Falsafi, Babak and Grot, Boris},
title = {BuMP: Bulk Memory Access Prediction and Streaming},
year = {2014},
isbn = {9781479969982},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MICRO.2014.44},
doi = {10.1109/MICRO.2014.44},
abstract = {With the end of Dennard scaling, server power has emerged as the limiting factor in
the quest for more capable dataenters. Without the benefit of supply voltage scaling,
it is essential to lower the energy per operation to improve server efficiency. As
the industry moves to lean-core server processors, the energy bottleneck is shifting
toward main memory as a chief source of server energy consumption in modern dataenters.
Maximizing the energy efficiency of today's DRAM chips and interfaces requires amortizing
the costly DRAM page activations over multiple row buffer accesses.This work introduces
Bulk Memory Access Prediction and Streaming, or BuMP. We make the observation that
a significant fraction (59-79%) of all memory accesses fall into DRAM pages with high
access density, meaning that the majority of their cache blocks will be accessed within
a modest time frame of the first access. Accesses to high-density DRAM pages include
not only memory reads in response to load instructions, but also reads stemming from
store instructions as well as memory writes upon a dirty LLC eviction. The remaining
accesses go to low-density pages and virtually unpredictable reference patterns (e.g.,
hashed key lookups). BuMP employs a low-cost predictor to identify high-density pages
and triggers bulk transfer operations upon the first read or write to the page. In
doing so, BuMP enforces high row buffer locality where it is profitable, thereby reducing
DRAM energy per access by 23%, and improves server throughput by 11% across a wide
range of server applications.},
booktitle = {Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {545–557},
numpages = {13},
location = {Cambridge, United Kingdom},
series = {MICRO-47}
}

@article{10.14778/2367502.2367518,
author = {Lamb, Andrew and Fuller, Matt and Varadarajan, Ramakrishna and Tran, Nga and Vandiver, Ben and Doshi, Lyric and Bear, Chuck},
title = {The Vertica Analytic Database: C-Store 7 Years Later},
year = {2012},
issue_date = {August 2012},
publisher = {VLDB Endowment},
volume = {5},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2367502.2367518},
doi = {10.14778/2367502.2367518},
abstract = {This paper describes the system architecture of the Vertica Analytic Database (Vertica),
a commercialization of the design of the C-Store research prototype. Vertica demonstrates
a modern commercial RDBMS system that presents a classical relational interface while
at the same time achieving the high performance expected from modern "web scale" analytic
systems by making appropriate architectural choices. Vertica is also an instructive
lesson in how academic systems research can be directly commercialized into a successful
product.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1790–1801},
numpages = {12}
}

@article{10.14778/3007263.3007265,
author = {Sevenich, Martin and Hong, Sungpack and van Rest, Oskar and Wu, Zhe and Banerjee, Jayanta and Chafi, Hassan},
title = {Using Domain-Specific Languages for Analytic Graph Databases},
year = {2016},
issue_date = {September 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3007263.3007265},
doi = {10.14778/3007263.3007265},
abstract = {Recently graph has been drawing lots of attention both as a natural data model that
captures fine-grained relationships between data entities and as a tool for powerful
data analysis that considers such relationships. In this paper, we present a new graph
database system that integrates a robust graph storage with an efficient graph analytics
engine. Primarily, our system adopts two domain-specific languages (DSLs), one for
describing graph analysis algorithms and the other for graph pattern matching queries.
Compared to the API-based approaches in conventional graph processing systems, the
DSL-based approach provides users with more flexible and intuitive ways of expressing
algorithms and queries. Moreover, the DSL-based approach has significant performance
benefits as well, (1) by skipping (remote) API invocation overhead and (2) by applying
high-level optimization from the compiler.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {1257–1268},
numpages = {12}
}

@article{10.14778/2977797.2977806,
author = {Sch\"{a}tzle, Alexander and Przyjaciel-Zablocki, Martin and Skilevic, Simon and Lausen, Georg},
title = {S2RDF: RDF Querying with SPARQL on Spark},
year = {2016},
issue_date = {June 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {10},
issn = {2150-8097},
url = {https://doi.org/10.14778/2977797.2977806},
doi = {10.14778/2977797.2977806},
abstract = {RDF has become very popular for semantic data publishing due to its flexible and universal
graph-like data model. Thus, the ever-increasing size of RDF data collections raises
the need for scalable distributed approaches. We endorse the usage of existing infrastructures
for Big Data processing like Hadoop for this purpose. Yet, SPARQL query performance
is a major challenge as Hadoop is not intentionally designed for RDF processing. Existing
approaches often favor certain query pattern shapes while performance drops significantly
for other shapes. In this paper, we introduce a novel relational partitioning schema
for RDF data called ExtVP that uses a semi-join based preprocessing, akin to the concept
of Join Indices in relational databases, to efficiently minimize query input size
regardless of its pattern shape and diameter. Our prototype system S2RDF is built
on top of Spark and uses SQL to execute SPARQL queries over ExtVP. We demonstrate
its superior performance in comparison to state of the art SPARQL-on-Hadoop approaches.},
journal = {Proc. VLDB Endow.},
month = jun,
pages = {804–815},
numpages = {12}
}

@inproceedings{10.1145/3070607.3070610,
author = {Przyjaciel-Zablocki, Martin and Sch\"{a}tzle, Alexander and Lausen, Georg},
title = {Querying Semantic Knowledge Bases with SQL-on-Hadoop},
year = {2017},
isbn = {9781450350198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3070607.3070610},
doi = {10.1145/3070607.3070610},
abstract = {The constant growth of semantically-annotated data and an increasing interest in cross-domain
knowledge bases raises the need for expressive query languages for RDF and novel approaches
that enable their evaluation for web-scale data sizes. However, SPARQL, the W3C standard
query language for RDF, suffers from a rather limited capability to express navigational
queries. More expressive languages have been theoretically studied, however not implemented.
In this paper, we continue our work on TRIAL-QL, an expressive (SQL-like) RDF query
language based on the Triple Algebra with Recursion [31]. We present a new version
of our TRIAL-QL processor, which takes advantage of the current momentum in in-memory
SQL-on-Hadoop solutions and is built on top of Impala and SPARK while using one unified
data storage. We use our system to study the application of multiple evaluation algorithms,
storage strategies and optimizations on Impala and SPARK while highlighting their
properties. Comprehensive experiments examine the performance of our system in comparison
to other competitive RDF management systems. The obtained results demonstrate its
suitability for querying semantic knowledge bases by providing interactive query response
times for selective queries on datasets with more than one billion triple. More data-intensive
use-cases that produce, e.g. over 25 billion results finished in the order of minutes.},
booktitle = {Proceedings of the 4th ACM SIGMOD Workshop on Algorithms and Systems for MapReduce and Beyond},
articleno = {4},
numpages = {10},
location = {Chicago, IL, USA},
series = {BeyondMR'17}
}

@article{10.1145/3403954,
author = {Sharma, Pratima and Jindal, Rajni and Borah, Malaya Dutta},
title = {Blockchain Technology for Cloud Storage: A Systematic Literature Review},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3403954},
doi = {10.1145/3403954},
abstract = {The demand for Blockchain innovation and the significance of its application has inspired
ever-progressing exploration in various scientific and practical areas. Even though
it is still in the initial testing stage, the blockchain is being viewed as a progressive
solution to address present-day technology concerns, such as decentralization, identity,
trust, character, ownership of data, and information-driven choices. Simultaneously,
the world is facing an increase in the diversity and quantity of digital information
produced by machines and users. While effectively looking for the ideal approach to
storing and processing cloud data, the blockchain innovation provides significant
inputs. This article reviews the application of blockchain technology for securing
cloud storage.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {89},
numpages = {32},
keywords = {decentralization, Blockchain technology, cloud storage, cloud security, cloud computing}
}

@inproceedings{10.1145/3052973.3052977,
author = {Yuan, Xingliang and Guo, Yu and Wang, Xinyu and Wang, Cong and Li, Baochun and Jia, Xiaohua},
title = {EncKV: An Encrypted Key-Value Store with Rich Queries},
year = {2017},
isbn = {9781450349444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3052973.3052977},
doi = {10.1145/3052973.3052977},
abstract = {Distributed data stores have been rapidly evolving to serve the needs of large-scale
applications such as online gaming and real-time targeting. In particular, distributed
key-value stores have been widely adopted due to their superior performance. However,
these systems do not guarantee to provide strong protection of data confidentiality,
and as a result fall short of addressing serious privacy concerns raised from massive
data breaches.In this paper, we introduce EncKV, an encrypted key-value store with
secure rich query support. First, EncKV stores encrypted data records with multiple
secondary attributes in the form of encrypted key-value pairs. Second, it leverages
the latest practical primitives for searching over encrypted data, i.e., searchable
symmetric encryption and order-revealing encryption, and provides encrypted indexes
with guaranteed security to support exact-match and range-match queries via secondary
attributes of data records. Third, it carefully integrates these indexes into a distributed
index framework to facilitate secure query processing in parallel. To mitigate recent
inference attacks on encrypted database systems, EncKV protects the order information
during range queries, and presents an interactive batch query mechanism to further
hide the associations across data values on different attributes. We implement an
EncKV prototype on a Redis cluster, and conduct an extensive set of performance evaluations
on the Amazon EC2 public cloud platform. Our results show that EncKV effectively preserves
the efficiency and scalability of plaintext distributed key-value stores.},
booktitle = {Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security},
pages = {423–435},
numpages = {13},
keywords = {order-revealing encryption, searchable encryption, encrypted key-value store},
location = {Abu Dhabi, United Arab Emirates},
series = {ASIA CCS '17}
}

@inproceedings{10.1145/2247596.2247598,
author = {Borkar, Vinayak and Carey, Michael J. and Li, Chen},
title = {Inside "Big Data Management": Ogres, Onions, or Parfaits?},
year = {2012},
isbn = {9781450307901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2247596.2247598},
doi = {10.1145/2247596.2247598},
abstract = {In this paper we review the history of systems for managing "Big Data" as well as
today's activities and architectures from the (perhaps biased) perspective of three
"database guys" who have been watching this space for a number of years and are currently
working together on "Big Data" problems. Our focus is on architectural issues, and
particularly on the components and layers that have been developed recently (in open
source and elsewhere) and on how they are being used (or abused) to tackle challenges
posed by today's notion of "Big Data". Also covered is the approach we are taking
in the ASTERIX project at UC Irvine, where we are developing our own set of answers
to the questions of the "right" components and the "right" set of layers for taming
the "Big Data" beast. We close by sharing our opinions on what some of the important
open questions are in this area as well as our thoughts on how the dataintensive computing
community might best seek out answers.},
booktitle = {Proceedings of the 15th International Conference on Extending Database Technology},
pages = {3–14},
numpages = {12},
location = {Berlin, Germany},
series = {EDBT '12}
}

@inproceedings{10.1145/3397536.3422262,
author = {Bakli, Mohamed and Sakr, Mahmoud and Zim\'{a}nyi, Esteban},
title = {Distributed Spatiotemporal Trajectory Query Processing in SQL},
year = {2020},
isbn = {9781450380195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397536.3422262},
doi = {10.1145/3397536.3422262},
abstract = {Nowadays, the collection of moving object data is significantly increasing due to
the ubiquity of GPS-enabled devices. Managing and analyzing this kind of data is crucial
in many application domains, including social mobility, pandemics, and transportation.
In previous work, we have proposed the MobilityDB moving object database system. It
is a production-ready system, that is built on top of PostgreSQL and PostGIS. It accepts
SQL queries and offers most of the common spatiotemporal types and operations. In
this paper, to address the scalability requirement of big data, we provide an architecture
and an implementation of a distributed moving object database system based on MobilityDB.
More specifically, we define: (1) an architecture for deploying a distributed MobilityDB
database on a cluster using readily available tools, (2) two alternative trajectory
data partitioning and index partitioning methods, and (3) a query optimizer that is
capable of distributing spatiotemporal SQL queries over multiple MobilityDB instances.
The overall outcome is that the cluster is managed in SQL at the run-time and that
the user queries are transparently distributed and executed. This is validated with
experiments using a real dataset, which also compares MobilityDB with other relevant
systems.},
booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
pages = {87–98},
numpages = {12},
keywords = {distributed query processing, MobilityDB, trajectory data},
location = {Seattle, WA, USA},
series = {SIGSPATIAL '20}
}

@article{10.1145/3136623,
author = {Mansouri, Yaser and Toosi, Adel Nadjaran and Buyya, Rajkumar},
title = {Data Storage Management in Cloud Environments: Taxonomy, Survey, and Future Directions},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3136623},
doi = {10.1145/3136623},
abstract = {Storage as a Service (StaaS) is a vital component of cloud computing by offering the
vision of a virtually infinite pool of storage resources. It supports a variety of
cloud-based data store classes in terms of availability, scalability, ACID (Atomicity,
Consistency, Isolation, Durability) properties, data models, and price options. Application
providers deploy these storage classes across different cloud-based data stores not
only to tackle the challenges arising from reliance on a single cloud-based data store
but also to obtain higher availability, lower response time, and more cost efficiency.
Hence, in this article, we first discuss the key advantages and challenges of data-intensive
applications deployed within and across cloud-based data stores. Then, we provide
a comprehensive taxonomy that covers key aspects of cloud-based data store: data model,
data dispersion, data consistency, data transaction service, and data management cost.
Finally, we map various cloud-based data stores projects to our proposed taxonomy
to validate the taxonomy and identify areas for future research.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {91},
numpages = {51},
keywords = {and data management cost, Data management, data replication, transaction service, data consistency, data storage}
}

@article{10.14778/2994509.2994513,
author = {Lu, Lu and Shi, Xuanhua and Zhou, Yongluan and Zhang, Xiong and Jin, Hai and Pei, Cheng and He, Ligang and Geng, Yuanzhen},
title = {Lifetime-Based Memory Management for Distributed Data Processing Systems},
year = {2016},
issue_date = {August 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2994509.2994513},
doi = {10.14778/2994509.2994513},
abstract = {In-memory caching of intermediate data and eager combining of data in shuffle buffers
have been shown to be very effective in minimizing the re-computation and I/O cost
in distributed data processing systems like Spark and Flink. However, it has also
been widely reported that these techniques would create a large amount of long-living
data objects in the heap, which may quickly saturate the garbage collector, especially
when handling a large dataset, and hence would limit the scalability of the system.
To eliminate this problem, we propose a lifetime-based memory management framework,
which, by automatically analyzing the user-defined functions and data types, obtains
the expected lifetime of the data objects, and then allocates and releases memory
space accordingly to minimize the garbage collection overhead. In particular, we present
Deca, a concrete implementation of our proposal on top of Spark, which transparently
decomposes and groups objects with similar lifetimes into byte arrays and releases
their space altogether when their lifetimes come to an end. An extensive experimental
study using both synthetic and real datasets shows that, in comparing to Spark, Deca
is able to 1) reduce the garbage collection time by up to 99.9%, 2) to achieve up
to 22.7x speed up in terms of execution time in cases without data spilling and 41.6x
speedup in cases with data spilling, and 3) to consume up to 46.6% less memory.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {936–947},
numpages = {12}
}

@article{10.1145/3310361,
author = {Shi, Xuanhua and Ke, Zhixiang and Zhou, Yongluan and Jin, Hai and Lu, Lu and Zhang, Xiong and He, Ligang and Hu, Zhenyu and Wang, Fei},
title = {Deca: A Garbage Collection Optimizer for In-Memory Data Processing},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {1},
issn = {0734-2071},
url = {https://doi.org/10.1145/3310361},
doi = {10.1145/3310361},
abstract = {In-memory caching of intermediate data and active combining of data in shuffle buffers
have been shown to be very effective in minimizing the recomputation and I/O cost
in big data processing systems such as Spark and Flink. However, it has also been
widely reported that these techniques would create a large amount of long-living data
objects in the heap. These generated objects may quickly saturate the garbage collector,
especially when handling a large dataset, and hence, limit the scalability of the
system. To eliminate this problem, we propose a lifetime-based memory management framework,
which, by automatically analyzing the user-defined functions and data types, obtains
the expected lifetime of the data objects and then allocates and releases memory space
accordingly to minimize the garbage collection overhead. In particular, we present
Deca,<sup;>1</sup;> a concrete implementation of our proposal on top of Spark, which
transparently decomposes and groups objects with similar lifetimes into byte arrays
and releases their space altogether when their lifetimes come to an end. When systems
are processing very large data, Deca also provides field-oriented memory pages to
ensure high compression efficiency. Extensive experimental studies using both synthetic
and real datasets show that, in comparing to Spark, Deca is able to (1) reduce the
garbage collection time by up to 99.9%, (2) reduce the memory consumption by up to
46.6% and the storage space by 23.4%, (3) achieve 1.2\texttimes{} to 22.7\texttimes{} speedup in terms of
execution time in cases without data spilling and 16\texttimes{} to 41.6\texttimes{} speedup in cases with
data spilling, and (4) provide similar performance compared to domain-specific systems.},
journal = {ACM Trans. Comput. Syst.},
month = mar,
articleno = {3},
numpages = {47},
keywords = {memory management, distributed system, garbage collection, in-memory, Data processing system}
}

@inproceedings{10.1145/3064176.3064184,
author = {Zheng, Wenting and Li, Frank and Popa, Raluca Ada and Stoica, Ion and Agarwal, Rachit},
title = {MiniCrypt: Reconciling Encryption and Compression for Big Data Stores},
year = {2017},
isbn = {9781450349383},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3064176.3064184},
doi = {10.1145/3064176.3064184},
abstract = {We propose MiniCrypt, the first key-value store that reconciles encryption and compression
without compromising performance. At the core of MiniCrypt is an observation on data
compressibility trends in key-value stores, which enables grouping key-value pairs
into small key packs, together with a set of distributed systems techniques for retrieving,
updating, merging and splitting encrypted packs. Our evaluation shows that MiniCrypt
compresses data by as much as 4 times with respect to the vanilla key-value store,
and can increase the server's throughput by up to two orders of magnitude by fitting
more data in main memory.},
booktitle = {Proceedings of the Twelfth European Conference on Computer Systems},
pages = {191–204},
numpages = {14},
location = {Belgrade, Serbia},
series = {EuroSys '17}
}

@inproceedings{10.1145/3318464.3386134,
author = {Taft, Rebecca and Sharif, Irfan and Matei, Andrei and VanBenschoten, Nathan and Lewis, Jordan and Grieger, Tobias and Niemi, Kai and Woods, Andy and Birzin, Anne and Poss, Raphael and Bardea, Paul and Ranade, Amruta and Darnell, Ben and Gruneir, Bram and Jaffray, Justin and Zhang, Lucy and Mattis, Peter},
title = {CockroachDB: The Resilient Geo-Distributed SQL Database},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386134},
doi = {10.1145/3318464.3386134},
abstract = {We live in an increasingly interconnected world, with many organizations operating
across countries or even continents. To serve their global user base, organizations
are replacing their legacy DBMSs with cloud-based systems capable of scaling OLTP
workloads to millions of users. CockroachDB is a scalable SQL DBMS that was built
from the ground up to support these global OLTP workloads while maintaining high availability
and strong consistency. Just like its namesake, CockroachDB is resilient to disasters
through replication and automatic recovery mechanisms. This paper presents the design
of CockroachDB and its novel transaction model that supports consistent geo-distributed
transactions on commodity hardware. We describe how CockroachDB replicates and distributes
data to achieve fault tolerance and high performance, as well as how its distributed
SQL layer automatically scales with the size of the database cluster while providing
the standard SQL interface that users expect. Finally, we present a comprehensive
performance evaluation and share a couple of case studies of CockroachDB users. We
conclude by describing lessons learned while building CockroachDB over the last five
years.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1493–1509},
numpages = {17},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@article{10.1007/s00778-016-0422-9,
author = {Lu, Yue and Li, Yuguan and Eltabakh, Mohamed Y.},
title = {Decorating the Cloud: Enabling Annotation Management in MapReduce},
year = {2016},
issue_date = {June      2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {3},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-016-0422-9},
doi = {10.1007/s00778-016-0422-9},
abstract = {Data curation and annotation are indispensable mechanisms to a wide range of applications
for capturing various types of metadata information. This metadata not only increases
the data's credibility and merit, and allows end users and applications to make more
informed decisions, but also enables advanced processing over the data that is not
feasible otherwise. That is why annotation management has been extensively studied
in the context of scientific repositories, web documents, and relational database
systems. In this paper, we make the case that cloud-based applications that rely on
the emerging Hadoop infrastructure are also in need for data curation and annotation
and that the presence of such mechanisms in Hadoop would bring value-added capabilities
to these applications. We propose the "CloudNotes" system, a full-fledged MapReduce-based
annotation management engine. CloudNotes addresses several new challenges to annotation
management including: (1) scalable and distributed processing of annotations over
large clusters, (2) propagation of annotations under the MapReduce's blackbox execution
model, and (3) annotation-driven optimizations ranging from proactive prefetching
and colocation of annotations, annotation-aware task scheduling, novel shared execution
strategies among the annotation jobs, and concurrency control mechanisms for annotation
management. These challenges have not been addressed or explored before by the state-of-art
technologies. CloudNotes is built on top of the open-source Hadoop/HDFS infrastructure
and experimentally evaluated to demonstrate the practicality and scalability of its
features, and the effectiveness of its optimizations under large workloads.},
journal = {The VLDB Journal},
month = jun,
pages = {399–424},
numpages = {26},
keywords = {Cloud-based annotations, Distributed annotation management, MapReduce}
}

@proceedings{10.1145/2723372,
title = {SIGMOD '15: Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to SIGMOD 2015 -- officially, the 2015 ACM SIGMOD International Conference
on the Management of Data! This year's conference is being held in the beautiful cultural
capital of Australia, Melbourne. During the Gold Rush period of the 19th Century,
Melbourne was the richest city in the world, and as a result it is filled with many
unique neighborhoods and distinctive buildings. In addition to wonderful neighborhoods
to explore, the city has great museums and other cultural attractions, as well as
a fine multi-cultural atmosphere. For those who would like to explore the outdoors,
popular highlights are the Phillip Island Nature Park (90 minutes away), which features
wild penguins who return in a parade each day at sunset, and the Great Ocean Road,
one of the world's most scenic coastal drives, including the famous towering 12 Apostles.SIGMOD
2015's exciting technical program reflects not only traditional topics, but the database
community's role in broader data science and data analytics. The keynote from Laura
Haas, "The Power Behind the Throne: Information Integration in the Age of Data-Driven
Discovery" highlights the role of database and data integration techniques in the
growing field of data science. Jignesh Patel's talk, "From Data to Insights @ Bare
Metal Speed," explains how hardware and software need to be co-evolved to support
the needs of scalable data analytics. Jennifer Widom, winner of the 2015 ACM-W Athena
Lecturer Award for fundamental contributions to computer science, will give her award
talk, "Three Favorite Results," on Tuesday. Christopher R\'{e} will lead a panel on "Machine
Learning and Databases: The Sound of Things to Come or a Cacophony of Hype?," with
participants Divyakant Agrawal, Magdalena Balazinska, Michael Cafarella, Michael Jordan,
Tim Kraska, and Raghu Ramakrishnan. Of course, there are also 106 research paper presentations,
4 tutorials, 30 demonstrations, and 18 industrial papers. Papers will be presented
both as talks during the research sessions, and as part of plenary Poster Sessions.SIGMOD
2015 is preceded by the PhD Workshop, as well as workshops on leading-edge topics
like data analytics (DanaC), databases and the Web (WebDB), exploratory search (ExploreDB),
managing and mining spatial data (GeoRich), and graph data (GRADES); the New Researcher
Symposium will take place on Wednesday. The banquet will be held in a Melbourne landmark,
the Town Hall.As in recent years, we had two submission deadlines for SIGMOD this
year, one in August and one in November. The review process was journal-style, with
multiple rounds of reviews coordinated by the Group Leaders. We accepted 34 of 137
papers from the first deadline and 72 of 278 from the second deadline. The total acceptance
rate was about 25.5%, and we believe that the revision processhas improved the quality
of the technical program.},
location = {Melbourne, Victoria, Australia}
}

