@inproceedings{10.1145/2602622.2602624,
author = {Mior, Michael J.},
title = {Automated Schema Design for NoSQL Databases},
year = {2014},
isbn = {9781450329248},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602622.2602624},
doi = {10.1145/2602622.2602624},
abstract = {Selecting appropriate indices and materialized views is critical for high performance
in relational databases. By example, we show that the problem of schema optimization
is also highly relevant for NoSQL databases. We explore the problem of schema design
in NoSQL databases with a goal of optimizing query performance while minimizing storage
overhead. Our suggested approach uses the cost of executing a given workload for a
given schema to guide the mapping from the application data model to a physical schema.
We propose a cost-driven approach for optimization and discuss its usefulness as part
of an automated schema design tool.},
booktitle = {Proceedings of the 2014 SIGMOD PhD Symposium},
pages = {41–45},
numpages = {5},
keywords = {schema optimization, nosql, workload modeling},
location = {Snowbird, Utah, USA},
series = {SIGMOD'14 PhD Symposium}
}

@article{10.1145/3158661,
author = {Davoudian, Ali and Chen, Liu and Liu, Mengchi},
title = {A Survey on NoSQL Stores},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3158661},
doi = {10.1145/3158661},
abstract = {Recent demands for storing and querying big data have revealed various shortcomings
of traditional relational database systems. This, in turn, has led to the emergence
of a new kind of complementary nonrelational data store, named as NoSQL. This survey
mainly aims at elucidating the design decisions of NoSQL stores with regard to the
four nonorthogonal design principles of distributed database systems: data model,
consistency model, data partitioning, and the CAP theorem. For each principle, its
available strategies and corresponding features, strengths, and drawbacks are explained.
Furthermore, various implementations of each strategy are exemplified and crystallized
through a collection of representative academic and industrial NoSQL technologies.
Finally, we disclose some existing challenges in developing effective NoSQL stores,
which need attention of the research community, application designers, and architects.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {40},
numpages = {43},
keywords = {ACID, partitioning, NoSQL, elasticity, CAP theorem, replication, consistency model, data model}
}

@inproceedings{10.1145/3265007.3265011,
author = {Maity, Biswajit and Acharya, Anal and Goto, Takaaki and Sen, Soumya},
title = {A Framework to Convert NoSQL to Relational Model},
year = {2018},
isbn = {9781450365741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3265007.3265011},
doi = {10.1145/3265007.3265011},
abstract = {Due to the exponential growth of NoSQL databases and in addition the circumstance
of perusing humongous volumes of information, maximum applications switch RDBMS to
NoSQL and pick it as information stockpiling framework. But we all know that RDBMS
have several advantages which make it a popular platform across several applications
over the decades. Therefore we view the standard problem of converting the RDBMS to
NoSQL in reverse approach and we conceptualize a problem where NoSQL is converted
back to a RDBMS based system. A generic framework is proposed in this paper so that
different NoSQL databases could be converted to RDBMS. This approach is illustrated
here using a case study on MongoDB and Neo4j. MongoDB is a document oriented database,
fully unstructured and schemaless whereas Neo4j is a graph oriented database, fully
unstructured and schemaless. This proves robustness of our proposed mechanism.},
booktitle = {Proceedings of the 6th ACM/ACIS International Conference on Applied Computing and Information Technology},
pages = {1–6},
numpages = {6},
keywords = {Cassandra, Data Conversion, Neo4j, MongoDB, NoSQL},
location = {Kunming, China},
series = {ACIT 2018}
}

@inproceedings{10.1145/3220199.3220212,
author = {Zhang, Chao and Xu, Jing},
title = {A Unified SQL Middleware for NoSQL Databases},
year = {2018},
isbn = {9781450364263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220199.3220212},
doi = {10.1145/3220199.3220212},
abstract = {With the popularity of smart mobile devices and the development of big data, NoSQL
databases came into being. Compared to the traditional relational databases, NoSQL
databases have the advantages of unstructured storage, high availability and high
scalability. So NoSQL databases are better able to handle the sheer volume of unstructured
data generated by large Web applications and mobile applications. But since there
are so many NoSQL databases today, each NoSQL database provides its own set of APIs,
lacking a uniform standard. As a result, NoSQL databases have not been well received,
though they perform better. This paper presents a unified architecture that allows
NoSQL databases to support standard SQL (Structured Query Language) operations. In
accordance with this architecture, we implement a middleware called NoMiddleware,
which preserves the benefits of SQL in NoSQL systems. And in order to better evaluate
the standard SQL query performance of NoSQL databases, we propose a set of micro-bench
called Nomicrobench. The experimental results show that NoMiddleware provides the
most complete SQL standard with the least overhead and benefits both in functionality
and performance.},
booktitle = {Proceedings of the 2018 International Conference on Big Data and Computing},
pages = {14–19},
numpages = {6},
keywords = {NoMicroBench, NoMiddleware, ANTLR, unity, Big Data, NoSQL, MongoDB, SQL},
location = {Shenzhen, China},
series = {ICBDC '18}
}

@inproceedings{10.1145/3341525.3387399,
author = {Kim, Suneuy},
title = {Seamless Integration of NoSQL Class into the Database Curriculum},
year = {2020},
isbn = {9781450368742},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341525.3387399},
doi = {10.1145/3341525.3387399},
abstract = {Since NoSQL emerged a decade ago, it has rapidly gained popularity and has been actively
incorporated into data management solutions for big data. This phenomenon brings positive
challenges to accommodate NoSQL topics in the database curriculum. This paper presents
our experience of teaching a NoSQL class over the last three years. The course uses
a comprehensive teaching methodology that combines lectures, hands-on assignments,
projects, and research-based approaches. The methodology aims at both students' in-depth
learning and seamless integration of NoSQL topics into the database curriculum. The
teaching methodology and course contents are detailed. Student evaluations of teaching,
assessment results, success stories, and challenges and lessons learned are presented.},
booktitle = {Proceedings of the 2020 ACM Conference on Innovation and Technology in Computer Science Education},
pages = {314–320},
numpages = {7},
keywords = {NoSQL, education, databases, curriculum, big data},
location = {Trondheim, Norway},
series = {ITiCSE '20}
}

@inproceedings{10.1145/3427228.3427260,
author = {Ferrari, Dario and Carminati, Michele and Polino, Mario and Zanero, Stefano},
title = {NoSQL Breakdown: A Large-Scale Analysis of Misconfigured NoSQL Services},
year = {2020},
isbn = {9781450388580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427228.3427260},
doi = {10.1145/3427228.3427260},
abstract = {In the last years, NoSQL databases have grown in popularity due to their easy-to-deploy,
reliable, and scalable storage mechanism. While most NoSQL services offer access control
mechanisms, their default configurations grant access without any form of authentication,
resulting in misconfigurations that may expose data to the Internet, as demonstrated
by the recent high-profile data leaks. In this paper, we investigate the usage of
the most popular NoSQL databases, focusing on automatically analyzing and discovering
misconfigurations that may lead to security and privacy issues. We developed a tool
that automatically scans large IP subnets to detect the exposed services and performs
security analyses without storing nor exposing sensitive data. We analyzed 67,725,641
IP addresses between October 2019 and March 2020, spread across several Cloud Service
Providers (CSPs), and found 12,276 misconfigured databases. The risks associated with
exposed services range from data leaking, which may pose a significant menace to users’
privacy, to data tampering of resources stored in the vulnerable databases, which
may pose a relevant threat to a web service reputation. Regarding the last point,
we found 742 potentially vulnerable websites linked to misconfigured instances with
the write permission enabled to anonymous users. },
booktitle = {Annual Computer Security Applications Conference},
pages = {567–581},
numpages = {15},
keywords = {vulnerabilities., NoSQL services, database and storage security, misconfiguration},
location = {Austin, USA},
series = {ACSAC '20}
}

@inproceedings{10.1145/3053600.3053622,
author = {Reniers, Vincent and Van Landuyt, Dimitri and Rafique, Ansar and Joosen, Wouter},
title = {On the State of NoSQL Benchmarks},
year = {2017},
isbn = {9781450348997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3053600.3053622},
doi = {10.1145/3053600.3053622},
abstract = {The proliferation of Big Data systems and namely NoSQL databases has resulted in a
tremendous heterogeneity in its offerings. It has become increasingly difficult to
compare and select the most optimal NoSQL storage technology. Current benchmark efforts,
such as the Yahoo! Cloud Serving Benchmark (YCSB), evaluate simple read and write
operations on a primary key. However, while YCSB has become the de-facto benchmark
solution for practitioners and NoSQL vendors, it is lacking in capabilities to extensively
evaluate specific NoSQL solutions.In this paper, we present a systematic survey of
current NoSQL benchmarks, in which we identify a clear gap in benchmarking more advanced
workloads (e.g. nested document search) for features specific to NoSQL database families
(e.g. document stores). Secondly, based on our survey, we discuss the strengths and
weaknesses of the different benchmark design approaches, and argue in favor of a benchmark
suite that targets specific families of NoSQL databases yet still allows overall comparison
of databases in terms of their commonalities.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
pages = {107–112},
numpages = {6},
keywords = {performance benchmarks, ycsb, nosql benchmarks},
location = {L'Aquila, Italy},
series = {ICPE '17 Companion}
}

@inproceedings{10.1145/3286606.3286806,
author = {Amellal, H. and Meslouhi, A. and El Allati, A.},
title = {Reduce Data Processing Time in NoSQL Databases Based on Grover's Algorithm},
year = {2018},
isbn = {9781450365628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286606.3286806},
doi = {10.1145/3286606.3286806},
abstract = {In order to reduce data processing time in NoSQL databases, we propose in this paper
a quantum approach to extracting information from unstructured databases. In fact,
we apply Grover's algorithm instead of classical algorithms to search in NoSQL databases.},
booktitle = {Proceedings of the 3rd International Conference on Smart City Applications},
articleno = {29},
numpages = {3},
keywords = {NoSQL databases, Grover's algorithm, quantum computing, data mining},
location = {Tetouan, Morocco},
series = {SCA '18}
}

@inproceedings{10.1145/2926534.2926535,
author = {Schildgen, Johannes and Lottermann, Thomas and De\ss{}loch, Stefan},
title = {Cross-System NoSQL Data Transformations with NotaQL},
year = {2016},
isbn = {9781450343114},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2926534.2926535},
doi = {10.1145/2926534.2926535},
abstract = {The rising adoption of NoSQL technology in enterprises causes a heterogeneous landscape
of different data stores. Different stores provide distinct advantages and disadvantages,
making it necessary for enterprises to facilitate multiple systems for specific purposes.
This resulting polyglot persistence is difficult to handle for developers since some
data needs to be replicated and aggregated between different and within the same stores.
Currently, there are no uniform tools to perform these data transformations since
all stores feature different APIs and data models. In this paper, we present the transformation
language NotaQL that allows cross-system data transformations. These transformations
are output-oriented, meaning that the structure of a transformation script is similar
to that of the output. Besides, we provide an aggregation-centric approach, which
makes aggregation operations as easy as possible.},
booktitle = {Proceedings of the 3rd ACM SIGMOD Workshop on Algorithms and Systems for MapReduce and Beyond},
articleno = {5},
numpages = {10},
location = {San Francisco, California},
series = {BeyondMR '16}
}

@article{10.1145/1978915.1978919,
author = {Cattell, Rick},
title = {Scalable SQL and NoSQL Data Stores},
year = {2011},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/1978915.1978919},
doi = {10.1145/1978915.1978919},
abstract = {In this paper, we examine a number of SQL and socalled "NoSQL" data stores designed
to scale simple OLTP-style application loads over many servers. Originally motivated
by Web 2.0 applications, these systems are designed to scale to thousands or millions
of users doing updates as well as reads, in contrast to traditional DBMSs and data
warehouses. We contrast the new systems on their data model, consistency mechanisms,
storage mechanisms, durability guarantees, availability, query support, and other
dimensions. These systems typically sacrifice some of these dimensions, e.g. database-wide
transaction consistency, in order to achieve others, e.g. higher availability and
scalability.},
journal = {SIGMOD Rec.},
month = may,
pages = {12–27},
numpages = {16}
}

@article{10.14778/3339490.3339498,
author = {Cao, Yang and Fan, Wenfei and Yuan, Tengfei},
title = {Block as a Value for SQL over NoSQL},
year = {2019},
issue_date = {June 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {10},
issn = {2150-8097},
url = {https://doi.org/10.14778/3339490.3339498},
doi = {10.14778/3339490.3339498},
abstract = {This paper presents Zidian, a middleware for key-value (KV) stores to speed up SQL
query evaluation over NoSQL. As opposed to common practice that takes a tuple id or
primary key as key and the entire tuple as value, Zidian proposes a block-as-a-value
model BaaV. BaaV represents a relation as keyed blocks (k, B), where k is a key of
a block (a set) B of partial tuples. We extend relational algebra to BaaV.We show
that under BaaV, Zidian substantially reduces data access and communication cost.
We provide characterizations (sufficient and necessary conditions) for (a) result-preserving
queries, i.e., queries covered by available BaaV stores, (b) scan-free queries, i.e.,
queries that can be evaluated without scanning any table, and (c) bounded queries,
i.e., queries that can be answered by accessing a bounded amount of data. We show
that in parallel processing, Zidian guarantees (a) no scans for scan-free queries,
(b) bounded communication cost for bounded queries; and (c) parallel scalability,
i.e., speed up when adding processors. Moreover, Zidian can be plugged into existing
SQL-over-NoSQL systems and retains horizontal scalability. Using benchmark and real-life
data, we empirically verify that Zidian improves existing SQL-over-NoSQL systems by
2 orders of magnitude on average.},
journal = {Proc. VLDB Endow.},
month = jun,
pages = {1153–1166},
numpages = {14}
}

@inproceedings{10.1145/3282308.3282321,
author = {Moreno, Julio and Fernandez, Eduardo B. and Fernandez-Medina, Eduardo and Serrano, Manuel A.},
title = {A Security Pattern for Key-Value NoSQL Database Authorization},
year = {2018},
isbn = {9781450363877},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3282308.3282321},
doi = {10.1145/3282308.3282321},
abstract = {Numerous authorization models have been proposed for relational databases. On the
other hand, several NoSQL databases used in Big Data applications use a new model
appropriate to their requirements for structure, speed, and large amount of data.
This model protects each individual cell in key-value databases by labeling them with
authorization rights following a Role-Based Access Control model or similar. We present
here a pattern to describe this model as it exists in several Big Data systems.},
booktitle = {Proceedings of the 23rd European Conference on Pattern Languages of Programs},
articleno = {12},
numpages = {4},
keywords = {Cloud Computing, Secure Databases, Big Data, Security Pattern},
location = {Irsee, Germany},
series = {EuroPLoP '18}
}

@inproceedings{10.1145/3340964.3340981,
author = {Koutroumanis, Nikolaos and Nikitopoulos, Panagiotis and Vlachou, Akrivi and Doulkeridis, Christos},
title = {NoDA: Unified NoSQL Data Access Operators for Mobility Data},
year = {2019},
isbn = {9781450362801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340964.3340981},
doi = {10.1145/3340964.3340981},
abstract = {In this paper, we propose NoDA, an abstraction layer consisting of spatio-temporal
data access operators, which is used to access NoSQL storage engines in a unified
way. NoDA alleviates the burden from big data developers of learning the query language
of each NoSQL store, and offers a unified view of the underlying NoSQL store. Our
approach is inspired by the equivalent paradigm of drivers (such as JDBC) in the relational
database world, where the application code is indifferent to the exact underlying
database engine. Still, the challenges in the NoSQL world are manifold, because of
the lack of standardization in data access. We focus on the specific case of mobility
data, and show how spatial and spatio-temporal operators, such as range queries and
k-nearest neighbor, are supported in a unified way. Moreover, we present challenges
and solutions for supporting spatial and spatio-temporal data in NoSQL stores.},
booktitle = {Proceedings of the 16th International Symposium on Spatial and Temporal Databases},
pages = {174–177},
numpages = {4},
keywords = {data access operators, spatio-temporal data, NoSQL},
location = {Vienna, Austria},
series = {SSTD '19}
}

@inproceedings{10.1145/2811222.2811235,
author = {Abell\'{o}, Alberto},
title = {Big Data Design},
year = {2015},
isbn = {9781450337854},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2811222.2811235},
doi = {10.1145/2811222.2811235},
abstract = {It is widely accepted today that Relational databases are not appropriate in highly
distributed shared-nothing architectures of commodity hardware, that need to handle
poorly structured heterogeneous data. This has brought the blooming of NoSQL systems
with the purpose of mitigating such problem, specially in the presence of analytical
workloads. Thus, the change in the data model and the new analytical needs beyond
OLAP take us to rethink methods and models to design and manage these newborn repositories.
In this paper, we will analyze state of the art and future research directions.},
booktitle = {Proceedings of the ACM Eighteenth International Workshop on Data Warehousing and OLAP},
pages = {35–38},
numpages = {4},
keywords = {nosql, database design, big data},
location = {Melbourne, Australia},
series = {DOLAP '15}
}

@article{10.1145/2674026.2674035,
author = {Piatetsky, Gregory},
title = {Interview: Michael Brodie, Leading Database Researcher, Industry Leader, Thinker},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1931-0145},
url = {https://doi.org/10.1145/2674026.2674035},
doi = {10.1145/2674026.2674035},
abstract = {We discuss the most important database research advances, industry developments, role
of relational and NoSQL databases, Computing Reality, Data Curation, Cloud Computing,
Tamr and Jisto startups, what he learned as a chief Scientist of Verizon, Knowledge
Discovery, Privacy Issues, and more.},
journal = {SIGKDD Explor. Newsl.},
month = sep,
pages = {57–63},
numpages = {7},
keywords = {verizon, privacy, cloud computing, NoSQL, data curation, computing reality}
}

@inproceedings{10.1145/2661136.2661151,
author = {Lorenz, David H. and Rosenan, Boaz},
title = {Versionable, Branchable, and Mergeable Application State},
year = {2014},
isbn = {9781450332101},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661136.2661151},
doi = {10.1145/2661136.2661151},
abstract = {NoSQL databases are rapidly becoming the storage of choice for large-scale Web applications.
However, for the sake of scalability these applications trade consistency for availability.
In this paper, we regain control over this trade-off by adapting an existing approach,
version control (VC), to application state. By using VC, the data model is defined
by the application and not by the database. The consistency model is determined at
runtime by deciding when to merge and with whom. We describe the design of a VC system
named VERCAST that provides fine-grained control over the consistency model used in
maintaining application state.},
booktitle = {Proceedings of the 2014 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming &amp; Software},
pages = {29–42},
numpages = {14},
keywords = {git, version control (vc), availability, conflict resolution, source control management (scm), nosql, optimistic replication, consistency, transactions},
location = {Portland, Oregon, USA},
series = {Onward! 2014}
}

@inproceedings{10.1145/2457317.2457351,
author = {Holzschuher, Florian and Peinl, Ren\'{e}},
title = {Performance of Graph Query Languages: Comparison of Cypher, Gremlin and Native Access in Neo4j},
year = {2013},
isbn = {9781450315999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2457317.2457351},
doi = {10.1145/2457317.2457351},
abstract = {NoSQL and especially graph databases are constantly gaining popularity among developers
of Web 2.0 applications as they promise to deliver superior performance when handling
highly interconnected data compared to traditional relational databases. Apache Shindig
is the reference implementation for OpenSocial with its highly interconnected data
model. However, the default back-end is based on a relational database. In this paper
we describe our experiences with a different back-end based on the graph database
Neo4j and compare the alternatives for querying data with each other and the JPA-based
sample back-end running on MySQL. Moreover, we analyze why the different approaches
often may yield such diverging results concerning throughput. The results show that
the graph-based back-end can match and even outperform the traditional JPA implementation
and that Cypher is a promising candidate for a standard graph query language, but
still leaves room for improvements.},
booktitle = {Proceedings of the Joint EDBT/ICDT 2013 Workshops},
pages = {195–204},
numpages = {10},
keywords = {graph query processing and performance optimization, Neo4j, graph query processing for social networks, benchmarks, graph databases, NoSQL},
location = {Genoa, Italy},
series = {EDBT '13}
}

@inproceedings{10.1145/3345509.3349278,
author = {Rubart, Jessica},
title = {On Managing Spatial Hypermedia with Document Stores},
year = {2019},
isbn = {9781450368995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345509.3349278},
doi = {10.1145/3345509.3349278},
abstract = {In spatial hypertext, linking is implicit. Users can use spatial and visual attributes,
such as proximity, shape, or color, to describe the information space. Spatial hypertext
models are special. For example, they need a flexible handling of attributes. Using
a relational database management system (DBMS) in the backend, leads to an impedance
mismatch problem as spatial hypertext and relational database approaches build on
different structural paradigms. In this paper, we introduce the spatial hypertext
impedance mismatch and argue for a special structure store to avoid or, at least,
reduce the introduced problem. Document stores appear to be an intuitive way or at
least a good starting point for providing an intuitive way to store, retrieve, and
manage spatial hypermedia structures. In this paper, we analyze document stores in
more detail in order to estimate their suitability as spatial hypermedia structure
store.},
booktitle = {Proceedings of the 2nd International Workshop on Human Factors in Hypertext},
pages = {13–18},
numpages = {6},
keywords = {impedance mismatch, spatial hypertext, document store, spatial hypertext impedance mismatch, nosql},
location = {Hof, Germany},
series = {HUMAN '19}
}

@inproceedings{10.1145/2896825.2896828,
author = {Zareian, Saeed and Fokaefs, Marios and Khazaei, Hamzeh and Litoiu, Marin and Zhang, Xi},
title = {A Big Data Framework for Cloud Monitoring},
year = {2016},
isbn = {9781450341523},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896825.2896828},
doi = {10.1145/2896825.2896828},
abstract = {Elasticity is a key component of modern cloud environments and monitoring is an essential
part of this process. Monitoring demonstrates several challenges including gathering
metrics from a variety of layers (infrastructure, platform, application), the need
for fast processing of this data to enable efficient elasticity and the proper management
of this data in order to facilitate analysis of current and past data and future predictions.
In this work, we classify monitoring as a big data problem and propose appropriate
solutions in a layered, pluggable and extendable architecture for a monitoring component.
More specifically, we propose the use of NoSQL databases as the back-end and BigQueue
as a write buffer to achieve high throughput. Our evaluation shows that our monitoring
is capable of achieving response time of a few hundreds of milliseconds for the insertion
of hundreds of rows regardless of the underlying NoSQL database.},
booktitle = {Proceedings of the 2nd International Workshop on BIG Data Software Engineering},
pages = {58–64},
numpages = {7},
keywords = {cloud applications, big data, performance analysis, monitoring system, NoSQL datastores},
location = {Austin, Texas},
series = {BIGDSE '16}
}

@inproceedings{10.1145/2912845.2912869,
author = {Luyen, LE Ngoc and Tireau, Anne and Venkatesan, Aravind and Neveu, Pascal and Larmande, Pierre},
title = {Development of a Knowledge System for Big Data: Case Study to Plant Phenotyping Data},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912869},
doi = {10.1145/2912845.2912869},
abstract = {In the recent years, the data deluge in many areas of scientific research brings challenges
in the treatment and improvement of agricultural data. Research in bioinformatics
field does not outside this trend. This paper presents some approaches aiming to solve
the Big Data problem by combining the increase in semantic search capacity on existing
data in the plant research laboratories. This helps us to strengthen user experiments
on the data obtained in this research by infering new knowledge. To achieve this,
there exist several approaches having different characteristics and using different
platforms. Nevertheless, we can summarize it in two main directions: the query re-writing
and data transformation to RDF graphs. In reality, we can solve the problem from origin
of increasing capacity on semantic data with triplets. Thus, data transformation to
RDF graphs direction was chosen to work on the practical part. However, the synchronization
data in the same format is required before processing the triplets because our current
data are heterogeneous. The data obtained for triplets are larger that regular triplestores
could manage. So we evaluate some of them thus we can compare the benefits and drawbacks
of each and choose the best system for our problem.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {27},
numpages = {9},
keywords = {Triplestore, Reasoning, SPARQL, Ontology, NoSQL, Knowledge base, Inference, Big Data, Benchmark, xR2RML},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/3410352.3410788,
author = {Yertay, Dias},
title = {Overview of the Method for Choosing the Most Suitable Database System According to Certain Criteria},
year = {2020},
isbn = {9781450377362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410352.3410788},
doi = {10.1145/3410352.3410788},
abstract = {Nowadays, information technology is one of the key component of ongoing projects in
all sectors. Medicine, mechanical engineering, the banking sector, the agro-industry,
all of them are on one level or another side of using information technologies and
information systems is to predict, calculate or facilitate automatic actions. Information
systems do not only simplify daily tasks, even also makes it possible to use resources
optimally. However, the information systems require using of optimal solutions and
the choice of implementation technologies by themselves. As a result, it seems relevant
to develop an automated system for calculating the most suitable database for projects.
The purpose of the research is to develop the system for selecting database, also
taking into an account the possibility of using hybrid SQL and NoSQL technologies.
This article provides an overview also analysis existing types of databases and a
comparative analysis method for choosing a system.},
booktitle = {Proceedings of the 6th International Conference on Engineering &amp; MIS 2020},
articleno = {52},
numpages = {7},
keywords = {The automated system, Analyzing the integration of SQL and NoSQL technologies, Information Technology},
location = {Almaty, Kazakhstan},
series = {ICEMIS'20}
}

@inproceedings{10.1145/3205977.3205998,
author = {Colombo, Pietro and Ferrari, Elena},
title = {Access Control in the Era of Big Data: State of the Art and Research Directions},
year = {2018},
isbn = {9781450356664},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3205977.3205998},
doi = {10.1145/3205977.3205998},
abstract = {Data security and privacy issues are magnified by the volume, the variety, and the
velocity of Big Data and by the lack, up to now, of a standard data model and related
data manipulation language. In this paper, we focus on one of the key data security
services, that is, access control, by highlighting the differences with traditional
data management systems and describing a set of requirements that any access control
solution for Big Data platforms may fulfill. We then describe the state of the art
and discuss open research issues.},
booktitle = {Proceedings of the 23nd ACM on Symposium on Access Control Models and Technologies},
pages = {185–192},
numpages = {8},
keywords = {access control, privacy, NOSQL data management systems, big data},
location = {Indianapolis, Indiana, USA},
series = {SACMAT '18}
}

@inproceedings{10.1145/2381716.2381806,
author = {Chakraborty, Sushan and Sarkar, Madhulina and Mukherjee, Nandini},
title = {Implementation of Execution History in Non-Relational Databases for Feedback-Guided Job Modeling},
year = {2012},
isbn = {9781450311854},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2381716.2381806},
doi = {10.1145/2381716.2381806},
abstract = {A feedback-guided Resource Requirement Prediction technique has been described in
[2]. An Execution History is built and maintained for all the jobs which are executed
in a large distributed system like Grid. When a new job arrives, its clones are sought
for in the Execution History and if clones are found, relevant performance information
are retrieved and used for estimating resource requirements. In this paper, we focus
on the implementation details of Execution History. Instead of using relational databases,
here we have used NoSQL database, MongoDB. The reasons for using MongoDB are discussed.
Techniques for storing and retrieving data from the Execution History are also described.
Overheads for such operations are measured and presented in the result part of this
paper.},
booktitle = {Proceedings of the CUBE International Information Technology Conference},
pages = {476–482},
numpages = {7},
keywords = {metrics, clone detection, execution history, MongoDB},
location = {Pune, India},
series = {CUBE '12}
}

@inproceedings{10.1145/3295500.3356191,
author = {Netti, Alessio and M\"{u}ller, Micha and Auweter, Axel and Guillen, Carla and Ott, Michael and Tafani, Daniele and Schulz, Martin},
title = {From Facility to Application Sensor Data: Modular, Continuous and Holistic Monitoring with DCDB},
year = {2019},
isbn = {9781450362290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3295500.3356191},
doi = {10.1145/3295500.3356191},
abstract = {Today's HPC installations are highly-complex systems, and their complexity will only
increase as we move to exascale and beyond. At each layer, from facilities to systems,
from runtimes to applications, a wide range of tuning decisions must be made in order
to achieve efficient operation. This, however, requires systematic and continuous
monitoring of system and user data. While many insular solutions exist, a system for
holistic and facility-wide monitoring is still lacking in the current HPC ecosystem.In
this paper we introduce DCDB, a comprehensive monitoring system capable of integrating
data from all system levels. It is designed as a modular and highly-scalable framework
based on a plugin infrastructure. All monitored data is aggregated at a distributed
noSQL data store for analysis and cross-system correlation. We demonstrate the performance
and scalability of DCDB, and describe two use cases in the area of energy management
and characterization.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {64},
numpages = {27},
keywords = {infrastructure management, distributed data store, monitoring, high-performance computing, application analysis},
location = {Denver, Colorado},
series = {SC '19}
}

@inproceedings{10.1145/3456172.3456214,
author = {Naka, Edjola and Guliashki, Vassil},
title = {Optimization Techniques in Data Management: A Survey},
year = {2021},
isbn = {9781450388450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3456172.3456214},
doi = {10.1145/3456172.3456214},
abstract = {Data Management can be defined as the process of extracting, storing, organizing,
and maintaining the&nbsp;data&nbsp;created and collected in organizations. Today's organizations
invest in data management solutions that provide an efficient way to manage data in
a unified structure. The enormously growth of data in the last decades has created
a necessity for the fast extracting, accessing, and processing of the data. Optimization
has been a key component in improving&nbsp;the system's performance, searching and accessing
data in different data management solutions. Optimization is a mathematical discipline
that formulates mathematical models and finds the best solution among a set of feasible
solutions. This paper aims to give a general overview of applications of optimization
techniques and algorithms in different areas of data management in the last decades.
Data management includes a large group of functionalities, but we will focus on studying
and reviewing the recent development of optimization algorithms used in databases,
data warehouses, big data and machine learning. Furthermore, this paper will identify
applications of optimization in data management, reviews the current solutions proposed
and emphasize future topics where there is a lack of studies in data management.},
booktitle = {2021 7th International Conference on Computing and Data Engineering},
pages = {8–13},
numpages = {6},
keywords = {Optimization, Machine Learning, Database, Big Data},
location = {Phuket, Thailand},
series = {ICCDE 2021}
}

@inproceedings{10.5555/3049877.3049891,
author = {Fokaefs, Marios and Barna, Cornel and Veleda, Rodrigo and Litoiu, Marin and Wigglesworth, Joe and Mateescu, Radu},
title = {Enabling Devops for Containerized Data-Intensive Applications: An Exploratory Study},
year = {2016},
publisher = {IBM Corp.},
address = {USA},
abstract = {In an ever-changing landscape of software technology, new development paradigms, novel
infrastructure technologies and emerging application domains reveal exciting opportunities,
but also unprecedented challenges for developers, practitioners and software engineers.
Amongst this innovation, containers as infrastructure support, data-intensive application
as a domain and DevOps as a development paradigm have gained significant popularity
recently. In this work, we focus on these concepts and present an exploratory study
on how to develop such applications, deploy and deliver them in Docker containers
and eventually manage them by enabling autoscaling on the container level. In the
paper, we detail our experimental process pointing out the problems we encountered
along with the solutions we used. Eventually, we present a set of stable experiments
to demonstrate the autoscaling capabilities we achieved.},
booktitle = {Proceedings of the 26th Annual International Conference on Computer Science and Software Engineering},
pages = {138–148},
numpages = {11},
keywords = {cloud computing, devops, data analytics, autoscaling, big data, adaptive systems, containers},
location = {Toronto, Ontario, Canada},
series = {CASCON '16}
}

@inproceedings{10.1145/2693561.2693563,
author = {Klein, John and Gorton, Ian},
title = {Runtime Performance Challenges in Big Data Systems},
year = {2015},
isbn = {9781450333405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2693561.2693563},
doi = {10.1145/2693561.2693563},
abstract = {Big data systems are becoming pervasive. They are distributed systems that include
redundant processing nodes, replicated storage, and frequently execute on a shared 'cloud' infrastructure. For these systems, design-time predictions are insufficient
to assure runtime performance in production. This is due to the scale of the deployed
system, the continually evolving workloads, and the unpredictable quality of service
of the shared infrastructure. Consequently, a solution for addressing performance
requirements needs sophisticated runtime observability and measurement. Observability
gives real-time insights into a system's health and status, both at the system and
application level, and provides historical data repositories for forensic analysis,
capacity planning, and predictive analytics. Due to the scale and heterogeneity of
big data systems, significant challenges exist in the design, customization and operations
of observability capabilities. These challenges include economical creation and insertion
of monitors into hundreds or thousands of computation and data nodes, efficient, low
overhead collection and storage of measurements (which is itself a big data problem),
and application-aware aggregation and visualization. In this paper we propose a reference
architecture to address these challenges, which uses a model-driven engineering toolkit
to generate architecture-aware monitors and application-specific visualizations.},
booktitle = {Proceedings of the 2015 Workshop on Challenges in Performance Methods for Software Development},
pages = {17–22},
numpages = {6},
keywords = {big data, observability, model-driven engineering},
location = {Austin, Texas, USA},
series = {WOSP '15}
}

@inproceedings{10.1145/2666310.2666481,
author = {Lee, Kisung and Ganti, Raghu K. and Srivatsa, Mudhakar and Liu, Ling},
title = {Efficient Spatial Query Processing for Big Data},
year = {2014},
isbn = {9781450331319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2666310.2666481},
doi = {10.1145/2666310.2666481},
abstract = {Spatial queries are widely used in many data mining and analytics applications. However,
a huge and growing size of spatial data makes it challenging to process the spatial
queries efficiently. In this paper we present a lightweight and scalable spatial index
for big data stored in distributed storage systems. Experimental results show the
efficiency and effectiveness of our spatial indexing technique for different spatial
queries.},
booktitle = {Proceedings of the 22nd ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {469–472},
numpages = {4},
keywords = {spatial query, big data, spatial indexing},
location = {Dallas, Texas},
series = {SIGSPATIAL '14}
}

@inproceedings{10.1145/3401335.3401356,
author = {Phani, Chitti S.},
title = {Data Collection and Evaluation Framework for Local Energy Systems},
year = {2020},
isbn = {9781450375955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3401335.3401356},
doi = {10.1145/3401335.3401356},
abstract = {Increasing presence of distributed energy resources (DERs) is evolving electricity
distribution system to decentralized and active networks. Utilising the data emerging
from such networks is an essential step for developing sustainable future energy systems,
as such systems must be closely monitored and managed to avoid blackouts and failures.
The datasets within energy systems are very large, must be immutable, are often interrelated,
yet owned by competing organisations. How then should these datasets be stored and
shared? This is the main focus of my research. To provide a reliable way of collecting,
storing, sharing, and analysing energy systems data, I propose to use the newly emerging
distributed ledger technology. I will work towards developing a middleware-like platform
over distributed ledgers, which would aim to provide a reliable data storage solution
for immutable energy datasets, while also maintaining relationships between the datasets
and addressing the read and write latency and scalability problems of the present
distributed ledgers.},
booktitle = {Proceedings of the 7th International Conference on ICT for Sustainability},
pages = {185–188},
numpages = {4},
keywords = {data store, data access, distributed systems, blockchain, data relations, de-centralized energy, data model},
location = {Bristol, United Kingdom},
series = {ICT4S2020}
}

@article{10.1145/3150226,
author = {Pouyanfar, Samira and Yang, Yimin and Chen, Shu-Ching and Shyu, Mei-Ling and Iyengar, S. S.},
title = {Multimedia Big Data Analytics: A Survey},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3150226},
doi = {10.1145/3150226},
abstract = {With the proliferation of online services and mobile technologies, the world has stepped
into a multimedia big data era. A vast amount of research work has been done in the
multimedia area, targeting different aspects of big data analytics, such as the capture,
storage, indexing, mining, and retrieval of multimedia big data. However, very few
research work provides a complete survey of the whole pine-line of the multimedia
big data analytics, including the management and analysis of the large amount of data,
the challenges and opportunities, and the promising research directions. To serve
this purpose, we present this survey, which conducts a comprehensive overview of the
state-of-the-art research work on multimedia big data analytics. It also aims to bridge
the gap between multimedia challenges and big data solutions by providing the current
big data frameworks, their applications in multimedia analyses, the strengths and
limitations of the existing methods, and the potential future directions in multimedia
big data analytics. To the best of our knowledge, this is the first survey that targets
the most recent multimedia management techniques for very large-scale data and also
provides the research studies and technologies advancing the multimedia analyses in
this big data era.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {10},
numpages = {34},
keywords = {machine learning, 5V challenges, Big data analytics, data mining, retrieval, mobile multimedia, multimedia analysis, indexing, survey, multimedia databases}
}

@inproceedings{10.1145/2675744.2675752,
author = {Vaikuntam, Aparna and Perumal, Vinodh Kumar},
title = {Evaluation of Contemporary Graph Databases},
year = {2014},
isbn = {9781605588148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675744.2675752},
doi = {10.1145/2675744.2675752},
abstract = {Graph databases attempt to make the modelling and processing of highly interconnected
data, easier and more efficient by representing a system as a graph-like structure
of nodes and edges. The fundamental premise of Graph Databases, unlike relational,
is explicit and distinct definition of relationships and direct, non-index based access
of related nodes from a given node. Growth of graph databases happened in large part
with a need for complex processing of interconnected documents in the WWW and the
surge in social networking. What was initially proprietary research has now transformed
into a plethora of commercial and open source products, increasingly being adopted
outside the internet services industry. This paper attempts to evaluate several such
contemporary Graph Databases from a subjective feature-based and empirical performance-based
perspective.},
booktitle = {Proceedings of the 7th ACM India Computing Conference},
articleno = {6},
numpages = {10},
keywords = {databases, open source, survey, evaluation, subjective, graph databases, feature based, empirical, performance based},
location = {Nagpur, India},
series = {COMPUTE '14}
}

@inproceedings{10.1145/3331076.3331126,
author = {Zgolli, Asma and Collet, Christine and Bobineau, Christophe},
title = {DWS: A Data Placement Approach for Smart Grid Ecosystems},
year = {2019},
isbn = {9781450362498},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331076.3331126},
doi = {10.1145/3331076.3331126},
abstract = {In Smart grid ecosystems, it is important to carefully choose the placement of the
datasets across different kind of big data systems in order to achieve high performance
of the workloads and conformity with the business and data ecosystem. Our approach
for datasets placement is based on metadata about datasets, workloads, and systems.
This paper gives a general overview of the data placement module, proposes a high-level
design and data model for our solution and presents the placement criteria.},
booktitle = {Proceedings of the 23rd International Database Applications &amp; Engineering Symposium},
articleno = {39},
numpages = {5},
keywords = {recommendation, meta-data, data lakes, optimization, datasets, big data},
location = {Athens, Greece},
series = {IDEAS '19}
}

@article{10.1145/3012286,
author = {Amato, Flora and Moscato, Vincenzo and Picariello, Antonio and Colace, Francesco and Santo, Massimo De and Schreiber, Fabio A. and Tanca, Letizia},
title = {Big Data Meets Digital Cultural Heritage: Design and Implementation of SCRABS, A Smart Context-AwaRe Browsing Assistant for Cultural EnvironmentS},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1556-4673},
url = {https://doi.org/10.1145/3012286},
doi = {10.1145/3012286},
abstract = {Information and Communication Technologies have radically changed the modern Cultural
Heritage scenery: Simple traditional Information Systems supporting the management
of cultural artifacts have left the place to complex systems that expose rich information
extracted from heterogeneous data sources—like Sensor Networks, Social Networks, Digital
Libraries, Multimedia Collections, Web Data Service, and so on—by means of sophisticated
applications that enhance the users’ experience. In this article, we describe SCRABS,
a Smart Context-awaRe Browsing assistant for cultural EnvironmentS. SCRABS has been
developed during the Cultural Heritage Information Systems national project and promoted
by DATABENC, the Cultural Heritage Technological District of the Campania Region,
in Italy. SCRABS has been designed on top of a Big Data technological stack as the
result of a multidisciplinary project carried out by a heterogeneous team of computer
scientists, archeologists, architects, and experts in humanities. We describe the
main ideas that support the system, showing its use in some real application scenarios
located in the Paestum Archeologica Sites.},
journal = {J. Comput. Cult. Herit.},
month = apr,
articleno = {6},
numpages = {23},
keywords = {multimedia, cultural heritage, Big data}
}

@inproceedings{10.1145/3468791.3468820,
author = {Ton That, Dai Hai and Gharehdaghi, Mohammadsaleh and Rasin, Alexander and Malik, Tanu},
title = {On Lowering Merge Costs of an LSM Tree},
year = {2021},
isbn = {9781450384131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468791.3468820},
doi = {10.1145/3468791.3468820},
abstract = { In column stores, which ingest large amounts of data into multiple column groups,
query performance deteriorates. Commercial column stores use log-structured merge
(LSM) tree on projections to ingest data rapidly. LSM tree improves ingestion performance,
but for column stores the sort-merge maintenance phase in an LSM tree is I/O-intensive,
which slows concurrent queries and reduces overall throughput. In this paper, we present
a simple heuristic approach to reduce the sorting and merging cost that arise when
data is ingested in column stores. We demonstrate how a Min-Max heuristic can construct
buckets and identify the level of sortedness in each range of data. Filled and relatively-sorted
buckets are written out to disk; unfilled buckets are retained to achieve a better
level of sortedness, thus avoiding the expensive sort-merge phase. We compare our
Min-Max approach with LSM tree and production columnar stores using real and synthetic
datasets. },
booktitle = {33rd International Conference on Scientific and Statistical Database Management},
pages = {253–258},
numpages = {6},
keywords = {write-optimized, Min-Max, column-oriented},
location = {Tampa, FL, USA},
series = {SSDBM 2021}
}

@inproceedings{10.1145/2897845.2897852,
author = {Yuan, Xingliang and Wang, Xinyu and Wang, Cong and Qian, Chen and Lin, Jianxiong},
title = {Building an Encrypted, Distributed, and Searchable Key-Value Store},
year = {2016},
isbn = {9781450342339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897845.2897852},
doi = {10.1145/2897845.2897852},
abstract = {Modern distributed key-value stores are offering superior performance, incremental
scalability, and fine availability for data-intensive computing and cloud-based applications.
Among those distributed data stores, the designs that ensure the confidentiality of
sensitive data, however, have not been fully explored yet. In this paper, we focus
on designing and implementing an encrypted, distributed, and searchable key-value
store. It achieves strong protection on data privacy while preserving all the above
prominent features of plaintext systems. We first design a secure data partition algorithm
that distributes encrypted data evenly across a cluster of nodes. Based on this algorithm,
we propose a secure transformation layer that supports multiple data models in a privacy-preserving
way, and implement two basic APIs for the proposed encrypted key-value store. To enable
secure search queries for secondary attributes of data, we leverage searchable symmetric
encryption to design the encrypted secondary indexes which consider security, efficiency,
and data locality simultaneously, and further enable secure query processing in parallel.
For completeness, we present formal security analysis to demonstrate the strong security
strength of the proposed designs. We implement the system prototype and deploy it
to a cluster at Microsoft Azure. Comprehensive performance evaluation is conducted
in terms of Put/Get throughput, Put/Get latency under different workloads, system
scaling cost, and secure query performance. The comparison with Redis shows that our
prototype can function in a practical manner.},
booktitle = {Proceedings of the 11th ACM on Asia Conference on Computer and Communications Security},
pages = {547–558},
numpages = {12},
keywords = {searchable encryption, key-value store},
location = {Xi'an, China},
series = {ASIA CCS '16}
}

@inproceedings{10.1145/2494621.2494630,
author = {Al-Shishtawy, Ahmad and Vlassov, Vladimir},
title = {ElastMan: Elasticity Manager for Elastic Key-Value Stores in the Cloud},
year = {2013},
isbn = {9781450321723},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494621.2494630},
doi = {10.1145/2494621.2494630},
abstract = {The increasing spread of elastic Cloud services, together with the pay-as-you-go pricing
model of Cloud computing, has led to the need of an elasticity controller. The controller
automatically resizes an elastic service in response to changes in workload, in order
to meet Service Level Objectives (SLOs) at a reduced cost. However, variable performance
of Cloud Virtual Machines and nonlinearities in Cloud services, such as the diminishing
reward of adding a service instance with increasing the scale, complicates the controller
design. We present the design and evaluation of ElastMan, an elasticity controller
for Cloud-based elastic key-value stores. ElastMan combines feedforward and feedback
control. Feedforward control is used to respond to spikes in the workload by quickly
resizing the service to meet SLOs at a minimal cost. Feedback control is used to correct
modeling errors and to handle diurnal workload. To address nonlinearities, our design
of ElastMan leverages the near-linear scalability of elastic Cloud services in order
to build a scale-independent model of the service. We have implemented and evaluated
ElastMan using the Voldemort key-value store running in an OpenStack Cloud environment.
Our evaluation shows the feasibility and effectiveness of our approach to automation
of Cloud service elasticity.},
booktitle = {Proceedings of the 2013 ACM Cloud and Autonomic Computing Conference},
articleno = {7},
numpages = {10},
keywords = {feedforward control, SLO, elasticity controller, feedback control, cloud computing, cloud storage},
location = {Miami, Florida, USA},
series = {CAC '13}
}

@article{10.14778/3352063.3352141,
author = {Li, Feifei},
title = {Cloud-Native Database Systems at Alibaba: Opportunities and Challenges},
year = {2019},
issue_date = {August 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3352063.3352141},
doi = {10.14778/3352063.3352141},
abstract = {Cloud-native databases become increasingly important for the era of cloud computing,
due to the needs for elasticity and on-demand usage by various applications. These
challenges from cloud applications present new opportunities for cloud-native databases
that cannot be fully addressed by traditional on-premise enterprise database systems.
A cloud-native database leverages software-hardware co-design to explore accelerations
offered by new hardware such as RDMA, NVM, kernel bypassing protocols such as DPDK.
Meanwhile, new design architectures, such as shared storage, enable a cloud-native
database to decouple computation from storage and provide excellent elasticity. For
highly concurrent workloads that require horizontal scalability, a cloud-native database
can leverage a shared-nothing layer to provide distributed query and transaction processing.
Applications also require cloud-native databases to offer high availability through
distributed consensus protocols.At Alibaba, we have explored a suite of technologies
to design cloud-native database systems. Our storage engine, X-Engine and PolarFS,
improves both write and read throughputs by using a LSM-tree design and self-adapted
separation of hot and cold data records. Based on these efforts, we have designed
and implemented POLARDB and its distributed version POLARDB-X, which has successfully
supported the extreme transaction workloads during the 2018 Global Shopping Festival
on November 11, 2018, and achieved commercial success on Alibaba Cloud. We have also
designed an OLAP system called AnalyticDB (ADB in short) for enabling real-time interactive
data analytics for big data. We have explored a self-driving database platform to
achieve autoscaling and intelligent database management. We will report key technologies
and lessons learned to highlight the technical challenges and opportunities for cloud-native
database systems at Alibaba.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {2263–2272},
numpages = {10}
}

@inproceedings{10.1145/3357223.3362715,
author = {Chen, Hongzhi and Li, Changji and Fang, Juncheng and Huang, Chenghuan and Cheng, James and Zhang, Jian and Hou, Yifan and Yan, Xiao},
title = {Grasper: A High Performance Distributed System for OLAP on Property Graphs},
year = {2019},
isbn = {9781450369732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357223.3362715},
doi = {10.1145/3357223.3362715},
abstract = {The property graph (PG) model is one of the most general graph data model and has
been widely adopted in many graph analytics and processing systems. However, existing
systems suffer from poor performance in terms of both latency and throughput for processing
online analytical workloads on PGs due to their design defects such as expensive interactions
with external databases, low parallelism, and high network overheads. In this paper,
we propose Grasper, a high performance distributed system for OLAP on property graphs.
Grasper adopts RDMA-aware system designs to reduce the network communication cost.
We propose a novel query execution model, called Expert Model, which supports adaptive
parallelism control at the fine-grained query operation level and allows tailored
optimizations for different categories of query operators, thus achieving high parallelism
and good load balancing. Experimental results show that Grasper achieves low latency
and high throughput on a broad range of online analytical workloads.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {87–100},
numpages = {14},
keywords = {Distributed Systems, RDMA, Graph Database, OLAP},
location = {Santa Cruz, CA, USA},
series = {SoCC '19}
}

@article{10.1145/3408314,
author = {Davoudian, Ali and Liu, Mengchi},
title = {Big Data Systems: A Software Engineering Perspective},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3408314},
doi = {10.1145/3408314},
abstract = {Big Data Systems (BDSs) are an emerging class of scalable software technologies whereby
massive amounts of heterogeneous data are gathered from multiple sources, managed,
analyzed (in batch, stream or hybrid fashion), and served to end-users and external
applications. Such systems pose specific challenges in all phases of software development
lifecycle and might become very complex by evolving data, technologies, and target
value over time. Consequently, many organizations and enterprises have found it difficult
to adopt BDSs. In this article, we provide insight into three major activities of
software engineering in the context of BDSs as well as the choices made to tackle
them regarding state-of-the-art research and industry efforts. These activities include
the engineering of requirements, designing and constructing software to meet the specified
requirements, and software/data quality assurance. We also disclose some open challenges
of developing effective BDSs, which need attention from both researchers and practitioners.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {110},
numpages = {39},
keywords = {Big Data, requirements engineering, software reference architecture, quality assurance, Big Data systems, software engineering}
}

@inproceedings{10.5555/2685048.2685068,
author = {Yuan, Ding and Luo, Yu and Zhuang, Xin and Rodrigues, Guilherme Renna and Zhao, Xu and Zhang, Yongle and Jain, Pranay U. and Stumm, Michael},
title = {Simple Testing Can Prevent Most Critical Failures: An Analysis of Production Failures in Distributed Data-Intensive Systems},
year = {2014},
isbn = {9781931971164},
publisher = {USENIX Association},
address = {USA},
abstract = {Large, production quality distributed systems still fail periodically, and do so sometimes
catastrophically, where most or all users experience an outage or data loss. We present
the result of a comprehensive study investigating 198 randomly selected, user-reported
failures that occurred on Cassandra, HBase, Hadoop Distributed File System (HDFS),
Hadoop MapReduce, and Redis, with the goal of understanding how one or multiple faults
eventually evolve into a user-visible failure. We found that from a testing point
of view, almost all failures require only 3 or fewer nodes to reproduce, which is
good news considering that these services typically run on a very large number of
nodes. However, multiple inputs are needed to trigger the failures with the order
between them being important. Finally, we found the error logs of these systems typically
contain sufficient data on both the errors and the input events that triggered the
failure, enabling the diagnose and the reproduction of the production failures.We
found the majority of catastrophic failures could easily have been prevented by performing
simple testing on error handling code - the last line of defense - even without an
understanding of the software design. We extracted three simple rules from the bugs
that have lead to some of the catastrophic failures, and developed a static checker,
Aspirator, capable of locating these bugs. Over 30% of the catastrophic failures would
have been prevented had Aspirator been used and the identified bugs fixed. Running
Aspirator on the code of 9 distributed systems located 143 bugs and bad practices
that have been fixed or confirmed by the developers.},
booktitle = {Proceedings of the 11th USENIX Conference on Operating Systems Design and Implementation},
pages = {249–265},
numpages = {17},
location = {Broomfield, CO},
series = {OSDI'14}
}

@inproceedings{10.1145/3132402.3132428,
author = {del Campo, Fernando Martin and Chow, Paul},
title = {Task Replication and Control for Highly Parallel In-Memory Stores},
year = {2017},
isbn = {9781450353359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132402.3132428},
doi = {10.1145/3132402.3132428},
abstract = {The computing world has made significant investments and developments around the von
Neumann architecture. This means that applications have been highly tuned to the CPU-based
server, which may not be the best platform for some applications. In this paper, we
examine a class of large-scale, data-intensive applications by first abstracting away
the architectural details to the essential operations. By doing this, we can study
the intrinsic behavior and requirements of these applications and then find the hardware
architecture that can implement the required operations most efficiently.Data-intensive
applications require the execution of several tasks to complete specific functions.
Even though, in many cases, there is a certain degree of parallelization that is achieved
through multi-threading processes on one or more cores, memory contention and architectural
constraints limit the amount of work that can be done in a concurrent fashion. Using
an alternative consumer-producer paradigm, this work presents the analysis of the
performance of a particular type of data-intensive applications (key-value stores)
under different traffic patterns. We replicate diverse stages and components of the
process, and the control of the system is altered to eliminate idle times. Results
show that very simple hardware modules can perform more efficiently than current solutions.
Specific combinations of features and replication of tasks are as much as one order
of magnitude faster than the baseline (traditional commodity servers).},
booktitle = {Proceedings of the International Symposium on Memory Systems},
pages = {312–326},
numpages = {15},
location = {Alexandria, Virginia},
series = {MEMSYS '17}
}

