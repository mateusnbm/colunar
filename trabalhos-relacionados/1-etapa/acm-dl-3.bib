@inproceedings{10.1145/3427228.3427260,
author = {Ferrari, Dario and Carminati, Michele and Polino, Mario and Zanero, Stefano},
title = {NoSQL Breakdown: A Large-Scale Analysis of Misconfigured NoSQL Services},
year = {2020},
isbn = {9781450388580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427228.3427260},
doi = {10.1145/3427228.3427260},
abstract = {In the last years, NoSQL databases have grown in popularity due to their easy-to-deploy,
reliable, and scalable storage mechanism. While most NoSQL services offer access control
mechanisms, their default configurations grant access without any form of authentication,
resulting in misconfigurations that may expose data to the Internet, as demonstrated
by the recent high-profile data leaks. In this paper, we investigate the usage of
the most popular NoSQL databases, focusing on automatically analyzing and discovering
misconfigurations that may lead to security and privacy issues. We developed a tool
that automatically scans large IP subnets to detect the exposed services and performs
security analyses without storing nor exposing sensitive data. We analyzed 67,725,641
IP addresses between October 2019 and March 2020, spread across several Cloud Service
Providers (CSPs), and found 12,276 misconfigured databases. The risks associated with
exposed services range from data leaking, which may pose a significant menace to users’
privacy, to data tampering of resources stored in the vulnerable databases, which
may pose a relevant threat to a web service reputation. Regarding the last point,
we found 742 potentially vulnerable websites linked to misconfigured instances with
the write permission enabled to anonymous users. },
booktitle = {Annual Computer Security Applications Conference},
pages = {567–581},
numpages = {15},
keywords = {database and storage security, NoSQL services, vulnerabilities., misconfiguration},
location = {Austin, USA},
series = {ACSAC '20}
}

@inproceedings{10.1145/3341525.3387399,
author = {Kim, Suneuy},
title = {Seamless Integration of NoSQL Class into the Database Curriculum},
year = {2020},
isbn = {9781450368742},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341525.3387399},
doi = {10.1145/3341525.3387399},
abstract = {Since NoSQL emerged a decade ago, it has rapidly gained popularity and has been actively
incorporated into data management solutions for big data. This phenomenon brings positive
challenges to accommodate NoSQL topics in the database curriculum. This paper presents
our experience of teaching a NoSQL class over the last three years. The course uses
a comprehensive teaching methodology that combines lectures, hands-on assignments,
projects, and research-based approaches. The methodology aims at both students' in-depth
learning and seamless integration of NoSQL topics into the database curriculum. The
teaching methodology and course contents are detailed. Student evaluations of teaching,
assessment results, success stories, and challenges and lessons learned are presented.},
booktitle = {Proceedings of the 2020 ACM Conference on Innovation and Technology in Computer Science Education},
pages = {314–320},
numpages = {7},
keywords = {NoSQL, education, databases, big data, curriculum},
location = {Trondheim, Norway},
series = {ITiCSE '20}
}

@inproceedings{10.1145/2835596.2835614,
author = {Liu, Kuien and Yao, Yandong and Guo, Danhuai},
title = {On Managing Geospatial Big-Data in Emergency Management: Some Perspectives},
year = {2015},
isbn = {9781450339704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2835596.2835614},
doi = {10.1145/2835596.2835614},
abstract = {With the rapid growth of mobile devices and applications, geo-tagged data is becoming
increasingly important in emergency management and has become a major workload for
big data storage systems. Traditional methods that storing geospatial data in centralized
databases suffer from inevitable limitations such like scaling out with the growing
size of geospatial data. In order to achieve scalability, a number of solutions on
big geospatial data management are proposed in recent years. We can simply classify
them into two kinds: extending on distributed databases, or migrating to big-data
storage systems. For previous, they mostly adopt the massive parallel processing (MPP)
based architecture, in which data are stored and retrieved in a set of independent
nodes. Each node can be treated as a traditional databases instance with geospatial
extension. For the latter, existing solutions tend to build an additional index layer
above general-purpose distributed data stores, e.g., HBASE, CASSANDRA, MangoDB, etc.,
to support geospatial data while integrating the big-data lineage. However, there
are no absolutely perfect data management systems on the earth. Some approaches are
desired for execution efficiency while some others are better on fulfilling the programming
level need for big data scenarios.In this paper, we analysis the requirements and
challenges on geospatial big data storage in emergency management, succeed with discussion
with individual perspective from practical cases. The purpose of this paper is not
only focused on how to program a geospatial data storage platform but also on how
to approve the rationality of geospatial big data system that we plan to build.},
booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on the Use of GIS in Emergency Management},
articleno = {5},
numpages = {4},
keywords = {geospatial, emergency management, big data, perspectives},
location = {Bellevue, Washington},
series = {EM-GIS '15}
}

@inproceedings{10.1145/2882903.2899412,
author = {Pandey, Varun and Kipf, Andreas and Vorona, Dimitri and M\"{u}hlbauer, Tobias and Neumann, Thomas and Kemper, Alfons},
title = {High-Performance Geospatial Analytics in HyPerSpace},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2899412},
doi = {10.1145/2882903.2899412},
abstract = {In the past few years, massive amounts of location-based data has been captured. Numerous
datasets containing user location information are readily available to the public.
Analyzing such datasets can lead to fascinating insights into the mobility patterns
and behaviors of users. Moreover, in recent times a number of geospatial data-driven
companies like Uber, Lyft, and Foursquare have emerged. Real-time analysis of geospatial
data is essential and enables an emerging class of applications. Database support
for geospatial operations is turning into a necessity instead of a distinct feature
provided by only a few databases. Even though a lot of database systems provide geospatial
support nowadays, queries often do not consider the most current database state. Geospatial
queries are inherently slow given the fact that some of these queries require a couple
of geometric computations. Disk-based database systems that do support geospatial
datatypes and queries, provide rich features and functions, but they fall behind when
performance is considered: specifically if real-time analysis of the latest transactional
state is a requirement. In this demonstration, we present HyPerSpace, an extension
to the high-performance main-memory database system HyPer developed at the Technical
University of Munich, capable of processing geospatial queries with sub-second latencies.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {2145–2148},
numpages = {4},
keywords = {indexing schemes, geospatial data processing},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@inproceedings{10.1145/2666310.2666410,
author = {Patroumpas, Kostas and Giannopoulos, Giorgos and Athanasiou, Spiros},
title = {Towards GeoSpatial Semantic Data Management: Strengths, Weaknesses, and Challenges Ahead},
year = {2014},
isbn = {9781450331319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2666310.2666410},
doi = {10.1145/2666310.2666410},
abstract = {An immense wealth of data is already accessible through the Semantic Web and an increasing
part of it also has geospatial context or relevance. Although existing technology
is mature enough to integrate a variety of information from heterogeneous sources
into interlinked features, it still falls behind when it comes to representation and
reasoning on spatial characteristics. It is only lately that several RDF stores have
begun to accommodate geospatial entities and to enable some kind of processing on
them. To address interoperability, the OGC has recently adopted the GeoSPARQL standard,
which defines a vocabulary for representing geometric types in RDF and an extension
to the SPARQL language for formulating queries. In this paper, we provide a comprehensive
review of the current state-of-the-art in geospatially-enabled semantic data management.
Apart from an insightful analysis of the available architectures in industry and academia,
we conduct an evaluation study on prominent RDF stores with geospatial support. We
also compare their performance and attested capabilities to renowned DBMSs widely
used in geospatial applications. We introduce a methodology suitable to assess RDF
stores for robustness against large geospatial datasets, and also for expressiveness
on a variety of queries involving both spatial and thematic criteria. As our findings
demonstrate, the potential for query optimization, advanced indexing schemes, and
spatio-semantic extensions is significant. Towards this goal, we point out several
challenging issues for joint research by the GIS and Semantic Web communities.},
booktitle = {Proceedings of the 22nd ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {301–310},
numpages = {10},
keywords = {triple store, geospatial linked data, RDF, GeoSPARQL, evaluation},
location = {Dallas, Texas},
series = {SIGSPATIAL '14}
}

@inproceedings{10.1145/3356395.3365598,
author = {Roy, Avipsa and Fouch\'{e}, Edouard and Morales, Rafael Rodriguez and M\"{o}hler, Gregor},
title = {In-Database Geospatial Analytics Using Python},
year = {2019},
isbn = {9781450369541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356395.3365598},
doi = {10.1145/3356395.3365598},
abstract = {The amount of spatial data acquired from crowdsourced platforms, mobile devices, sensors
and cartographic agencies has grown exponentially over the past few years. Nearly
half of the spatial data available currently are stored and processed through large
relational databases. Due to a lack of generic open source tools, researchers and
analysts often have difficulty in extracting and analyzing large amounts of spatial
data from traditional databases. In order to overcome this challenge, the most effective
way is to perform the analysis directly in the database, which enables quick retrieval
and visualization of spatial data stored in relational databases. Also, working in-database
reduces the network overhead, as users do not need to replicate the complete data
into their local system. While a number of spatial analysis libraries are readily
available, they do not work in-database, and typically require additional platform-specific
software. Our goal is to bridge this gap by developing a new method through an open
source software to perform fast and seamless spatial analysis without having to store
the data in-memory. We propose a framework implemented in Python, which embeds geospatial
analytics into a spatial database (i.e. IBM DB2 ®). The framework internally translates
the spatial functions written by the user into SQL queries, which follow the standards
of Open Geospatial Consortium (OGC) and can operate on single as well as multiple
geometries. We then demonstrate how to combine the results of spatial operations with
visualization methods such as choropleth maps within Jupyter notebooks. Finally, we
elaborate upon the benefits of our approach via a real-world use case, in which we
analyze crime hotspots in New York City using the in-database spatial functions.},
booktitle = {Proceedings of the 2nd ACM SIGSPATIAL International Workshop on Advances on Resilient and Intelligent Cities},
pages = {17–24},
numpages = {8},
keywords = {Maps, Python, geospatial analytics, crime analysis, In-database analytics, spatial data},
location = {Chicago, IL, USA},
series = {ARIC'19}
}

@inproceedings{10.1145/2987491.2987494,
author = {Butgereit, Laurie},
title = {Four NoSQLs in Four Fun Fortnights: Exploring NoSQLs in a Corporate IT Environment},
year = {2016},
isbn = {9781450348058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2987491.2987494},
doi = {10.1145/2987491.2987494},
abstract = {The argument of SQL vs NoSQL always requires the answer "It depends." If the participants
in the argument about which database is better or more appropriate in a given situation
have no background in any NoSQL database, then it is difficult to make informed decisions.
In general, NoSQL databases arrange themselves into four groups: key-value stores,
columnar databases, graph databases, and document databases. This paper describes
a project to use gamification to encourage members of a busy corporate IT department
to learn about the four groups of NoSQLs and give them brief hands-on experiences
with one of each of the four categories: Redis, Cassandra, Neo4j, and MongoDB. The
encouragement was done by using a combination of physical and virtual treasure hunts.
Eight fun treasure hunts were created (two for each group of NoSQL) where participants
in the office either worked individually or teamed up with other participants to find
data hidden in each of the different types of NoSQL database. This data would then
lead to a physical treasure box full of chocolates which was hidden in the office
common area.},
booktitle = {Proceedings of the Annual Conference of the South African Institute of Computer Scientists and Information Technologists},
articleno = {8},
numpages = {6},
keywords = {neo4j, redis, mongodb, noSQL, cassandra},
location = {Johannesburg, South Africa},
series = {SAICSIT '16}
}

@inproceedings{10.1109/CCGrid.2014.57,
author = {Gao, Xiaoming and Qiu, Judy},
title = {Supporting Queries and Analyses of Large-Scale Social Media Data with Customizable and Scalable Indexing Techniques over NoSQL Databases},
year = {2014},
isbn = {9781479927838},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2014.57},
doi = {10.1109/CCGrid.2014.57},
abstract = {Social media data analysis demonstrates two special characteristics in Big Data processing.
First, most analyses focus on data subsets related to specific social events or activities
instead of the whole dataset. Second, analysis workflows consist of multiple stages,
and algorithms applied in each stage may use different computation and communication
patterns depending on processing frameworks. This paper presents our efforts in supporting
the data storage and processing requirements for such characteristics. To achieve
efficient queries about target data subsets, we propose a general customizable and
scalable indexing framework that can be built over distributed NoSQL databases. This
framework allows users to define suitable customized index structures for their query
patterns against social media data, and supports scalable indexing of both historical
and streaming data. We implement this framework on HBase, and name it IndexedHBase.
Starting from IndexedHBase, we build a distributed analysis stack based on YARN to
support analysis algorithms using different processing frameworks, such as Hadoop
MapReduce, Harp, and Giraph. This analysis stack is used to host the Truthy social
media data observatory, and we have applied the customized index structures in supporting
both query evaluation and sophisticated analysis algorithms. Performance tests show
that our solutions outperform implementations using both direct raw data scans and
current indexing mechanisms in existing NoSQL databases.},
booktitle = {Proceedings of the 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {587–590},
numpages = {4},
keywords = {YARN, customizable and scalable indexing, NoSQL databases, social media data analysis},
location = {Chicago, Illinois},
series = {CCGRID '14}
}

@inproceedings{10.1145/2896825.2896831,
author = {Scavuzzo, Marco and Tamburri, Damian A. and Di Nitto, Elisabetta},
title = {Providing Big Data Applications with Fault-Tolerant Data Migration across Heterogeneous NoSQL Databases},
year = {2016},
isbn = {9781450341523},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896825.2896831},
doi = {10.1145/2896825.2896831},
abstract = {The recent growing interest on highly-available data-intensive applications sparked
the need for flexible and portable storage technologies, e.g., NoSQL databases. Unfortunately,
the lack of standard interfaces and architectures for NoSQLs makes it difficult and
expensive to create portable applications, which results in vendor lock-in. Building
on previous work, we aim at providing guaranteed fault-tolerant techniques and supporting
architectures to port or migrate data to and across heterogeneous NoSQL technology.
To prove the effectiveness of our approach we evaluate it on an industrial case-study.
We conclude that our method and supporting architecture offer an efficient and fault-tolerant
mechanism for NoSQL portability and interoperation.},
booktitle = {Proceedings of the 2nd International Workshop on BIG Data Software Engineering},
pages = {26–32},
numpages = {7},
location = {Austin, Texas},
series = {BIGDSE '16}
}

@inproceedings{10.1145/3340964.3340981,
author = {Koutroumanis, Nikolaos and Nikitopoulos, Panagiotis and Vlachou, Akrivi and Doulkeridis, Christos},
title = {NoDA: Unified NoSQL Data Access Operators for Mobility Data},
year = {2019},
isbn = {9781450362801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340964.3340981},
doi = {10.1145/3340964.3340981},
abstract = {In this paper, we propose NoDA, an abstraction layer consisting of spatio-temporal
data access operators, which is used to access NoSQL storage engines in a unified
way. NoDA alleviates the burden from big data developers of learning the query language
of each NoSQL store, and offers a unified view of the underlying NoSQL store. Our
approach is inspired by the equivalent paradigm of drivers (such as JDBC) in the relational
database world, where the application code is indifferent to the exact underlying
database engine. Still, the challenges in the NoSQL world are manifold, because of
the lack of standardization in data access. We focus on the specific case of mobility
data, and show how spatial and spatio-temporal operators, such as range queries and
k-nearest neighbor, are supported in a unified way. Moreover, we present challenges
and solutions for supporting spatial and spatio-temporal data in NoSQL stores.},
booktitle = {Proceedings of the 16th International Symposium on Spatial and Temporal Databases},
pages = {174–177},
numpages = {4},
keywords = {NoSQL, data access operators, spatio-temporal data},
location = {Vienna, Austria},
series = {SSTD '19}
}

@inproceedings{10.1145/2851613.2851660,
author = {Tavares, Alberto Trindade and Oliveira, Marcelo Iury S. and L\'{o}scio, Bernadette Farias},
title = {Data Producer Catalogs for the Web of Things: A Study on NoSQL Solutions},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851660},
doi = {10.1145/2851613.2851660},
abstract = {The rise of Web of Things (WoT) has created new challenges for processing distributed
data, such as the large volume, heterogeneity and autonomy of data producers. Therefore,
efficient mechanisms for publishing and discovering data producers on the WoT become
necessary. One possible solution to help such tasks is to provide a catalog with data
producers' metadata. That catalog must be flexible, to deal with heterogeneity, and
scalable, to deal with massive volume of data producers and consumers. In order to
handle those requirements, different NoSQL solutions have been adopted. However, there
is still no consensus about the most suitable solution for cataloguing data producers
in the context of the WoT. In this work, different NoSQL databases are evaluated and
some guidance to help the identification of the most efficient catalog solution for
a given workload is also provided.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {980–985},
numpages = {6},
keywords = {web of things, internet of things, catalog, NoSQL},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/2338714.2338736,
author = {Dobo\v{s}, Jozef and Steed, Anthony},
title = {3D Revision Control Framework},
year = {2012},
isbn = {9781450314329},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2338714.2338736},
doi = {10.1145/2338714.2338736},
abstract = {The maintenance of assets in a large 3D scene can involve many authors with potentially
different skills and different modeling tools. The standard paradigm of collaborative
editing is sharing of files between various instances of applications. This presents
problems, not limited to maintaining consistency of the models and dealing with concurrent
edits in the same part of a scene. In this paper, we present a novel framework for
non-linear revision control and online distribution of 3D assets. This framework supports
concurrent asynchronous 3D editing of models and tracking of multiple revisions so
that they can be integrated at a later date. It thus provides similar functionality
to file-based revision control systems, but is built around a NoSQL database, hence
avoids the constraints of file based storage. The framework supports distributed editing
over the Internet and additional lightweight clients in web-browsers with WebGL support.},
booktitle = {Proceedings of the 17th International Conference on 3D Web Technology},
pages = {121–129},
numpages = {9},
keywords = {NoSQL, version tracking, revision history, Java liveconnect, WebGL client, 3D database},
location = {Los Angeles, California},
series = {Web3D '12}
}

@inproceedings{10.1145/2361999.2362039,
author = {Begoli, Edmon},
title = {A Short Survey on the State of the Art in Architectures and Platforms for Large Scale Data Analysis and Knowledge Discovery from Data},
year = {2012},
isbn = {9781450315685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361999.2362039},
doi = {10.1145/2361999.2362039},
abstract = {Intended as a survey for practicing architects and researchers seeking an overview
of the state-of-the-art architectures for data analysis, this paper provides an overview
of the emerging data management and analytic platforms including parallel databases,
Hadoop-based systems, High Performance Computing (HPC) platforms and platforms popularly
referred to as NoSQL platforms. Platforms are presented based on their relevance,
analysis they support and the data organization model they support.},
booktitle = {Proceedings of the WICSA/ECSA 2012 Companion Volume},
pages = {177–183},
numpages = {7},
keywords = {massively parallel processing, NoSQL, knowledge discovery from data, software architecture, large scale data analysis, big data},
location = {Helsinki, Finland},
series = {WICSA/ECSA '12}
}

@inproceedings{10.1145/3085504.3085528,
author = {Qader, Mohiuddin Abdul and Hristidis, Vagelis},
title = {DualDB: An Efficient LSM-Based Publish/Subscribe Storage System},
year = {2017},
isbn = {9781450352826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3085504.3085528},
doi = {10.1145/3085504.3085528},
abstract = {Publish/Subscribe systems allow subscribers to monitor for events of interest generated
by publishers. Current publish/subscribe query systems are efficient when the subscriptions
(queries) are relatively static -- for instance, the set of followers in Twitter --
or can fit in memory. However, an increasing number of applications in this era of
Big Data and Internet of Things (IoT) are based on a highly dynamic query paradigm,
where continuous queries are in the millions and are created and expire in a rate
comparable, or even higher, to that of the data (event) entries. For instance moving
objects like airplanes, cars or sensors may continuously generate measurement data
like air pressure or traffic, which are consumed by other moving objects.In this paper
we propose and compare a novel publish/subscribe storage architecture, DualDB, based
on the popular NoSQL Log-Structured Merge Tree (LSM) storage paradigm, to support
high-throughput and dynamic publish/subscribe systems. Our method naturally supports
queries on both past and future data, and generate instant notifications, which are
desirable properties missing from many previous systems. We implemented and experimentally
evaluated our methods on the popular LSM-based LevelDB system, using real datasets.
Our results show that we can achieve significantly higher throughput compared to state-of-the-art
baselines.},
booktitle = {Proceedings of the 29th International Conference on Scientific and Statistical Database Management},
articleno = {24},
numpages = {6},
keywords = {Publish/Subscribe, Triggers, NoSQL, Log-Structured Merge Tree, Continuous Query, Instant Notification, LevelDB, Big Data},
location = {Chicago, IL, USA},
series = {SSDBM '17}
}

@inproceedings{10.1145/3331076.3331101,
author = {Makris, Antonios and Tserpes, Konstantinos and Anagnostopoulos, Dimosthenis and Nikolaidou, Mara and de Macedo, Jose Ant\^{o}nio Fernandes},
title = {Database System Comparison Based on Spatiotemporal Functionality},
year = {2019},
isbn = {9781450362498},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331076.3331101},
doi = {10.1145/3331076.3331101},
abstract = {The amount of sources and sheer volumes of spatiotemporal data have met an unprecedented
growth during the last decade. As a consequence, a rapidly increasing number of applications
are seeking to generate value by crunching those data. The development of a system
that will tap into the potential value of the spatiotemporal big data analysis for
a multitude of applications remains one of the biggest challenges in computer engineering.
This paper delves into the key-characteristics of the most prominent suchlike systems.
In particular, it provides a thorough analysis of NoSQL datastores as well as a traditional
relational database system in terms of their geospatial querying capabilities.},
booktitle = {Proceedings of the 23rd International Database Applications &amp; Engineering Symposium},
articleno = {21},
numpages = {7},
keywords = {data stores, spatio-temporal databases, spatio-temporal characteristics, geospatial functionality},
location = {Athens, Greece},
series = {IDEAS '19}
}

@inproceedings{10.1145/2384716.2384777,
author = {Webber, Jim},
title = {A Programmatic Introduction to Neo4j},
year = {2012},
isbn = {9781450315630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2384716.2384777},
doi = {10.1145/2384716.2384777},
abstract = {In this workshop we provide a hands-on introduction to the popular open source graph
database Neo4j [1] through fixing a series of increasingly sophisticated, but broken,
test cases each of which highlights an important graph modeling or API affordance.},
booktitle = {Proceedings of the 3rd Annual Conference on Systems, Programming, and Applications: Software for Humanity},
pages = {217–218},
numpages = {2},
keywords = {neo4j, jvm, java, nosql, graph databases},
location = {Tucson, Arizona, USA},
series = {SPLASH '12}
}

@inproceedings{10.5555/3049877.3049893,
author = {Serrano, Diego and Stroulia, Eleni},
title = {From Relations to Multi-Dimensional Maps: A SQL-to-HBase Transformation Methodology},
year = {2016},
publisher = {IBM Corp.},
address = {USA},
abstract = {In this paper, we describe a methodology for migrating applications relying on relational
databases to HBase backends. Our methodology includes (a) a SQL-to-HBASE data-schema
migration step, and (b) a transformation of the application SQL queries to equivalent
sequences of HBase API calls. Our data-schema migration method relies on a set of
HBase-organization guidelines to drive a four-step data-schema transformation process.
Some of these guidelines are query-agnostic: we defined them based on related literature
regarding the desired properties of the HBase organization. Other guidelines are query-aware:
we formulated them to incorporate data-access paths, extracted from query logs, in
order to improve the quality of the transformation and the eventual access efficiency
of the HBase repository. Our transformation method maintains a mapping between source
and target schema that is used to create sequences of HBase API calls, equivalent
to SQL queries in the relational database. We illustrate and validate our method with
a case study and a comprehensive performance evaluation.},
booktitle = {Proceedings of the 26th Annual International Conference on Computer Science and Software Engineering},
pages = {156–165},
numpages = {10},
keywords = {program translation, NoSQL, distributed databases, HBase, database design and modeling, data translation},
location = {Toronto, Ontario, Canada},
series = {CASCON '16}
}

@inproceedings{10.1145/3105831.3105841,
author = {Costa, Carlos and Santos, Maribel Yasmina},
title = {The SusCity Big Data Warehousing Approach for Smart Cities},
year = {2017},
isbn = {9781450352208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3105831.3105841},
doi = {10.1145/3105831.3105841},
abstract = {Nowadays, the concept of Smart City provides a rich analytical context, highlighting
the need to store and process vast amounts of heterogeneous data flowing at different
velocities. This data is defined as Big Data, which imposes significant difficulties
in traditional data techniques and technologies. Data Warehouses (DWs) have long been
recognized as a fundamental enterprise asset, providing fact-based decision support
for several organizations. The concept of DW is evolving. Traditionally, Relational
Database Management Systems (RDBMSs) are used to store historical data, providing
different analytical perspectives regarding several business processes. With the current
advancements in Big Data techniques and technologies, the concept of Big Data Warehouse
(BDW) emerges to surpass several limitations of traditional DWs. This paper presents
a novel approach for designing and implementing BDWs, which has been supporting the
SusCity data visualization platform. The BDW is a crucial component of the SusCity
research project in the context of Smart Cities, supporting analytical tasks based
on data collected in the city of Lisbon.},
booktitle = {Proceedings of the 21st International Database Engineering &amp; Applications Symposium},
pages = {264–273},
numpages = {10},
keywords = {Hadoop, Data Warehouse, Smart Cities, Big Data Warehousing, NoSQL, Big Data},
location = {Bristol, United Kingdom},
series = {IDEAS 2017}
}

@inproceedings{10.1145/2345316.2345339,
author = {Deng, Hongli and Gunda, Kiran and Rasheed, Zeeshan and Haering, Niels},
title = {Retrieving Large-Scale High Density Video Target Tracks from Spatial Database},
year = {2012},
isbn = {9781450311137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2345316.2345339},
doi = {10.1145/2345316.2345339},
abstract = {With more and more live sensors being added to geospatial applications, huge amount
of sensor data are generated and saved in spatial database. Managing and mining these
large-scale ever-changing data becomes new challenges for geospatial studies. In this
paper, we present an application-oriented case study to show how to retrieve target
tracking data from big dataset saved in spatial database. Our video event retrieval
system collects thirty days (8790 GB) high definition video data from six surveillance
cameras, analyze them and extract roughly ten million video target tracks. These tracks
are projected onto world coordinates and pumped into a spatial database. The system
performance of inserting and retrieving these tracks is analyzed in terms of spatial
data type design, spatial index configuration, online operation capacity, query optimization
and scalability handling. Our insights of saving, managing and retrieving target tracks
in a large-scale are presented.},
booktitle = {Proceedings of the 3rd International Conference on Computing for Geospatial Research and Applications},
articleno = {19},
numpages = {8},
keywords = {large-scale spatial data retrieval, video event search, spatial index, video surveillance, spatial database},
location = {Washington, D.C., USA},
series = {COM.Geo '12}
}

@inproceedings{10.1145/3347146.3359076,
author = {Hu, Han and Phan, NhatHai and Ye, Xinyue and Jin, Ruoming and Ding, Kele and Dou, Dejing and Vo, Huy T.},
title = {DrugTracker: A Community-Focused Drug Abuse Monitoring and Supporting System Using Social Media and Geospatial Data (Demo Paper)},
year = {2019},
isbn = {9781450369091},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3347146.3359076},
doi = {10.1145/3347146.3359076},
abstract = {In this paper, we present a community-focused drug abuse monitoring and supporting
system, called DrugTracker, that utilizes social media and geospatial data in near
real-time. Through the system, users can: (1) Detect drug abuse risk behaviors from
social media platforms, e.g., Twitter; (2) Analyze drug abuse risk behaviors by querying
consolidated and live datasets with keywords, spatial entities, and time constraints;
and (3) Explore the query results and associated data through a web-based user interface
in thematic choropleth, heatmap, and statistical charts. To protect the privacy of
the Twitter users, whose data is collected, the system automatically hides the re-identification
elements in tweets and aggregates the geo-tags into areas such as census tracts. For
the demonstration purpose, our DrugTracker system is populated with a database that
contains about 10 million tweets from the year 2017, that were annotated as drug abuse
risk behavior positive by our deep learning model.},
booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {564–567},
numpages = {4},
keywords = {deep learning, social media, visualization, drug abuse},
location = {Chicago, IL, USA},
series = {SIGSPATIAL '19}
}

@inproceedings{10.1145/2447481.2447486,
author = {Zhong, Yunqin and Zhu, Xiaomin and Fang, Jinyun},
title = {Elastic and Effective Spatio-Temporal Query Processing Scheme on Hadoop},
year = {2012},
isbn = {9781450316927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2447481.2447486},
doi = {10.1145/2447481.2447486},
abstract = {Geospatial applications have become prevalent in both scientific research and industry.
Spatio-Temporal query processing is a fundamental issue for driving geospatial applications.
However, the state-of-the-art spatio-temporal query processing methods are facing
significant challenges as the data expand and concurrent users increase. In this paper
we present a novel spatio-temporal querying scheme to provide efficient query processing
over big geospatial data. The scheme improves query efficiency from three facets.
Firstly, taking geographic proximity and storage locality into consideration, we propose
a geospatial data organization approach to achieve high aggregate I/O throughput,
and design a distributed indexing framework for efficient pruning of the search space.
Furthermore, we design an indexing plus MapReduce query processing architecture to
improve data retrieval efficiency and query computation efficiency. In addition, we
design distributed caching model to accelerate the access response of hotspot spatial
objects. We evaluate the effectiveness of our scheme with comprehensive experiments
using real datasets and application scenarios.},
booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data},
pages = {33–42},
numpages = {10},
keywords = {spatial database, spatio-temporal query, geographic information system, Hadoop, cloud},
location = {Redondo Beach, California},
series = {BigSpatial '12}
}

@inproceedings{10.1145/3338906.3340454,
author = {Miryeganeh, Nima and Amoui, Mehdi and Hemmati, Hadi},
title = {An IR-Based Approach towards Automated Integration of Geo-Spatial Datasets in Map-Based Software Systems},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3340454},
doi = {10.1145/3338906.3340454},
abstract = {Data is arguably the most valuable asset of the modern world. In this era, the success
of any data-intensive solution relies on the quality of data that drives it. Among
vast amount of data that are captured, managed, and analyzed everyday, geospatial
data are one of the most interesting class of data that hold geographical information
of real-world phenomena and can be visualized as digital maps. Geo-spatial data is
the source of many enterprise solutions that provide local information and insights.
Companies often aggregate geospacial datasets from various sources in order to increase
the quality of such solutions. However, a lack of a global standard model for geospatial
datasets makes the task of merging and integrating datasets difficult and error prone.
Traditionally, this aggregation was accomplished by domain experts manually validating
the data integration process by merging new data sources and/or new versions of previous
data against conflicts and other requirement violations. However, this manual approach
is not scalable is a hinder toward rapid release when dealing with big datasets which
change frequently. Thus more automated approaches with limited interaction with domain
experts is required. As a first step to tackle this problem, we have leveraged Information
Retrieval (IR) and geospatial search techniques to propose a systematic and automated
conflict identification approach. To evaluate our approach, we conduct a case study
in which we measure the accuracy of our approach in several real-world scenarios and
followed by interviews with Localintel Inc. software developers to get their feedbacks.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {946–954},
numpages = {9},
keywords = {Map-based Software, Geospatial Datasets, Text Mining, Data Intensive Software, TFIDF, Data Integration},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3006386.3006391,
author = {Sorokine, Alexandre and Karthik, Rajasekar and King, Anthony and Budhendra, Bhaduri},
title = {Big Data as a Service from an Urban Information System},
year = {2016},
isbn = {9781450345811},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3006386.3006391},
doi = {10.1145/3006386.3006391},
abstract = {Big Data has already proven itself as a valuable tool that lets geographers and urban
researchers utilize large data resources to generate new insights. However, wider
adoption of Big Data techniques in these areas is impeded by a number of difficulties
in both knowledge discovery and data and science production. Typically users face
such problems as disparate and scattered data, data management, spatial searching,
insufficient computational capacity for data-driven analysis and modelling, and the
lack of tools to quickly visualize and summarize large data and analysis results.
Here we propose an architecture for an Urban Information System (UrbIS) that mitigates
these problems by utilizing the Big Data as a Service (BDaaS) concept. With technological
roots in High-performance Computing (HPC), BDaaS is based on the idea of outsourcing
computations to different computing paradigms, scalable to super-computers. UrbIS
aims to incorporate federated metadata search, integrated modeling and analysis, and
geovisualization into a single seamless workflow. The system is under active development
and is built around various emerging technologies that include hybrid and NoSQL databases,
massively parallel systems, GPU computing, and WebGL-based geographic visualization.
UrbIS is designed to facilitate the use of Big Data across multiple cities to better
understand how urban areas impact the environment and how climate change and other
environmental change impact urban areas.},
booktitle = {Proceedings of the 5th ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data},
pages = {34–41},
numpages = {8},
keywords = {environmental change impact, urban informatics, high-performance geocomputing, big data as a service},
location = {Burlingame, California},
series = {BigSpatial '16}
}

@article{10.1145/3185047,
author = {Basole, Rahul C. and Srinivasan, Arjun and Park, Hyunwoo and Patel, Shiv},
title = {Ecoxight: Discovery, Exploration, and Analysis of Business Ecosystems Using Interactive Visualization},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3185047},
doi = {10.1145/3185047},
abstract = {The term ecosystem is used pervasively in industry, government, and academia to describe
the complex, dynamic, hyperconnected nature of many social, economic, and technical
systems that exist today. Ecosystems are characterized by a large, dynamic, and heterogeneous
set of geospatially distributed entities that are interconnected through various types
of relationships. This study describes the design and development of ecoxight, a Web-based
visualization platform that provides multiple coordinated views of multipartite, multiattribute,
dynamic, and geospatial ecosystem data with novel and rich interaction capabilities
to augment decision makers ecosystem intelligence. The design of ecoxight was informed
by an extensive multiphase field study of executives. The ecoxight platform not only
provides capabilities to interactively explore and make sense of ecosystems but also
provides rich visual construction capabilities to help decision makers align their
mental model. We demonstrate the usability, utility, and value of our system using
multiple evaluation studies with practitioners using socially curated data on the
emerging application programming interface ecosystem. We report on our findings and
conclude with research implications. Collectively, our study contributes to design
science research at the intersection of information systems and strategy and the rapidly
emerging field of visual enterprise analytics.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = apr,
articleno = {6},
numpages = {26},
keywords = {multipartite graph, geospatial data, Network visualization, temporal network}
}

@article{10.1145/3323214,
author = {Lu, Jiaheng and Holubov\'{a}, Irena},
title = {Multi-Model Databases: A New Journey to Handle the Variety of Data},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3323214},
doi = {10.1145/3323214},
abstract = {The variety of data is one of the most challenging issues for the research and practice
in data management systems. The data are naturally organized in different formats
and models, including structured data, semi-structured data, and unstructured data.
In this survey, we introduce the area of multi-model DBMSs that build a single database
platform to manage multi-model data. Even though multi-model databases are a newly
emerging area, in recent years, we have witnessed many database systems to embrace
this category. We provide a general classification and multi-dimensional comparisons
for the most popular multi-model databases. This comprehensive introduction on existing
approaches and open problems, from the technique and application perspective, make
this survey useful for motivating new multi-model database approaches, as well as
serving as a technical reference for developing multi-model database applications.},
journal = {ACM Comput. Surv.},
month = jun,
articleno = {55},
numpages = {38},
keywords = {NoSQL database management systems, Big data management, multi-model databases}
}

@inproceedings{10.1145/2905055.2905202,
author = {Kumar, Sunil and Shekhar, Jayant and Gupta, Himanshu},
title = {Agent Based Security Model for Cloud Big Data},
year = {2016},
isbn = {9781450339629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2905055.2905202},
doi = {10.1145/2905055.2905202},
abstract = {As we know that digitization is one of boon of 21st century technologies. With the
massstorage of digital information and development of internet based technologies
like cloud computing, researcher interest has been increased in Big Data and its security.
The term Big Data refers to the huge amount of digital information. Actually, Big
Data is not a fully new technology; but it is the expansion of data mining technique.
In this paper, we propose an agent based security model for cloud big data. The main
objective of this security model is to facilitate the IT companies in term of data
protection; those are using Cloud Big Data for the analyzing purpose.},
booktitle = {Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies},
articleno = {142},
numpages = {5},
keywords = {Agent Based Security, Cloud Security, Big Data Security, NoSQL Security},
location = {Udaipur, India},
series = {ICTCS '16}
}

@inproceedings{10.1145/3356999.3365467,
author = {Bakli, Mohamed and Sakr, Mahmoud and Zimanyi, Esteban},
title = {Distributed Moving Object Data Management in MobilityDB},
year = {2019},
isbn = {9781450369664},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356999.3365467},
doi = {10.1145/3356999.3365467},
abstract = {The availability of moving object data that is being collected nowadays, and the demand
of using them in applications, have generated the need for spatiotemporal data management
systems. MobilityDB is an open source moving object database system. Its core function
is to efficiently store and query moving object trajectories. It is engineered up
from PostgreSQL and PostGIS, providing spatiotemporal data management via SQL. In
order to store and analyze the massive datasets of trajectories, a scalable version
is required. In this paper, we present a solution to distribute MobilityDB using Citus.
Citus is a PostgreSQL extension for distributed query processing. We report on the
integration architecture, and the types of queries that can be distributed out of
the box. The experiments prove the feasibility of the solution, and show a significant
speed up in queries.},
booktitle = {Proceedings of the 8th ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data},
articleno = {1},
numpages = {10},
keywords = {sharding, distributed query planning, trajectory},
location = {Chicago, Illinois},
series = {BigSpatial '19}
}

@inproceedings{10.1145/2064959.2064962,
author = {Kazemitabar, Seyed Jalal and Banaei-Kashani, Farnoush and McLeod, Dennis},
title = {Geostreaming in Cloud},
year = {2011},
isbn = {9781450310369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2064959.2064962},
doi = {10.1145/2064959.2064962},
abstract = {In recent years, geospatial databases have been commercialized and widely exposed
to mass users. Current exponential growth in data generation and querying rates for
these data highlights the importance of efficient techniques for streaming. Traditional
database technology, which operates on persistent and less dynamic data objects does
not meet the requirements for efficient geospatial data streaming. Geostreaming, the
intersection of data stream processing and geospatial querying, is an ongoing research
focus in this area. In this paper, we describe why cloud is the most appropriate infrastructure
in which to support geospatial stream data processing. First, we argue that cloud
best fits the requirements of a large-scale geostreaming application. Second, we propose
ElaStream, a general cloud-based streaming infrastructure that enables huge parallelism
by means of the divide, conquer, and combine paradigm. Third, we examine key related
work in the data streaming and (geo)spatial database fields, and describe the challenges
ahead to build scalable cloud-based geostreaming applications.},
booktitle = {Proceedings of the 2nd ACM SIGSPATIAL International Workshop on GeoStreaming},
pages = {3–9},
numpages = {7},
keywords = {spatial databases, cloud computing, geostreaming, data stream processing},
location = {Chicago, Illinois},
series = {IWGS '11}
}

@inproceedings{10.1145/2837060.2837062,
author = {Cho, Wonhee and Choi, Eunmi},
title = {A GPS Trajectory Map-Matching Mechanism with DTG Big Data on the HBase System},
year = {2015},
isbn = {9781450338462},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2837060.2837062},
doi = {10.1145/2837060.2837062},
abstract = {Since smartphones equipped with GPS have been produced, the need to conduct an analysis
by matching the mass of GPS trajectory data on a digital map has increased. However,
the study of the existing map-matching algorithm technique is mainly for navigation.
In order to analyze large amounts of GPS trajectories on a server, issues of the speed
and performance of the system exist. The purpose of this study is to utilize a map-matching
system using HBase, which is a distributed NoSQL DB in a Hadoop ecosystem. We defined
the table specification of HBase for mounting the digital map and proposed and implemented
the method for analysis with a map-matching algorithm. In this paper, we present the
map-matching methodology using the NoSQL DB of Hadoop ecosystem for analyzing GPS
trajectory.},
booktitle = {Proceedings of the 2015 International Conference on Big Data Applications and Services},
pages = {22–29},
numpages = {8},
keywords = {map matching, Hadoop, Big data, HBase, spatial analysis},
location = {Jeju Island, Republic of Korea},
series = {BigDAS '15}
}

@inproceedings{10.1145/2407696.2407705,
author = {Dobo\v{s}, Jozef and Simondetti, Alvise and Steed, Anthony},
title = {Visualizing 3D Models in Aid of Public Consultation},
year = {2012},
isbn = {9781450319164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2407696.2407705},
doi = {10.1145/2407696.2407705},
abstract = {We present a novel approach to visualizing 3D models in aid of public consultation.
3D assets of architectural and engineering proposals are decomposed into individual
constituents and uploaded to a NoSQL database (DB). A newly developed Android application
visualizes such centrally stored assets on mobile devices. Localized comments are
pushed back to the DB for subsequent analysis. This concept is expected to improve
the public engagement and significantly reduce the costs of running such events.},
booktitle = {SIGGRAPH Asia 2012 Symposium on Apps},
articleno = {9},
numpages = {1},
location = {Singapore, Singapore},
series = {SA '12}
}

@inproceedings{10.1145/2949550.2949578,
author = {Soltani, Kiumars and Soliman, Aiman and Padmanabhan, Anand and Wang, Shaowen},
title = {UrbanFlow: Large-Scale Framework to Integrate Social Media and Authoritative Landuse Maps},
year = {2016},
isbn = {9781450347556},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2949550.2949578},
doi = {10.1145/2949550.2949578},
abstract = {Everyday massive amounts of geo-tagged information are generated around urban environment
using micro-blogging services and content sharing platforms. These new Big Geospatial
Data sources provide an opportunity to understand people activities and their interaction
with the urban environment. In this regard, it is crucial to integrate geo-tagged
micro-data with more authoritative sources such as official landuse maps. This integration
would benefit the urban research community by combining real time information about
people activities and their spatial interaction with the synoptic view of physical
infrastructure as depicted in official landuse maps. However, the scientific effort
for integrating heterogeneous data sources is hindered by the lack of scalable Geospatial
synthesis capabilities to accommodate the massive volume and fast update of microdata.We
developed UrbanFlow, a platform to integrate geolocated Twitter data with detailed
landuse map (parcel level) to detect and analyze individual human mobility patterns.
The platform provides scientists with a set of tools to extract key locations of each
Twitter user, assess the extraction quality and uncertainty, and analyze city neighbors'
connectivity based on detected users' frequent visitation patterns. These capabilities
are built on a novel scalable solution for the point in/nearest polygon algorithm,
implemented on Hadoop to harness the power of distributed systems to combine massive
point data and large number of polygon in scale-out fashion. Our results showed that
we are able to effectively process large data stream of Twitter data (2.42 billion
tweets) and synthesize that with highly detailed landuse map (468,641 parcels for
the city of Chicago).},
booktitle = {Proceedings of the XSEDE16 Conference on Diversity, Big Data, and Science at Scale},
articleno = {2},
numpages = {8},
keywords = {Data Synthesis, CyberGIS, Urban Mobility, Interactive Visualization, Social Media, Hadoop, Point in Polygon},
location = {Miami, USA},
series = {XSEDE16}
}

@inproceedings{10.1145/2451716.2451718,
author = {Yu, Liang and Liu, Yong and Lee, Jong},
title = {SSTDE: An Open Source Semantic Spatiotemporal Data Engine for Sensor Web},
year = {2012},
isbn = {9781450317016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2451716.2451718},
doi = {10.1145/2451716.2451718},
abstract = {Recently, many tools have emerged to manage sensor web data using Semantic Web technologies
for effective heterogeneous data integration. However, a remaining challenge is how
to manage the massive volumes of sensor data in their semantic form, i.e., Resource
Description Framework (RDF) triples. Our survey revealed that most semantic tools
either do not have geospatial support, or have severe limitations on providing full
GeoSPARQL support and good performance for complex queries. In this paper, we present
an open source Semantic Spatiotemporal Data Engine (SSTDE), which incorporates both
semantic tools and Geographic Information System (GIS) systems under a hybrid architecture.
Our main contribution includes 1) introducing the sub-graph index to substitute the
single node index, which results in significant performance gain for a spatiotemporal
query; 2) developing a query optimization algorithm based on graph matching; 3) proposing
a benchmark test for spatiotemporal query over triple stores. The spatiotemporal SPARQL
query is intelligently decomposed and executed on different systems, which significantly
improves the query performance by more than a hundred times comparing to other solutions.},
booktitle = {Proceedings of the First ACM SIGSPATIAL Workshop on Sensor Web Enablement},
pages = {9–16},
numpages = {8},
keywords = {sensor web, semantic triple store, spatiotemporal query, GeoSPARQL, graph index, hybrid architecture},
location = {Redondo Beach, California},
series = {SWE '12}
}

@inproceedings{10.1145/3129757.3129758,
author = {Timonin, Alexey Y. and Bozhday, Alexander S. and Bershadsky, Alexander M.},
title = {Analysis of Unstructured Text Data for a Person Social Profile},
year = {2017},
isbn = {9781450354127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3129757.3129758},
doi = {10.1145/3129757.3129758},
abstract = {The greatest scientific interest for analysts are Internet open social data, because
it has a direct link with all kinds of human activity. However, these data are not
suitable for the application in its original form. Information should be presented
in a structured, convenient, human-readable form which is called a social profile.
The social profile building is carried out through the analysis of the filtered Internet
open source data. Analysis of personal profile data is achieved through the use of
mathematical set theory, Big Data software, NoSQL data stores and analytic tools for
social media. This article discusses methods of unstructured textual data analysis
in relation to a social profile. Special attention is given to the search of implicit
dependences in texts using visual analysis and natural language processing means.
Phase of the textual data analysis is the most important in terms of results and complicated
to implement. There is the possibility to partially automate the process of information
analyzing through the use of visual analysis, natural language processing (NLP), neural
networks and specialized algorithms. Resulted data provide a detailed in-depth review
of the social profile entities and relations. It can be used in further deeper social
researches.},
booktitle = {Proceedings of the Internationsl Conference on Electronic Governance and Open Society: Challenges in Eurasia},
pages = {1–5},
numpages = {5},
keywords = {data mining, unstructured data, big data, natural language processing, personal social profile, social media, data analysis, visual analysis, public data sources, text analysis},
location = {St. Petersburg, Russia},
series = {eGose '17}
}

@inproceedings{10.1145/2534303.2534310,
author = {Hwang, Myung-Hwa and Wang, Shaowen and Cao, Guofeng and Padmanabhan, Anand and Zhang, Zhenhua},
title = {Spatiotemporal Transformation of Social Media Geostreams: A Case Study of Twitter for Flu Risk Analysis},
year = {2013},
isbn = {9781450325325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2534303.2534310},
doi = {10.1145/2534303.2534310},
abstract = {Georeferenced social media data streams (social media geostreams) are providing promising
opportunities to gain new insights into spatiotemporal aspects of human interactions
on cyber space and their relation with real-world activities. In particular, such
opportunities are motivating public health researchers to improve the surveillance
of disease epidemics by means of spatiotemporal analysis of social media geostreams.
One essential requirement in achieving such geostream-based disease surveillance is
to establish scalable data infrastructures capable of real-time transformation of
massive geostreams into spatiotemporally organized data to which analytical methods
are readily applicable. To fulfill this requirement, this study develops a data pipeline
solution where multiple computational components are integrated to collect, process,
and aggregate social media geostreams in near real time. As a test case, this solution
focuses on one well-known social media geostream, the Twitter data stream, and one
type of disease epidemics, the flu. The pipeline solution facilitates multiscale spatiotemporal
analysis of flu risks by collecting geotagged tweets from the Twitter Streaming API,
identifying flu-related tweets through keyword match, aggregating tweets at multiple
spatial granularities in near real time, and storing tweets and the aggregate statistics
in a distributed NoSQL database. Although developed for the surveillance of flu epidemics,
the pipeline would serve as a general framework for building scalable data infrastructures
that can support real-time spatiotemporal analysis of social media geostreams in the
application domains beyond disease mapping and public health.},
booktitle = {Proceedings of the 4th ACM SIGSPATIAL International Workshop on GeoStreaming},
pages = {12–21},
numpages = {10},
keywords = {data pipeline, disease surveillance, social media geostreams, spatiotemporal analysis},
location = {Orlando, Florida},
series = {IWGS '13}
}

@inproceedings{10.1145/3127479.3132254,
author = {Iyer, Anand Padmanabha and Stoica, Ion},
title = {A Scalable Distributed Spatial Index for the Internet-of-Things},
year = {2017},
isbn = {9781450350280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127479.3132254},
doi = {10.1145/3127479.3132254},
abstract = {The increasing interest in the Internet-of-Things (IoT) suggests that a new source
of big data is imminent---the machines and sensors in the IoT ecosystem. The fundamental
characteristic of the data produced by these sources is that they are inherently geospatial
in nature. In addition, they exhibit unprecedented and unpredictable skews. Thus,
big data systems designed for IoT applications must be able to efficiently ingest,
index and query spatial data having heavy and unpredictable skews. Spatial indexing
is well explored area of research in literature, but little attention has been given
to the topic of efficient distributed spatial indexing.In this paper, we propose Sift,
a distributed spatial index and its implementation. Unlike systems that depend on
load balancing mechanisms that kick-in post ingestion, Sift tries to distribute the
incoming data along the distributed structure at indexing time and thus incurs minimal
rebalancing overhead. Sift depends only on an underlying key-value store, hence is
implementable in many existing big data stores. Our evaluations of Sift on a popular
open source data store show promising results---Sift achieves up to 8\texttimes{} reduction in
indexing overhead while simultaneously reducing the query latency and index size by
over 2\texttimes{} and 3\texttimes{} respectively, in a distributed environment compared to the state-of-the-art.},
booktitle = {Proceedings of the 2017 Symposium on Cloud Computing},
pages = {548–560},
numpages = {13},
keywords = {distributed data store, spatial indexing, big data},
location = {Santa Clara, California},
series = {SoCC '17}
}

@inproceedings{10.1145/2070770.2070773,
author = {Shook, Eric and Wang, Shaowen},
title = {A Parallel Input-Output System for Resolving Spatial Data Challenges: An Agent-Based Model Case Study},
year = {2011},
isbn = {9781450310406},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2070770.2070773},
doi = {10.1145/2070770.2070773},
abstract = {With recent advances in data collection technologies such as remote sensing and global
positioning systems, the amount of spatial data being produced has been increasing
at a staggering rate. Simultaneously, a shift is being experienced in computing from
single-core to multi-core processors. To effectively utilize the computational power
afforded by these new generation of processors for serving data-intensive geospatial
applications, parallel computing techniques need to be employed. Parallel computing,
however, raises new challenges associated with handling the input and output of spatial
data in parallel. This paper describes a Parallel Input/Output System (PIOS) to address
challenges associated with handling large amounts of diverse spatial data. The PIOS
is based on a hierarchical structure that uses a scalable file partitioning strategy
and combines data and metadata to enable efficient handling of terabyte-scale data
sets in parallel. A spatially-explicit agent-based model is developed as a case study.
Computational experiments were conducted on a supercomputer supported by the National
Science Foundation. PIOS achieved ten times speedup in parallel input/output time,
and was demonstrated to efficiently scale to over one thousand processing cores and
handle multiple terabytes of data.},
booktitle = {Proceedings of the ACM SIGSPATIAL Second International Workshop on High Performance and Distributed Geographic Information Systems},
pages = {18–25},
numpages = {8},
location = {Chicago, Illinois},
series = {HPDGIS '11}
}

@inproceedings{10.1145/3396864.3399705,
author = {Qu, Chengyi and Morel, Alicia Esquivel and Dahlquist, Drew and Calyam, Prasad},
title = {DroneNet-Sim: A Learning-Based Trace Simulation Framework for Control Networking in Drone Video Analytics},
year = {2020},
isbn = {9781450380102},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396864.3399705},
doi = {10.1145/3396864.3399705},
abstract = {Unmanned Aerial Vehicles (UAVs) or drones equipped with cameras are extensively used
in different applications for environmental situational awareness such as: smart agriculture,
border security, intelligent transportation. Realistic UAV testbed building for developing
novel network control algorithms relating to video streaming/analytics is time-consuming
and difficult. Challenges arise when executing high-scale drone video analytics experiments
due to: constraints in drone manufacturing, government regulation restrictions, and
limited energy resources. Also, developing algorithms requires ability to understand
impact of network protocol selection to handle diverse UAV mobility models as well
as dynamic network status during edge-network video processing. In this paper, we
propose a novel learning-based trace simulation framework viz. "DroneNet-Sim" that
integrates simulation on both drone and networking sides. It allows for experimentation
with network protocol selection (i.e., TCP/HTTP, UDP/RTP, QUIC) and video properties
selection (i.e., codec, resolution) to ensure satisfactory video quality delivery
in different drone flight scenarios. Using machine learning models, we show how DroneNet-Sim
can process real-world drone traces that include various mobility models, geospatial
link information and on-time network status obtained from real-world data-gathering
efforts. Trace-based experiments with our DroneNet-Sim shows how video quality delivery
(i.e., PSNR) using suitable control networking matches real-world measurements in
terms of machine learning model accuracy.},
booktitle = {Proceedings of the 6th ACM Workshop on Micro Aerial Vehicle Networks, Systems, and Applications},
articleno = {8},
numpages = {6},
keywords = {machine learning, trace-based simulation, networking protocols, drone video analytics},
location = {Toronto, Ontario, Canada},
series = {DroNet '20}
}

@article{10.14778/2536222.2536227,
author = {Aji, Ablimit and Wang, Fusheng and Vo, Hoang and Lee, Rubao and Liu, Qiaoling and Zhang, Xiaodong and Saltz, Joel},
title = {Hadoop GIS: A High Performance Spatial Data Warehousing System over Mapreduce},
year = {2013},
issue_date = {August 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/2536222.2536227},
doi = {10.14778/2536222.2536227},
abstract = {Support of high performance queries on large volumes of spatial data becomes increasingly
important in many application domains, including geospatial problems in numerous fields,
location based services, and emerging scientific applications that are increasingly
data- and compute-intensive. The emergence of massive scale spatial data is due to
the proliferation of cost effective and ubiquitous positioning technologies, development
of high resolution imaging technologies, and contribution from a large number of community
users. There are two major challenges for managing and querying massive spatial data
to support spatial queries: the explosion of spatial data, and the high computational
complexity of spatial queries. In this paper, we present Hadoop-GIS - a scalable and
high performance spatial data warehousing system for running large scale spatial queries
on Hadoop. Hadoop-GIS supports multiple types of spatial queries on MapReduce through
spatial partitioning, customizable spatial query engine RESQUE, implicit parallel
spatial query execution on MapReduce, and effective methods for amending query results
through handling boundary objects. Hadoop-GIS utilizes global partition indexing and
customizable on demand local spatial indexing to achieve efficient query processing.
Hadoop-GIS is integrated into Hive to support declarative spatial queries with an
integrated architecture. Our experiments have demonstrated the high efficiency of
Hadoop-GIS on query response and high scalability to run on commodity clusters. Our
comparative experiments have showed that performance of Hadoop-GIS is on par with
parallel SDBMS and outperforms SDBMS for compute-intensive queries. Hadoop-GIS is
available as a set of library for processing spatial queries, and as an integrated
software package in Hive.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1009–1020},
numpages = {12}
}

@inproceedings{10.1145/3357141.3357603,
author = {Azevedo, Leonardo Guerreiro and Ferreira, Rodrigo da Silva and Silva, Viviane Torres da and de Bayser, Maximillien and Soares, Elton F. de S. and Thiago, Raphael Melo},
title = {Geological Data Access on a Polyglot Database Using a Service Architecture},
year = {2019},
isbn = {9781450376372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357141.3357603},
doi = {10.1145/3357141.3357603},
abstract = {In a microservice architecture, solutions are built through collaboration of distributed
services across networks. In the Oil &amp; Gas industry, in exploration and production
phases, organization units executes different services over several diverse datasets.
Geological data usually is in high volume and encompasses different kinds of data
objects, with diverse structure and nature, such as seismic data, seismic horizon
and well data. Querying, processing, and composing geological data presents strong
demands for domain knowledge representation and reasoning, and tailored processing
techniques. This work presents an application of microservices architecture and polyglot
persistence technologies to handle the requirements of geological data in the Oil
&amp; Gas domain. This architecture allows parties communicate in a light way, while encapsulating
processing and data access to a geological database and to a knowledge base. It also
works as a common layer, composing parties' services results for data consumption
by clients. We exemplify the proposal by presenting and analyzing its use in a real
scenario which includes some of the implemented queries in a developed system to support
geological data analysis. We present the main characteristics of the system and highlights
lessons learned in its development.},
booktitle = {Proceedings of the XIII Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {103–112},
numpages = {10},
keywords = {reasoning, microservice architecture, polyglot persistence, geological data},
location = {Salvador, Brazil},
series = {SBCARS '19}
}

@inproceedings{10.1145/3323878.3325807,
author = {Holubov\'{a}, Irena and Scherzinger, Stefanie},
title = {Unlocking the Potential of NextGen Multi-Model Databases for Semantic Big Data Projects},
year = {2019},
isbn = {9781450367660},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323878.3325807},
doi = {10.1145/3323878.3325807},
abstract = {A new vision in semantic big data processing is to create enterprise data hubs, with
a 360° view on all data that matters to a corporation. As we discuss in this paper,
a new generation of multi-model database systems seems a promising architectural choice
for building such scalable, non-native triple stores. In this paper, we first characterize
this new generation of multi-model databases. Then, discussing an example scenario,
we show how they allow for agile and flexible schema management, spanning a large
design space for creative and incremental data modelling. We identify the challenge
of generating sound triple-views from data stored in several, interlinked models,
for SPARQL querying. We regard this as one of several appealing research challenges
where the semantic big data and the database architecture community may join forces.},
booktitle = {Proceedings of the International Workshop on Semantic Big Data},
articleno = {6},
numpages = {6},
keywords = {schema evolution, semantic data management, multi-model DBMS},
location = {Amsterdam, Netherlands},
series = {SBD '19}
}

@inproceedings{10.1145/3077584.3077587,
author = {Li, Chengming and Wu, Zheng and Yin, Jie},
title = {Research on Oracle-Based Integrative Storage and Management of Spatial Data},
year = {2017},
isbn = {9781450348331},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077584.3077587},
doi = {10.1145/3077584.3077587},
abstract = {An Oracle-based object-oriented vector-raster-integrated spatial database management
system was proposed in this paper based upon the combination of mature relational
spatial database storage and spatial data engine technologies in order to solve the
problem of low transferability and efficiency that exists in the storage and management
of multi-source heterogeneous spatial data. By rearranging the data transfer flow,
establishing an integrated vector and raster data model, and optimizing the spatial
data retrieval mechanism, this system enabled united storage and efficient management
of spatial data. A comparison with ArcSDE, a piece of international leading similar
software, revealed this technology's higher data transfer performance and better query
and retrieval efficiency},
booktitle = {Proceedings of the 2017 International Conference on Information System and Data Mining},
pages = {16–22},
numpages = {7},
keywords = {vector and raster integration management, data transfer, Spatial data, retrieval optimization},
location = {Charleston, SC, USA},
series = {ICISDM '17}
}

@article{10.1145/2505403.2505411,
author = {Liang, Steve and Liu, Yong and Xu, Yan},
title = {SWE 2012 Workshop Report: The First International Workshop on Sensor Web Enablement},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
url = {https://doi.org/10.1145/2505403.2505411},
doi = {10.1145/2505403.2505411},
abstract = {The international Sensor Web Enablement (SWE) workshop series bring together sensor
web experts to present and discuss the latest ideas about the broadly defined Sensor
Web, its infrastructure, relevant algorithms and new innovative applications. The
SWE workshops also serve as a forum for Sensor Web users, developers and researchers
to debate and discuss the current state-of-the-art and to shape the future of the
Sensor Web. SWE workshops especially welcome real-world results and deployments of
the sensor web systems.},
journal = {SIGSPATIAL Special},
month = mar,
pages = {17},
numpages = {1}
}

@article{10.1145/3431843.3431850,
author = {Sarwat, Mohamed},
title = {Spatial Data Systems Support for the Internet of Things: Challenges and Opportunities},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
url = {https://doi.org/10.1145/3431843.3431850},
doi = {10.1145/3431843.3431850},
abstract = {The Internet of Things (IoT) has recently received significant attention. An IoT device
may possess an array of sensors that for example monitors the air temperature, carbon
monoxide level, wifi signals, and sound intensity. IoT data is initially created on
the device, then sent over to a central database system (e.g., the cloud) that organizes
and prepares such data for the ongoing use by myriad applications, which include but
are not limited to smart home, smart city, the industrial internet, connected cars,
and connected health. Data generated by IoT devices is inherently spatial and temporal.
For instance, an audio signal represents the variation of the sound intensity (retrieved
by a sound sensor) over the time dimension. Furthermore, IoT devices are either installed
in a static location (e.g., a building, a traffic intersection) or can be attached
to moving objects such as a connected vehicle or a wearable device. In this article,
we argue that existing IoT data systems do not properly consider the SpatioTemporal
aspect of such data. Hence, the article represents a call for action to the SIGSPATIAL
community in order to conduct research on building systems and applications that treat
both the spatial and temporal dimensions of IoT data as first class citizens.},
journal = {SIGSPATIAL Special},
month = oct,
pages = {42–47},
numpages = {6}
}

@inproceedings{10.1145/2675316.2675324,
author = {Karthik, Rajasekar},
title = {SAME4HPC: A Promising Approach in Building a Scalable and Mobile Environment for High-Performance Computing},
year = {2014},
isbn = {9781450331425},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675316.2675324},
doi = {10.1145/2675316.2675324},
abstract = {In this paper, an architecture for building Scalable And Mobile Environment For High-Performance
Computing with spatial capabilities called SAME4HPC is described using cutting-edge
technologies and standards such as Node.js, HTML5, ECMAScript 6, and PostgreSQL 9.4.
Mobile devices are increasingly becoming powerful enough to run high-performance apps.
At the same time, there exist a significant number of low-end and older devices that
rely heavily on the server or the cloud infrastructure to do the heavy lifting. Our
architecture aims to support both of these types of devices to provide high-performance
and rich user experience. A cloud infrastructure consisting of OpenStack with Ubuntu,
GeoServer, and high-performance JavaScript frameworks are some of the key open-source
and industry standard practices that has been adopted in this architecture.},
booktitle = {Proceedings of the Third ACM SIGSPATIAL International Workshop on Mobile Geographic Information Systems},
pages = {68–71},
numpages = {4},
keywords = {architecture, HTML5, mobile platforms, high-performance computing, scalability, cloud infrastructure, node.js, JavaScript},
location = {Dallas, Texas},
series = {MobiGIS '14}
}

@inproceedings{10.1145/2095536.2095618,
author = {Tao, Hung Q. and Nguyen, Yen-Vy L. and Nguyen, Hieu M. and Huynh, Viet H. and Nguyen, Tuan A.},
title = {Egobile: Where Social Networks Go Mobile},
year = {2011},
isbn = {9781450307840},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2095536.2095618},
doi = {10.1145/2095536.2095618},
abstract = {The pervasiveness of always-on smartphones and wireless networks, has provided users
chances to connect to mobile social networks everywhere. However, the diversity of
phone manufacturers and operating systems is hindering the development of location-aware
social network applications. Via introducing the Egobile application, this paper addresses
the major issues of developing mobile context-aware social networks with collaborative
filtering method for recommendations.},
booktitle = {Proceedings of the 13th International Conference on Information Integration and Web-Based Applications and Services},
pages = {408–411},
numpages = {4},
keywords = {geolocation API, context-aware social networks, social networks, context-aware computing},
location = {Ho Chi Minh City, Vietnam},
series = {iiWAS '11}
}

@inproceedings{10.1109/CCGrid.2016.19,
author = {Szuba, Marek and Ameri, Parinaz and Grabowski, Udo and Meyer, J\"{o}rg and Streit, Achim},
title = {A Distributed System for Storing and Processing Data from Earth-Observing Satellites: System Design and Performance Evaluation of the Visualisation Tool},
year = {2016},
isbn = {9781509024520},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2016.19},
doi = {10.1109/CCGrid.2016.19},
abstract = {We present a distributed system for storage, processing, three-dimensional visualisation
and basic analysis of data from Earth-observing satellites. The database and the server
have been designed for high performance and scalability, whereas the client is highly
portable thanks to having been designed as a HTML5- and WebGL-based Web application.
The system is based on the so-called MEAN stack, a modern replacement for LAMP which
has steadily been gaining traction among high-performance Web applications. We demonstrate
the performance of the system from the perspective of an user operating the client.},
booktitle = {Proceedings of the 16th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {169–174},
numpages = {6},
keywords = {MongoDB, web application, WebGL, Node.js, express, distributed processing, MEAN stack, AngularJS, geospatialdata analysis},
location = {Cartagena, Columbia},
series = {CCGRID '16}
}

@article{10.1145/3150226,
author = {Pouyanfar, Samira and Yang, Yimin and Chen, Shu-Ching and Shyu, Mei-Ling and Iyengar, S. S.},
title = {Multimedia Big Data Analytics: A Survey},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3150226},
doi = {10.1145/3150226},
abstract = {With the proliferation of online services and mobile technologies, the world has stepped
into a multimedia big data era. A vast amount of research work has been done in the
multimedia area, targeting different aspects of big data analytics, such as the capture,
storage, indexing, mining, and retrieval of multimedia big data. However, very few
research work provides a complete survey of the whole pine-line of the multimedia
big data analytics, including the management and analysis of the large amount of data,
the challenges and opportunities, and the promising research directions. To serve
this purpose, we present this survey, which conducts a comprehensive overview of the
state-of-the-art research work on multimedia big data analytics. It also aims to bridge
the gap between multimedia challenges and big data solutions by providing the current
big data frameworks, their applications in multimedia analyses, the strengths and
limitations of the existing methods, and the potential future directions in multimedia
big data analytics. To the best of our knowledge, this is the first survey that targets
the most recent multimedia management techniques for very large-scale data and also
provides the research studies and technologies advancing the multimedia analyses in
this big data era.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {10},
numpages = {34},
keywords = {Big data analytics, mobile multimedia, data mining, retrieval, 5V challenges, machine learning, multimedia databases, survey, indexing, multimedia analysis}
}

@inproceedings{10.1145/2502081.2502259,
author = {Wang, Guanfeng and Seo, Beomjoo and Yin, Yifang and Zimmermann, Roger and Shen, Zhijie},
title = {OSCOR: An Orientation Sensor Data Correction System for Mobile Generated Contents},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502259},
doi = {10.1145/2502081.2502259},
abstract = {In addition to positioning data, other sensor information -- such as orientation data,
have become a useful and powerful contextual feature. Such auxiliary information can
facilitate higher-level semantic description inferences in many multimedia applications,
e.g., video tagging and video summarization. However, sensor data collected from current
mobile devices is often not accurate enough for upstream multimedia analysis. An effective
orientation data correction system for mobile multimedia content has been an elusive
goal so far. Here we present a system, termed Oscor, which aims to improve the accuracy
of noisy orientation sensor measurements generated by mobile devices during image
and video recording. We provide a user-friendly camera interface to facilitate the
gathering of additional information, which enables the correction process on the server-side.
Geographic field-of-view (FOV) visualizations based on the original and corrected
sensor data help users understand the corrected contextual information and how the
erroneous data possibly may affect further processes.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {439–440},
numpages = {2},
keywords = {mobile multimedia, data correction, camera orientation, digital compass},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/3378393.3402275,
author = {Agarwal, Dhruv and Iyengar, Srinivasan and Swaminathan, Manohar and Sharma, Eash and Raj, Ashish and Hatwar, Aadithya},
title = {Modulo: Drive-by Sensing at City-Scale on the Cheap},
year = {2020},
isbn = {9781450371292},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378393.3402275},
doi = {10.1145/3378393.3402275},
abstract = {Ambient air pollution in urban areas is a significant health hazard, with over 4.2
million deaths annually attributed to it. A crucial step in tackling these challenge
is to measure air quality at a fine spatiotemporal granularity. A promising approach
for several smart city projects, called drive-by sensing, is to leverage vehicles
retrofitted with different sensors (pollution monitors, etc.) that can provide the
desired spatiotemporal coverage at a fraction of the cost. However, deploying a drive-by
sensing network at a city-scale to optimally select vehicles from a large fleet is
still unexplored. In this paper, we propose Modulo -- a system to bootstrap drive-by
sensing deployment by taking into consideration a variety of aspects such as spatiotemporal
coverage, budget constraints. Modulo is well-suited to satisfy unique deployment constraints
such as colocations with other sensors (needed for gas and PM sensor calibration),
etc. We compare Modulo with two baseline algorithms on real-world taxi and bus datasets.
Modulo significantly outperforms the baselines when a fleet comprises of both taxis
and fixed-route vehicles such as public transport buses. Finally, we present a real-world
case study that uses Modulo to select vehicles for an air pollution sensing application.},
booktitle = {Proceedings of the 3rd ACM SIGCAS Conference on Computing and Sustainable Societies},
pages = {187–197},
numpages = {11},
keywords = {low-cost sensing, optimal sensor deployment, drive-by sensing},
location = {Ecuador},
series = {COMPASS '20}
}

@inproceedings{10.1145/2811271.2811276,
author = {Gong, Yikai and Deng, Fengmin and Sinnott, Richard O.},
title = {Identification of (near) Real-Time Traffic Congestion in the Cities of Australia through Twitter},
year = {2015},
isbn = {9781450337861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2811271.2811276},
doi = {10.1145/2811271.2811276},
abstract = {Transport congestion is an increasing problem especially for larger cities. Typically
traffic conditions are monitored in Australia by state and/or federal authorities
using expensive electronic devices/sensors on roads or through CCTV cameras. However
there is an alternative and far cheaper way to monitor real-time traffic status on
roads: through targeted social media analytics. Social networking sites such as Twitter
are hugely popular, public and often real-time in nature. A growing number of people
post tweets about their lives and feelings every day and everywhere, often with location-based
service information included. In this paper, we present an architecture and novel
harvesting and analytics approach that exploits this information to identify near
real-time transport congestion. Specifically, we present an algorithm for targeted
harvesting of tweets solely on the road network using the definitive road network
data for Australia. We then implement spatial-temporal clustering algorithms to identify
spatio-temporal clusters of tweets on roads to identify potential traffic congestion.
We show the scalability of the solution through the use of the large-scale Cloud facilities
offered through the National eResearch Collaboration Tools and Resources (NeCTAR --
www.nectar.org.au) Research Cloud.},
booktitle = {Proceedings of the ACM First International Workshop on Understanding the City with Urban Informatics},
pages = {7–12},
numpages = {6},
keywords = {transport, data mining, social media, gis},
location = {Melbourne, Australia},
series = {UCUI '15}
}

@inproceedings{10.1145/3265007.3265015,
author = {Zhang, Bin and Zhu, Guobin and Yu, Riji and Wei, Shaoyan and Peng, Ling and Fei, Dingzhou and Yu, Xuesong and Pan, Peiwen},
title = {Research on the Innovation of Trajectory Big Data in Social Governance},
year = {2018},
isbn = {9781450365741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3265007.3265015},
doi = {10.1145/3265007.3265015},
abstract = {With the development of modern society. The unprecedented prosperity of science &amp;
technology and finance. Objects formed a huge amount of track data in its movement.
The large amount of track data contains rich spatio-temporal characteristics information,
it exposes the privacy information such as the behavior characteristics, interests
and social habits of mobile objects. Through trajectory data processing technology.
It can excavate information such as human activity pattern and behavior characteristic,
urban vehicle movement characteristic, atmospheric environment change law and so on.
The large amount of track data also reveals the privacy information, such as the behavior
characteristics, interests and social habits of mobile objects, which is rich in spatio-temporal
characteristics information. This paper begins with the significance of the study
of trajectory big data. Introducing track big data acquisition mode and social application
in various fields, In the specific application With the development of modern society.
The unprecedented prosperity of science &amp; technology and finance. Objects formed a
huge amount of track data in its movement. The large amount of track data contains
rich spatio-temporal characteristics information, it exposes the privacy information
such as the behavior characteristics, interests and social habits of mobile objects.
Through trajectory data processing technology. It can excavate information such as
human activity pattern and behavior characteristic, urban vehicle movement characteristic,
atmospheric environment change law and so on. The large amount of track data also
reveals the privacy information, such as the behavior characteristics, interests and
social habits of mobile objects, which is rich in spatio-temporal characteristics
information. This paper begins with the significance of the study of trajectory big
data. Introducing track big data acquisition mode and social application in various
fields, In the specific application, we pay more attention to the object's trajectory
privacy protection. Applying the big data of trajectory to social governance; In addition,
the application of big data in social governance is summarized and the future work
prospect is discussed. We pay more attention to the object's trajectory privacy protection.
Applying the big data of trajectory to social governance; In addition, the application
of big data in social governance is summarized and the future work prospect is discussed.},
booktitle = {Proceedings of the 6th ACM/ACIS International Conference on Applied Computing and Information Technology},
pages = {38–42},
numpages = {5},
keywords = {Trajectory Big Data, Social Governance, Privacy Protection, Social Computing},
location = {Kunming, China},
series = {ACIT 2018}
}

@inproceedings{10.1145/2460756.2460759,
author = {Elkhatib, Yehia and Blair, Gordon S. and Surajbali, Bholanathsingh},
title = {Experiences of Using a Hybrid Cloud to Construct an Environmental Virtual Observatory},
year = {2013},
isbn = {9781450320757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460756.2460759},
doi = {10.1145/2460756.2460759},
abstract = {Environmental science is often fragmented: data is collected using mismatched formats
and conventions, and models are misaligned and run in isolation. Cloud computing offers
a lot of potential in the way of resolving such issues by supporting data from different
sources and at various scales, by facilitating the integration of models to create
more sophisticated software services, and by providing a sustainable source of suitable
computational and storage resources. In this paper, we highlight some of our experiences
in building the Environmental Virtual Observatory pilot (EVOp), a tailored cloud-based
infrastructure and associated web-based tools designed to enable users from different
backgrounds to access data concerning different environmental issues. We review our
architecture design, the current deployment and prototypes. We also reflect on lessons
learned. We believe that such experiences are of benefit to other scientific communities
looking to assemble virtual observatories or similar virtual research environments.},
booktitle = {Proceedings of the 3rd International Workshop on Cloud Data and Platforms},
pages = {13–18},
numpages = {6},
keywords = {virtual research environment, open science, environmental science, e-science, cloud computing, science gateway, hybrid infrastructure, cyberinfrastructure, virtual observatory},
location = {Prague, Czech Republic},
series = {CloudDP '13}
}

@article{10.14778/3007263.3007274,
author = {Fernandes, Ricardo and Zaczkowski, Piotr and G\"{o}ttler, Bernd and Ettinoffe, Conor and Moussa, Anis},
title = {TrafficDB: HERE's High Performance Shared-Memory Data Store},
year = {2016},
issue_date = {September 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3007263.3007274},
doi = {10.14778/3007263.3007274},
abstract = {HERE's traffic-aware services enable route planning and traffic visualisation on web,
mobile and connected car applications. These services process thousands of requests
per second and require efficient ways to access the information needed to provide
a timely response to end-users. The characteristics of road traffic information and
these traffic-aware services require storage solutions with specific performance features.
A route planning application utilising traffic congestion information to calculate
the optimal route from an origin to a destination might hit a database with millions
of queries per second. However, existing storage solutions are not prepared to handle
such volumes of concurrent read operations, as well as to provide the desired vertical
scalability. This paper presents TrafficDB, a shared-memory data store, designed to
provide high rates of read operations, enabling applications to directly access the
data from memory. Our evaluation demonstrates that TrafficDB handles millions of read
operations and provides near-linear scalability on multi-core machines, where additional
processes can be spawned to increase the systems' throughput without a noticeable
impact on the latency of querying the data store. The paper concludes with a description
of how TrafficDB improved the performance of our traffic-aware services running in
production.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {1365–1376},
numpages = {12}
}

@inproceedings{10.1145/2789187.2789196,
author = {Tsou, Ming-Hsiang and Jung, Chin-Te and Allen, Chris and Yang, Jiue-An and Gawron, Jean-Mark and Spitzberg, Brian H. and Han, Su},
title = {Social Media Analytics and Research Test-Bed (SMART Dashboard)},
year = {2015},
isbn = {9781450339230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2789187.2789196},
doi = {10.1145/2789187.2789196},
abstract = {We developed a social media analytics and research testbed (SMART) dashboard for monitoring
Twitter messages and tracking the diffusion of information in different cities. SMART
dashboard is an online geo-targeted search and analytics tool, including an automatic
data processing procedure to help researchers to 1) search tweets in different cities;
2) filter noise (such as removing redundant retweets and using machine learning methods
to improve precision); 3) analyze social media data from a spatiotemporal perspective,
and 4) visualize social media data in various ways (such as weekly and monthly trends,
top URLs, top retweets, top mentions, or top hashtags). By monitoring social messages
in geo-targeted cities, we hope that SMART dashboard can assist researchers investigate
and monitor various topics, such as flu outbreaks, drug abuse, and Ebola epidemics
at the municipal level.},
booktitle = {Proceedings of the 2015 International Conference on Social Media &amp; Society},
articleno = {2},
numpages = {7},
keywords = {spatiotemporal, geo-targeted, social media analytics, disease outbreak, Twitter},
location = {Toronto, Ontario, Canada},
series = {SMSociety '15}
}

@inproceedings{10.1145/2882903.2882962,
author = {Ogden, Peter and Thomas, David and Pietzuch, Peter},
title = {AT-GIS: Highly Parallel Spatial Query Processing with Associative Transducers},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2882962},
doi = {10.1145/2882903.2882962},
abstract = {Users in many domains, including urban planning, transportation, and environmental
science want to execute analytical queries over continuously updated spatial datasets.
Current solutions for large-scale spatial query processing either rely on extensions
to RDBMS, which entails expensive loading and indexing phases when the data changes,
or distributed map/reduce frameworks, running on resource-hungry compute clusters.
Both solutions struggle with the sequential bottleneck of parsing complex, hierarchical
spatial data formats, which frequently dominates query execution time. Our goal is
to fully exploit the parallelism offered by modern multi-core CPUs for parsing and
query execution, thus providing the performance of a cluster with the resources of
a single machine. We describe AT-GIS, a highly-parallel spatial query processing system
that scales linearly to a large number of CPU cores. AT-GIS integrates the parsing
and querying of spatial data using a new computational abstraction called associative
transducers (ATs). ATs can form a single data-parallel pipeline for computation without
requiring the spatial input data to be split into logically independent blocks. Using
ATs, AT-GIS can execute, in parallel, spatial query operators on the raw input data
in multiple formats, without any pre-processing. On a single 64-core machine, AT-GIT
provides 3x the performance of an 8-node Hadoop cluster with 192 cores for containment
queries, and 10x for aggregation queries.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {1041–1054},
numpages = {14},
keywords = {multi-core CPUs, NODB, XML, JSON, spatial query processing, parallel automata},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@inproceedings{10.1145/2479787.2479831,
author = {Corchero, Aitor and Domingo, Xavier and Garc\'{\i}a, Roberto},
title = {Semantic Sensor Web Data Exploration and Visualization for Intelligent Decision Support: Position Paper},
year = {2013},
isbn = {9781450318501},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2479787.2479831},
doi = {10.1145/2479787.2479831},
abstract = {To date, Semantic Sensor Web research and development has focused on establishing
common techniques and practices that homogenize how to discover sensors, collect their
data, integrate them, extract information from them, etc. However, as these issues
are overcome and huge data bases of sensor data begin to emerge, the focus should
change to improve the data management and the information overload, discarding the
non relevant information from the relevant one, and on the other hand, allow easy
and intuitive navigation through it. The objective is to move up the wisdom hierarchy
and empower users so they can start discovering new relevant knowledge and making
decissions based on that. In this position paper, we start drafting an architecture,
aligned with current practices and standards, which facilitates the whole process:
from data collecting and storing, to wisdom generation and navigation. Efforts will
focus on empower users to spot trends or events in data. Moreover, the system will
learn from the discoveries made by users so it can later automatise the detection
of similar situations and integrate users wisdom.},
booktitle = {Proceedings of the 3rd International Conference on Web Intelligence, Mining and Semantics},
articleno = {44},
numpages = {4},
keywords = {decision support, data, exploration, semantic web, sensor, visualisation},
location = {Madrid, Spain},
series = {WIMS '13}
}

@inproceedings{10.1145/2737909.2737912,
author = {Fox, Geoffrey C. and Jha, Shantenu and Qiu, Judy and Luckow, Andre},
title = {Towards an Understanding of Facets and Exemplars of Big Data Applications},
year = {2014},
isbn = {9781450330312},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2737909.2737912},
doi = {10.1145/2737909.2737912},
abstract = {We study many Big Data applications from a variety of research and commercial areas
and suggest a set of characteristic features and possible kernel benchmarks that stress
those features for data analytics. We draw conclusions for the hardware and software
architectures that are suggested by this analysis.},
booktitle = {Proceedings of the 20 Years of Beowulf Workshop on Honor of Thomas Sterling's 65th Birthday},
pages = {7–16},
numpages = {10},
location = {Annapolis, MD, USA},
series = {Beowulf '14}
}

@inproceedings{10.1145/2996890.2996898,
author = {Sinnott, Richard O. and Thomas, Natasha and Bansal, Himanshu and Zhao, Zeyu},
title = {My Ever Changing Moods: Sentiment-Based Event Detection on the Cloud},
year = {2016},
isbn = {9781450346160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2996890.2996898},
doi = {10.1145/2996890.2996898},
abstract = {Twitter is a globally used micro-blogging platform with hundreds of millions of tweets
sent every day. Many researchers have explored Twitter analytics across a wide range
of areas such as topic modeling, sentiment analysis, event detection, as well as the
application of Twitter for a variety of domain-specific application areas, e.g. disaster
management. One area that has not been explored is how changes in sentiment can be
used to identify events. In this paper we present a scalable Cloud-based platform
for harvesting, processing, analyzing and visualizing large-scale Twitter data. We
focus especially on how changes in sentiment can be used to identify events in given
contexts. What is novel is that the events that are detected are not dependent explicitly
on the topic of any given tweet, but entirely on the change in sentiment. This offers
new capabilities for event detection that have hitherto not been explored. To illustrate
the approach, we present case studies related to sporting events identified entirely
through changing sentiment with specific focus on the 2014 FIFA World Cup of Soccer
and the 2015 World Cup of Cricket. (Abstract)},
booktitle = {Proceedings of the 9th International Conference on Utility and Cloud Computing},
pages = {175–184},
numpages = {10},
keywords = {sentiment analysis, cloud computing, social media, event detection},
location = {Shanghai, China},
series = {UCC '16}
}

@inproceedings{10.1145/3418094.3418105,
author = {Ekpenyong, Moses E. and Udoh, Samuel S. and Edoho, Mercy E. and Udo, Ifiok J. and Udo, Edward N. and Fakiyesi, Temitope J. and Oyong, Samuel B.},
title = {Hybrid Collaborative Model for Evidence-Based Healthcare Practice},
year = {2020},
isbn = {9781450377768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3418094.3418105},
doi = {10.1145/3418094.3418105},
abstract = {Incorporating evidence-based healthcare practice would improve patients' response
and safety and make patients partners in current healthcare practice. This partnership
is certain to offer patients the opportunity to guide safety initiatives through data
access by clinicians and encourage evidence-based healthcare while alleviating potential
medical errors. In this paper, we promote a collaborative model that integrates interrelated
concepts for responsive healthcare services that target patient-centred healthcare--with
healthcare providers and relevant stakeholders in the loop. The implementation strategies
for fulfilling the desired healthcare outcomes as well as design implications are
also provided. The model is expected to offer transformative impact that would drive
our weak healthcare system for improved healthcare and complement the huge dearth
in healthcare services. The outcome is shared prosperity and health, and a mainstream
of the people into healthcare decision making for informed policy planning and implementation.},
booktitle = {Proceedings of the 4th International Conference on Medical and Health Informatics},
pages = {90–97},
numpages = {8},
keywords = {patient-centred healthcare, collaborative model, evidence-based practice},
location = {Kamakura City, Japan},
series = {ICMHI 2020}
}

@inproceedings{10.1145/3342827.3342843,
author = {Shen, Zhengru and Wang, Xi and Spruit, Marco},
title = {Big Data Framework for Scalable and Efficient Biomedical Literature Mining in the Cloud},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342843},
doi = {10.1145/3342827.3342843},
abstract = {The massive size of available biomedical literature requires researchers to utilize
novel big data technologies in data storage and analysis. Among them is cloud computing
which has become the most popular solution for big data applications in industry.
However, many bioinformaticians still rely on expensive and inefficient in-house infrastructure
to discover knowledge from biomedical literature. Although some cloud-based solutions
were constructed recently, they failed to sufficiently address a few key issues including
scalability, flexibility, and reusability. Moreover, no study has taken computational
cost into consideration. To fill the gap, we proposed a cloud-based big data framework
that enables researchers to perform reproducible and scalable large-scale biomedical
literature mining in an efficient and cost-effective way. Additionally, a cloud agnostic
platform was constructed and then evaluated on two open access corpora with millions
of full-text biomedical articles. The results indicate that our framework supports
scalable and efficient large-scale biomedical literature mining.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {80–86},
numpages = {7},
keywords = {big data, document classification, cloud computing, topic modeling, biomedical literature, text mining},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3297280.3297504,
author = {Pittaras, Nikiforos and Papadakis, George and Stamoulis, George and Argyriou, Giorgos and Taniskidou, Efi Karra and Thanos, Emmanouil and Giannakopoulos, George and Tsekouras, Leonidas and Koubarakis, Manolis},
title = {GeoSensor: Semantifying Change and Event Detection over Big Data},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297504},
doi = {10.1145/3297280.3297504},
abstract = {GeoSensor is a novel, open-source system that enriches change detection over satellite
images with event detection over news items and social media content. GeoSensor combines
these two orthogonal operations through state-of-the-art Semantic Web technologies.
At its core lies the open-source, semantics-enabled Big Data infrastructure developed
by the EU H2020 BigDataEurope project. This allows GeoSensor to offer an on-line functionality,
despite facing three major challenges of Big Data: Volume (a single satellite image
typically occupies a few GBs), Variety (its data sources include two different types
of satellite images and various types of user-generated content) and Veracity, as
the accuracy of the end result is crucial for the usefulness of our system. We present
GeoSensor's architecture in detail, highlighting the advantages of using semantics
for taking the most of the knowledge extracted from news items and Earth Observation
products. We also verify GeoSensor's efficiency through a preliminary experimental
study.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {2259–2266},
numpages = {8},
keywords = {event detection, change detection, satellite data, big data, linked data},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.5555/3049877.3049888,
author = {Hemmings, Matthew and McGeer, Rick and Ricart, Glenn and Stege, Ulrike},
title = {Base64Geo: An Efficient Data Structure and Transmission Format for Large, Dense, Scalar GIS Datasets},
year = {2016},
publisher = {IBM Corp.},
address = {USA},
abstract = {We describe Base64Geo, a data structure and transmission format for large-scale, dense,
scalar GIS datasets. Base64Geo encodes a rectangular grid of scalar GIS values as
an array of strings, where each character is in the range [0 − 9A − Z a − z + /].
Each string represents the values on a specific latitude value, read west to east;
the strings themselves are arranged south to north. The resulting structure gives
a wire format for data transmission that is two orders of magnitude more efficient
than standard GIS and a compact database structure that is searched with simple string
operations. Disk dataset size is reduced by an order of magnitude over a corresponding
CSV structure, and by two orders of magnitude over an indexed GIS database. Search
times on the string-based Base64Geo dataset are an order of magnitude smaller than
search times from a quad-tree based searcher on the same dataset.},
booktitle = {Proceedings of the 26th Annual International Conference on Computer Science and Software Engineering},
pages = {106–115},
numpages = {10},
location = {Toronto, Ontario, Canada},
series = {CASCON '16}
}

@inproceedings{10.1145/3460866.3461772,
author = {C\'{e}rin, Christophe and Andres, Fr\'{e}d\'{e}ric and Geldwerth-Feniger, Danielle},
title = {Towards an Emulation Tool Based on Ontologies and Data Life Cycles for Studying Smart Buildings},
year = {2021},
isbn = {9781450384650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460866.3461772},
doi = {10.1145/3460866.3461772},
abstract = {In this paper, we share our vision to study a complex Information Technology (IT)
system handling a massive amount of data in the context of 'smart buildings.' One
technique for analyzing complex IT systems relies on emulation, where the final software
system is fully deployed on real architectures, and is evaluated in considering "small"
instances of situations the system is supposed to solve. We propose a software architecture
for studying the ecosystem of 'smart buildings'. This software architecture is built:
1) on top of ontologies for the description of smart buildings; 2) on a special tool
for mastering the life cycle of data produced by sensors and actuators inside the
buildings.We assume that it is equally important to model both the building's components
and the flow of data produced inside the building. We use existing software components
for both goals and to make real our concerns. According to a translational methodology,
we also discuss use cases for illustrating the potential of our approach and the particular
challenges associated with making the two main components of our emulation tool inter-operate.Therefore,
our main contribution is to propose a comprehensive, ambitious and realistic research
plan to guide communities. The paper illustrates how computer scientists and smart
buildings domain scientists may communicate to address and solve specific research
problems related to Big Data in emergent distributed environments. We are also guessing
that experimental results that can demonstrate the practicality of the proposed combination
of tools could be devised in the future, based on our broad vision. The paper is,
first and foremost, a visionary paper.},
booktitle = {Proceedings of the International Workshop on Big Data in Emergent Distributed Environments},
articleno = {8},
numpages = {15},
keywords = {systems and methods, big data tools, smart buildings, ontology, emulation principles, data life cycle},
location = {Virtual Event, China},
series = {BiDEDE '21}
}

@inproceedings{10.1109/CCGrid.2013.116,
author = {Cuzzocrea, Alfredo and Fortino, Giancarlo and Rana, Omer},
title = {Managing Data and Processes in Cloud-Enabled Large-Scale Sensor Networks: State-of-the-Art and Future Research Directions},
year = {2013},
isbn = {9780768549965},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2013.116},
doi = {10.1109/CCGrid.2013.116},
abstract = {In this paper we focus on state-of-the-art analysis and open research issues in the
context of Cloud-enabled large-scale sensor networks, which naturally complement the
emerging Big Data paradigm. We particularly address how data and processes are represented
and managed in such infrastructures, by highlighting benefits and limitations. We
also provide rigorous and critical discussion on actual trends and solutions available
in literature, along with future research directions in this scientific field.},
booktitle = {Proceedings of the 13th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {583–588},
numpages = {6},
keywords = {big data, cloud-enabled large-scale sensor networks, cloud computing},
location = {Delft, Netherlands},
series = {CCGRID '13}
}

@article{10.1145/3210752,
author = {Orenga-Rogl\'{a}, Sergio and Chalmeta, Ricardo},
title = {Framework for Implementing a Big Data Ecosystem in Organizations},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {62},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/3210752},
doi = {10.1145/3210752},
abstract = {Featuring the various dimensions of data management, it guides organizations through
implementation fundamentals.},
journal = {Commun. ACM},
month = dec,
pages = {58–65},
numpages = {8}
}

@inproceedings{10.1145/2484762.2484821,
author = {Padmanabhan, Anand and Wang, Shaowen and Cao, Guofeng and Hwang, Myunghwa and Zhao, Yanli and Zhang, Zhenhua and Gao, Yizhao},
title = {FluMapper: An Interactive CyberGIS Environment for Massive Location-Based Social Media Data Analysis},
year = {2013},
isbn = {9781450321709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2484762.2484821},
doi = {10.1145/2484762.2484821},
abstract = {Social media, such as social network (e.g., Facebook), microblogs (e.g. Twitter) have
experienced a spectacular rise in popularity, and attracting hundreds of millions
of users generating unprecedented amount of information. Twitter, for example, has
rapidly gained approximately 500 million registered users as of 2012, generating 340
million tweets daily. Although each tweet is limited to only 140 characters, the aggregate
of millions of tweets may provide a realistic representation of landscapes for a certain
topic of interest. Furthermore, with widespread use of location aware mobile devices,
users are sharing their whereabouts through social media services. This has resulted
in a dramatic increase in volume of spatial data and they are becoming a crucial attribute
of social media. These location-based social media thus could provide valuable insights
to understanding many geographic phenomena. Recent studies capitalizing on social
networking and media data show significant societal impacts, in many areas including
infectious disease tracking [1].},
booktitle = {Proceedings of the Conference on Extreme Science and Engineering Discovery Environment: Gateway to Discovery},
articleno = {33},
numpages = {2},
keywords = {flow mapping, CyberGIS, exploratory spatial data analysis, FluMapper, kernel density estimation},
location = {San Diego, California, USA},
series = {XSEDE '13}
}

@inproceedings{10.1145/3030024.3040984,
author = {Sheidin, Julia and Lanir, Joel and Kuflik, Tsvi and Bak, Peter},
title = {Visualizing Spatial-Temporal Evaluation of News Stories},
year = {2017},
isbn = {9781450348935},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3030024.3040984},
doi = {10.1145/3030024.3040984},
abstract = {News today are generated and distributed online by a multitude of sources all over
the world. Easy and efficient monitoring and analysis of news stories is of interest
to both professional analysts and the general public. One interesting aspect is the
magnitude and impact of a story as well as its evolution over time. In this work we
introduce an idea and a system that presents temporal and spatial evolution of news
world-wide, in two different levels, to help users quickly understand and act upon
the large amount of data. An overview option shows a general split of the reported
news, and a more detailed view provides interactive options for deeper analysis of
a single news episode. We demonstrate our system on data from news events generated
by the Europe Media Monitor (EMM), an online news aggregator platform.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces Companion},
pages = {65–68},
numpages = {4},
keywords = {news visualization, spatio-temporal visualization, news},
location = {Limassol, Cyprus},
series = {IUI '17 Companion}
}

@inproceedings{10.1145/2814864.2814869,
author = {Quoc, Hoan Nguyen Mau and Le Phuoc, Danh},
title = {An Elastic and Scalable Spatiotemporal Query Processing for Linked Sensor Data},
year = {2015},
isbn = {9781450334624},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814864.2814869},
doi = {10.1145/2814864.2814869},
abstract = {Recently, many approaches have been proposed to manage sensor data using Semantic
Web technologies for effective heterogeneous data integration. However, our research
survey revealed that these solutions primarily focused on semantic relationships and
still paid less attention to its temporal-spatial correlation. Most semantic approaches
do not have spatiotemporal support. Some of them have served limitations on providing
full spatiotemporal support but have poor performance for complex spatiotemporal aggregate
queries. In addition, while the volume of sensor data is rapidly growing, a challenge
of querying and managing the massive volumes of data generated by sensing devices
still remains unsolved. In this paper, we propose a spatiotemporal query engine for
sensor data based on Linked Data model. The ultimate goal of our approach is to provide
an elastic and scalable system which allows fast searching and analysis on the relationships
of space, time and semantic in sensor data. We also introduce a set of new query operators
in order to support spatiotemporal computing in linked sensor data context.},
booktitle = {Proceedings of the 11th International Conference on Semantic Systems},
pages = {17–24},
numpages = {8},
keywords = {internet of things, linked stream data, real-time search engine, graph of things},
location = {Vienna, Austria},
series = {SEMANTICS '15}
}

@article{10.1145/2108144.2108163,
author = {Doernhoefer, Mark},
title = {Surfing the Net for Software Engineering Notes},
year = {2012},
issue_date = {March 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/2108144.2108163},
doi = {10.1145/2108144.2108163},
journal = {SIGSOFT Softw. Eng. Notes},
month = apr,
pages = {11–20},
numpages = {10}
}

@inproceedings{10.1145/3299869.3314039,
author = {Chrysafis, Christos and Collins, Ben and Dugas, Scott and Dunkelberger, Jay and Ehsan, Moussa and Gray, Scott and Grieser, Alec and Herrnstadt, Ori and Lev-Ari, Kfir and Lin, Tao and McMahon, Mike and Schiefer, Nicholas and Shraer, Alexander},
title = {FoundationDB Record Layer: A Multi-Tenant Structured Datastore},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299869.3314039},
doi = {10.1145/3299869.3314039},
abstract = {The FoundationDB Record Layer is an open source library that provides a record-oriented
data store with semantics similar to a relational database implemented on top of FoundationDB,
an ordered, transactional key-value store. The Record Layer provides a lightweight,
highly extensible way to store structured data. It offers schema management and a
rich set of query and indexing facilities, some of which are not usually found in
traditional relational databases, such as nested record types, indexes on commit versions,
and indexes that span multiple record types. The Record Layer is stateless and built
for massive multi-tenancy, encapsulating and isolating all of a tenant's state, including
indexes, into a separate logical database. We demonstrate how the Record Layer is
used by CloudKit, Apple's cloud backend service, to provide powerful abstractions
to applications serving hundreds of millions of users. CloudKit uses the Record Layer
to host billions of independent databases, many with a common schema. Features provided
by the Record Layer enable CloudKit to provide richer APIs and stronger semantics
with reduced maintenance overhead and improved scalability.},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {1787–1802},
numpages = {16},
keywords = {stateless architecture, distributed databases, multi-tenancy, cloudkit, foundationdb, mobile backend},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}

@inproceedings{10.1145/3356471.3365233,
author = {Yin, Zhengcong and Xiong, Haoyi and Zhou, Xun and Goldberg, Daniel W. and Bennett, Dave and Zhang, Chong},
title = {A Deep Learning Based Illegal Parking Detection Platform},
year = {2019},
isbn = {9781450369572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356471.3365233},
doi = {10.1145/3356471.3365233},
abstract = {Illegal parking is a critical problem in large, growing cities. Currently, the responsibility
for detecting illegally parked vehicles has been left to law enforcement, which often
requires manual inspection. To improve the efficiency of law enforcement for vehicle
parking management, we propose a web-based analytic platform that leverages recent
advancements in computer vision. This proposed platform provides an algorithm to improve
the performance of detecting vehicle license plates from videos, based on an existing
deep learning approach. Also, we provide a method to estimate vehicle parking locations.
This platform is applicable for videos of security patrolling. End-users can define
restricted zones via a map-based interface and all vehicles located in these areas
can be efficiently identified once patrolling videos are received. This system is
evaluated by two videos captured in real-world parking lots. The results indicate
that the proposed platform can successfully identify vehicle plate numbers and estimate
their parking locations to support the management of urban parking infrastructure.},
booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
pages = {32–35},
numpages = {4},
keywords = {Illegal Parking, GeoAI, Location Estimation, Plate Number Extraction},
location = {Chicago, IL, USA},
series = {GeoAI 2019}
}

@inproceedings{10.1145/3314545.3314566,
author = {Mbah, Raymond Blanch K. and Rege, Manjeet and Misra, Bhabani},
title = {Using Spark and Scala for Discovering Latent Trends in Job Markets},
year = {2019},
isbn = {9781450366342},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314545.3314566},
doi = {10.1145/3314545.3314566},
abstract = {Job markets are experiencing an exponential growth in data alongside the recent explosion
of big data in various domains including health, security and finance. Staying current
with job market trends entails collecting, processing and analyzing huge amounts of
data. A typical challenge with analyzing job listings is that they vary drastically
with regards to verbiage, for instance a given job title or skill can be referred
to using different words or industry jargons. As a result, it becomes incumbent to
go beyond words present in job listings and carry out analysis aimed at discovering
latent structures and trends in job listings. In this paper, we present a systematic
approach of uncovering latent trends in job markets using big data technologies (Apache
Spark and Scala) and distributed semantic techniques such as latent semantic analysis
(LSA). We show how LSA can uncover patterns/relationships/trends that will otherwise
remain hidden if using traditional text mining techniques that rely only on word frequencies
in documents.},
booktitle = {Proceedings of the 2019 3rd International Conference on Compute and Data Analysis},
pages = {55–62},
numpages = {8},
keywords = {Natural Language Processing(NLP), Spark, Big Data, Scala, Singular Value Decomposition (SVD), Latent Semantic Analysis(LSA)},
location = {Kahului, HI, USA},
series = {ICCDA 2019}
}

@inproceedings{10.1145/2998181.2998271,
author = {Alcaidinho, Joelle and Freil, Larry and Kelly, Taylor and Marland, Kayla and Wu, Chunhui and Wittenbrook, Bradley and Valentin, Giancarlo and Jackson, Melody},
title = {Mobile Collaboration for Human and Canine Police Explosive Detection Teams},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998181.2998271},
doi = {10.1145/2998181.2998271},
abstract = {We designed a communication system for law enforcement officers to use when conducting
explosive detection searches with multiple agencies. Dogs trained in explosive detection
work alongside human handlers to form a K9 team, which are an integral part of these
searches. Officers in K9 teams have a strong bond and communication with these dogs,
but noisy locations, long distances, and crowded spaces present challenges. In addition,
other officers assigned as backup often lack the experience to read the cues from
the canine, which hinders the speed and effectiveness of the team. Coordinating a
search with teams from different municipalities presents challenges due to a lack
of standard collaboration tools. Getting the right information as quickly as possible
saves lives, whether this information is about the areas that have been searched or
the location of an explosive device. We hope that in addition to increasing public
safety, our system will make working conditions safer for law enforcement officers
and their canines.},
booktitle = {Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
pages = {925–933},
numpages = {9},
keywords = {police, explosive detection, animal computer-interaction, collaborative maps, law enforcement, k9},
location = {Portland, Oregon, USA},
series = {CSCW '17}
}

@article{10.1145/3239566,
author = {Moustaka, Vaia and Vakali, Athena and Anthopoulos, Leonidas G.},
title = {A Systematic Review for Smart City Data Analytics},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3239566},
doi = {10.1145/3239566},
abstract = {Smart cities (SCs) are becoming highly sophisticated ecosystems at which innovative
solutions and smart services are being deployed. These ecosystems consider SCs as
data production and sharing engines, setting new challenges for building effective
SC architectures and novel services. The aim of this article is to “connect the pieces”
among Data Science and SC domains, with a systematic literature review which identifies
the core topics, services, and methods applied in SC data monitoring. The survey focuses
on data harvesting and data mining processes over repeated SC data cycles. A survey
protocol is followed to reach both quantitative and semantically important entities.
The review results generate useful taxonomies for data scientists in the SC context,
which offers clear guidelines for corresponding future works. In particular, a taxonomy
is proposed for each of the main SC data entities, namely, the “D Taxonomy” for the
data production, the “M Taxonomy” for data analytics methods, and the “S Taxonomy”
for smart services. Each of these taxonomies clearly places entities in a classification
which is beneficial for multiple stakeholders and for multiple domains in urban smartness
targeting. Such indicative scenarios are outlined and conclusions are quite promising
for systemizing.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {103},
numpages = {41},
keywords = {crowd-sensing, Internet of Things, data harvesting, Data mining, taxonomy, systematic review, smart cities, smart services, crowd-sourcing, open data, smart dimensions}
}

@inproceedings{10.1145/2786006.2786007,
author = {Alsubaiee, Sattam and Carey, Michael J. and Li, Chen},
title = {LSM-Based Storage and Indexing: An Old Idea with Timely Benefits},
year = {2015},
isbn = {9781450336680},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786006.2786007},
doi = {10.1145/2786006.2786007},
abstract = {With the social-media data explosion, near real-time queries, particularly those of
a spatio-temporal nature, can be challenging. In this paper, we show how to efficiently
answer queries that target recent data within very large data sets. We describe a
solution that exploits a natural partitioning property that LSM-based indexes have
for components, allowing us to filter out many components when answering queries.
Our solution is generalizable to any LSM-based index structure, and can be applied
not just on temporal fields (e.g., based on recency), but on any "time-correlated
fields" such as Universally Unique Identifiers (UUIDs), user-provided integer ids,
etc. We have implemented and experimentally evaluated the solution in the context
of the AsterixDB system.},
booktitle = {Second International ACM Workshop on Managing and Mining Enriched Geo-Spatial Data},
pages = {1–6},
numpages = {6},
location = {Melbourne, VIC, Australia},
series = {GeoRich'15}
}

@inproceedings{10.1145/3323503.3349542,
author = {de Castro Perdomo, Diogo and Viterbo, Jos\'{e} and Saade, D\'{e}bora Christina Muchaluat},
title = {A Location-Based Architecture for Video Stream Selection in the Context of IoMT},
year = {2019},
isbn = {9781450367639},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323503.3349542},
doi = {10.1145/3323503.3349542},
abstract = {Location-Based Services (LBS) are part of our day-to-day life, assisting in a variety
of tasks such as tracing better routes to a destination, traffic verification and
even for safety applications such as car or people tracking. However, it may not possible
visually to identify some activities like attempted theft or kidnapping in the same
way that a surveillance camera with visual information. It was identified that the
integration of these two information (Global Position and Video) could improve surveillance
systems from a security perspective. IP cameras are common examples of multimedia
sensors that have become an increasingly popular and ubiquitously installed item in
urban centers applied to security. This paper proposes a location-based architecture
for video stream selection that can be used to monitor entities (people or not) in
a context of Internet of Multimedia Things (IoMT) through a representational model
of coverage areas of IP cameras. A prototype was implemented as a proof of concept
of the proposed architecture. This prototype includes a low latency video transmission
service to quickly start a video transmission only when an entity is inside a visual
field of view. Tests were performed on the Internet to check for video delivery latency,
bandwidth consumption, and the results suggest the effectiveness of the proposal.},
booktitle = {Proceedings of the 25th Brazillian Symposium on Multimedia and the Web},
pages = {461–468},
numpages = {8},
keywords = {IoMT, GPS, IP cameras, tracking, coverage area, LBS},
location = {Rio de Janeiro, Brazil},
series = {WebMedia '19}
}

@inbook{10.1145/3408877.3432457,
author = {Fekete, Alan and Kay, Judy and R\"{o}hm, Uwe},
title = {A Data-Centric Computing Curriculum for a Data Science Major},
year = {2021},
isbn = {9781450380621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408877.3432457},
abstract = {Many universities are introducing a new major in Data Science into their offering,
to reflect the explosive growth in this field and the career opportunities it provides.
As a field Data Science has elements from Computer Science and from Statistics, and
curricula plans differ widely, both in the balance between the CS and Stats aspects,
and also in the emphasis within the computing topics. This paper reports on the curriculum
that has been taught for three years now at the University of Sydney. In particular,
we describe the approach of a sequence of computing subjects which were developed
specifically for the major, in order to bring students over several years to a sophisticated
understanding of the data-handling aspects of Data Science. Students also take traditional
subjects from both CS (such as Data Structures or AI) and from Statistics (such as
Learning from Data and Statistical Inference). The data-centric specially-designed
subjects we discuss in this paper are (i) Informatics: Data and Computation (in the
first year), (ii) Big Data and Data Diversity (in the second year), and then upper-division
subjects on (iii) Data Science Platforms, and (iv) Human-in-the-Loop Data Analytics.},
booktitle = {Proceedings of the 52nd ACM Technical Symposium on Computer Science Education},
pages = {865–871},
numpages = {7}
}

@inproceedings{10.1145/3139958.3140011,
author = {Kim, Kyoung-Sook and Kim, Dongmin and Jeong, Hyemi and Ogawa, Hirotaka},
title = {Stinuum: A Holistic Visual Analysis of Moving Objects with Open Source Software},
year = {2017},
isbn = {9781450354905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139958.3140011},
doi = {10.1145/3139958.3140011},
abstract = {With the development of position tracking technologies and the increasing usage of
mobile devices, the analysis of moving objects, such as pedestrians, vehicles, drones,
and hurricanes has become an important topic in various applications including intelligent
transportation, disaster management, and urban planning. Many of existing studies
have focused on managing and analyzing only time-varying locations of point-based
objects. However, real-world moving phenomena are space-time continua occupying volumes,
having an area at a time; even more, they contain dynamic attributes depending on
time and space, such as the velocity of vehicles or the average of wind speed of hurricanes.
In this demonstration, we introduce a comprehensive data format to represent various
types of temporal geometries and dynamic properties of moving objects based on OGC®
Moving Features. Moreover, we present a visual extension of Cesium to visualize moving
objects in a space-time cube by cooperating with a data server that manages moving
objects in a Cassandra database via RESTful APIs. This demonstration presents how
to analyze a correlation between typhoon trajectories and geo-tagged Twitter messages
with our systems.},
booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
articleno = {85},
numpages = {4},
keywords = {JSON, Space-time Cube, OGC® Moving Features, REST, Cassandra, Cesium, Geovisualization},
location = {Redondo Beach, CA, USA},
series = {SIGSPATIAL '17}
}

@inproceedings{10.1145/3184558.3191560,
author = {Boella, Guido and Francis, Louise and Grassi, Elena and Kistner, Axel and Nitsche, Andreas and Noskov, Alexey and Sanasi, Luigi and Savoca, Adriano and Schifanella, Claudio and Tsampoulatidis, Ioannis},
title = {WeGovNow: A Map Based Platform to Engage the Local Civic Society},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3191560},
doi = {10.1145/3184558.3191560},
abstract = {In this paper we describe the advancement of WeGovNow, an Horizon 2020 European Union
project involving twelve partners from Germany, Sweden, Greece, Italy and United Kingdom,
aimed at using state-of-the-art digital technologies in community engagement platforms
to involve citizens in decision making processes within their local neighbourhood.
Different software components, both previously existing and developed specially for
the project and covering separate aspects of community engagement, were integrated
in a single web platform offering an homogeneous experience to the users. One of the
main common threads beyond this integration process is the ability to collect crowd
mapped information and show them back to the users in an engaging way on maps, harmonizing
data coming from the different components and making the mapped space easily explorable.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {1215–1219},
numpages = {5},
keywords = {geographic visualization, collaborative content creation},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/2063348.2063364,
author = {Wang, Daniel L. and Monkewitz, Serge M. and Lim, Kian-Tat and Becla, Jacek},
title = {Qserv: A Distributed Shared-Nothing Database for the LSST Catalog},
year = {2011},
isbn = {9781450311397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2063348.2063364},
doi = {10.1145/2063348.2063364},
abstract = {The LSST project will provide public access to a database catalog that, in its final
year, is estimated to include 26 billion stars and galaxies in dozens of trillion
detections in multiple petabytes. Because we are not aware of an existing open-source
database implementation that has been demonstrated to efficiently satisfy astronomers'
spatial self-joining and cross-matching queries at this scale, we have implemented
Qserv, a distributed shared-nothing SQL database query system. To speed development,
Qserv relies on two successful open-source software packages: the MySQL RDBMS and
the Xrootd distributed file system. We describe Qserv's design, architecture, and
ability to scale to LSST's data requirements. We illustrate its potential with test
results on a 150-node cluster using 55 billion rows and 30 terabytes of simulated
data. These results demonstrate the soundness of Qserv's approach and the scale it
achieves on today's hardware.},
booktitle = {State of the Practice Reports},
articleno = {12},
numpages = {11},
keywords = {distributed, shared-nothing, file system, database, MPP, parallel},
location = {Seattle, Washington},
series = {SC '11}
}

@article{10.1145/2662112,
author = {Grozev, Nikolay and Buyya, Rajkumar},
title = {Multi-Cloud Provisioning and Load Distribution for Three-Tier Applications},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1556-4665},
url = {https://doi.org/10.1145/2662112},
doi = {10.1145/2662112},
abstract = {Cloud data centers are becoming the preferred deployment environment for a wide range
of business applications because they provide many benefits compared to private in-house
infrastructure. However, the traditional approach of using a single cloud has several
limitations in terms of availability, avoiding vendor lock-in, and providing legislation-compliant
services with suitable Quality of Experience (QoE) to users worldwide. One way for
cloud clients to mitigate these issues is to use multiple clouds (i.e., a Multi-Cloud).
In this article, we introduce an approach for deploying three-tier applications across
multiple clouds in order to satisfy their key nonfunctional requirements. We propose
adaptive, dynamic, and reactive resource provisioning and load distribution algorithms
that heuristically optimize overall cost and response delays without violating essential
legislative and regulatory requirements. Our simulation with realistic workload, network,
and cloud characteristics shows that our method improves the state of the art in terms
of availability, regulatory compliance, and QoE with acceptable sacrifice in cost
and latency.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = oct,
articleno = {13},
numpages = {21},
keywords = {three-tier applications, autoscaling, Cloud computing, load balancing, Multi-Cloud}
}

@inproceedings{10.1145/2833165.2833176,
author = {Salmon, Loic and Ray, Cyril and Claramunt, Christophe},
title = {A Hybrid Approach Combining Real-Time and Archived Data for Mobility Analysis},
year = {2015},
isbn = {9781450339711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2833165.2833176},
doi = {10.1145/2833165.2833176},
abstract = {Mobility analysis is involved in many areas such as urban planning, traffic monitoring,
climatology, study of social and animal phenomena to mention a few examples. The emergence
and proliferation of mobile and sensor-based systems generate a significant increase
of spatial and temporal data in terms of volume and frequency of update. In particular,
the storage, management and analysis of the large data sets generated become a non
straightforward task. Current works related to the manipulation of mobility data have
been directed towards either mining archived historical data or continuous processing
of incoming data streams. Our research introduces a hybrid approach whose objective
is to provide a combined processing of real-time data streams and archived data. The
principles of our approach is to promote the distributed and parallelized processing
of mobility data. The whole framework is currently applied to the real-time monitoring
of maritime traffic.},
booktitle = {Proceedings of the 6th ACM SIGSPATIAL International Workshop on GeoStreaming},
pages = {43–48},
numpages = {6},
keywords = {Geostreaming, Maritime monitoring, Moving object Database},
location = {Bellevue, WA, USA},
series = {IWGS '15}
}

@inproceedings{10.1145/2512276.2512288,
author = {Zilora, Stephen J. and Bogaard, Daniel S. and Leone, Jim},
title = {The Changing Face of Information Technology},
year = {2013},
isbn = {9781450322393},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2512276.2512288},
doi = {10.1145/2512276.2512288},
abstract = {Information technology as an academic discipline began in the early 90's. Since then,
there have been many changes in how industry views the discipline. Today, information
technology is about large-scale operations. This may be manifested as supporting enterprise
services, working with big data, or supporting massive multi-user systems. In this
paper, we describe a new curriculum that is based upon the original work in the "2008
Curriculum Guidelines for Undergraduate Degree Programs in Information Technology"
document, but addresses modern information technology demands. We discuss a new curricular
model for teaching information technology and also the addition of analytics as an
overarching theme for the curriculum.},
booktitle = {Proceedings of the 14th Annual ACM SIGITE Conference on Information Technology Education},
pages = {29–34},
numpages = {6},
keywords = {programming, information technology, analytics, human computer interation, web technologies, networking, database, curriculum},
location = {Orlando, Florida, USA},
series = {SIGITE '13}
}

@inproceedings{10.1145/3428658.3430973,
author = {Rocha, Bartira Dantas and Silva, Larysse and Batista, Thais and Cavalcante, Everton and Gomes, Porf\'{\i}rio},
title = {An Ontology-Based Information Model for Multi-Domain Semantic Modeling and Analysis of Smart City Data},
year = {2020},
isbn = {9781450381963},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428658.3430973},
doi = {10.1145/3428658.3430973},
abstract = {Smart city services are typically defined according to domains (e.g., health, education,
safety) and supported by different systems. Consequently, the analysis of smart city
data is often domain-specific, thus limiting the capabilities of the offered services
and hampering decision-making that relies on isolated domain information. To support
a suitable analysis across multiple domains, it is necessary having a unified data
model able to handle the inherent heterogeneity of smart city data and take into account
both geographic and citizen information. This paper presents an ontology-based information
model to support multi-domain analysis in smart cities to foster interoperability
and powerful automated reasoning upon unambiguous information. The proposed information
model follows Linked Data principles and takes advantage of ontologies to define information
semantically. The semantic relationships and properties defined in the model also
allow inferring new pieces of information that improve accuracy when analyzing multiple
city domains. This paper reports an evaluation of the information model through ontological
metrics and competence questions.},
booktitle = {Proceedings of the Brazilian Symposium on Multimedia and the Web},
pages = {73–80},
numpages = {8},
keywords = {Semantic search, Ontologies, Linked Data, Information model, Inference, Smart cities},
location = {S\~{a}o Lu\'{\i}s, Brazil},
series = {WebMedia '20}
}

@inbook{10.1145/3448016.3457551,
author = {Cubukcu, Umur and Erdogan, Ozgun and Pathak, Sumedh and Sannakkayala, Sudhakar and Slot, Marco},
title = {Citus: Distributed PostgreSQL for Data-Intensive Applications},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457551},
abstract = {Citus is an open source distributed database engine for PostgreSQL that is implemented
as an extension. Citus gives users the ability to distribute data, queries, and transactions
in PostgreSQL across a cluster of PostgreSQL servers to handle the needs of data-intensive
applications. The development of Citus has largely been driven by conversations with
companies looking to scale PostgreSQL beyond a single server and their workload requirements.
This paper describes the requirements of four common workload patterns and how Citus
addresses those requirements. It also shares benchmark results demonstrating the performance
and scalability of Citus in each of the workload patterns and describes how Microsoft
uses Citus to address one of its most challenging data problems.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {2490–2502},
numpages = {13}
}

@inproceedings{10.1145/3354153.3354156,
author = {Suwansrikham, Parinya and She, Kun},
title = {Protection of Big Data Privacy on Multiple Cloud Providers by Asymmetric Security Scheme},
year = {2019},
isbn = {9781450372169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3354153.3354156},
doi = {10.1145/3354153.3354156},
abstract = {Big data is the name that defines data which has enormous size and unstructured. Due
to the file size is pretty huge. It is impracticable to store a large file in one
storage volume. However, cloud computing is a solution to this impossible. Data owner
can store the file in a cloud storage provider (CSP). Nevertheless, the new dilemma
has arisen. Relying on single cloud storage may generate trouble for the customer.
A CSP may stop its service anytime. Moreover, the CSP is the third party that user
have to trust without verification. In that case, the privacy or unauthorized accessing
of data may be violated without notice. To overcome this risk, we propose secure data
storage scheme for big data storing on multiple CSPs. The one big data file is split
into chunks and distributed to multiple cloud storage provider. After splitting the
file, metadata is generated. Metadata is a place to keep chunks information, includes;
chunk locations, access paths, username and password of the data owner, methods to
connect each CSP. The metadata is encrypted and transferred to the user who requests
to access the file. The user utilizes the metadata and chunks of the file to compose
the original file. This method will minimize the risk of privacy. The goal of this
paper is to provide the method to protect the privacy of data stored on multiple cloud
storage providers. Furthermore, we discuss and analyze how this data storage scheme
promote the protection of big data privacy.},
booktitle = {Proceedings of the 2019 2nd International Conference on Data Storage and Data Engineering},
pages = {47–53},
numpages = {7},
keywords = {Data Security and Privacy, Cloud Storage, Big Data},
location = {Jeju, Republic of Korea},
series = {DSDE 2019}
}

@inproceedings{10.1145/2487766.2487769,
author = {Scheidgen, Markus},
title = {Reference Representation Techniques for Large Models},
year = {2013},
isbn = {9781450321655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487766.2487769},
doi = {10.1145/2487766.2487769},
abstract = {If models consist of more and more objects, time and space required to process these
models becomes an issue. To solve this we can employ different existing frameworks
that use different model representations (e.g. trees in XMI or relational data with
CDO). Based on the observation that these frameworks reach different performance measures
for different operations and different model characteristics, we rise the question
if and how different model representations can be combined to mitigate performance
issues of individual representations.In this paper, we analyze different techniques
to represent references, which are one important aspect to process large models efficiently.
We present the persistence framework EMF-Fragments, which combines the representation
of references as source-object contained sets of target-objects (e.g. in XMI) within
the representation as relations similar to those in relational databases (e.g. with
CDO). We also present a performance evaluation for both representations and discuss
the use of both representations in three applications: models for source-code repositories,
scientific data, and geo-spatial data.},
booktitle = {Proceedings of the Workshop on Scalability in Model Driven Engineering},
articleno = {5},
numpages = {9},
keywords = {meta-modeling, EMF, model persistence, mining software repositories, big data},
location = {Budapest, Hungary},
series = {BigMDE '13}
}

@inproceedings{10.1145/3330204.3330246,
author = {Ribeiro, Elivaldo Lozer Fracalossi and Monteiro, Erasmo Leite and Claro, Daniela Barreiro and Maciel, Rita Suzana Pitangueira},
title = {A Conceptual Framework for Pragmatic Interoperability},
year = {2019},
isbn = {9781450372374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330204.3330246},
doi = {10.1145/3330204.3330246},
abstract = {Interoperability is the ability of heterogeneous systems to communicate with another
system transparently. Usually, interoperability is classified into syntactic, semantic,
and pragmatic. The syntactic level is related to the grammar and vocabulary of the
message swapped, the semantic level with the meaning of the data and the pragmatic
level with the understanding of the messages sent and received. A set of systems is
pragmatically interoperable when they share the same expectations about the effect
of messages exchanged between them. Due to the vast diversity of definitions and no
consensus, provide a pragmatic interoperability solution is a challenge. In this paper,
we propose a conceptual framework that aims to contribute to the unification of the
concept of pragmatic interoperability and common elements necessary for its realization.
For this, a unified definition and conceptual framework are presented. The framework
was applied in three different scenarios to demonstrate its applicability and, consequently,
validation of the unified concept.},
booktitle = {Proceedings of the XV Brazilian Symposium on Information Systems},
articleno = {36},
numpages = {8},
keywords = {Intention, Context, Canonical Model},
location = {Aracaju, Brazil},
series = {SBSI'19}
}

@inproceedings{10.1145/2659532.2659594,
author = {Jaakkola, Hannu and M\"{a}kinen, Timo and Etel\"{a}aho, Anna},
title = {Open Data: Opportunities and Challenges},
year = {2014},
isbn = {9781450327534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2659532.2659594},
doi = {10.1145/2659532.2659594},
abstract = {Open data is seen as a promising source of new business, especially in the SME sector,
in the form of new products, services and innovative solutions. High importance is
seen also in fostering citizens' participation in political and social life and increasing
the transparency of public authorities. The forerunners of the open data movement
in the public sector are the USA and the UK, which started to open their public data
resources in 2009. The first European Union open data related directive was drawn
up as early as 2003; however progress in putting the idea into practice has been slow
and adoptions by the wider member states are placed in the early 2010s. The beneficial
use of open data in real applications has progressed hand in hand with the improvement
of other ICT-related technologies. The (raw) data itself has no high value. The economic
value comes from a balanced combination of high quality open (data) resources combined
with the related value chain. This paper builds up a "big picture" of the role of
open data in current society. The approach is analytical and it clarifies the topic
from the viewpoints of both opportunities and challenges. The paper covers both general
aspects related to open data and results of the research and regional development
project conducted by the authors.},
booktitle = {Proceedings of the 15th International Conference on Computer Systems and Technologies},
pages = {25–39},
numpages = {15},
keywords = {networking, open data, data analysis, big data, public data},
location = {Ruse, Bulgaria},
series = {CompSysTech '14}
}

@inproceedings{10.1145/2534190.2534193,
author = {Loyola, Luis and Wong, Fernando and Pereira, Daniel and Sanson, Horacio},
title = {Extending Battery Lifetime of Mobile Devices with Geofence Capabilities on Dynamic-Speed Urban Environments},
year = {2013},
isbn = {9781450325318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2534190.2534193},
doi = {10.1145/2534190.2534193},
abstract = {In this work we present a battery-saving algorithm for Location Based Services (LBS)
that exploits the geofence functionalities provided by modern mobile operating systems
such as iOS and Android. The algorithm detects the surrounding areas of interest (AoI)
by taking advantage of the underlying structure of quadtrees, considerably saving
the number of requests to the LBS server made by the application, thus extending its
battery lifetime even in dynamic-speed environments. The areas of interest can have
any arbitrary shape and are not constrained to circles as in previous work. In our
experiments, through empirical and simulation tests, we show that a substantial reduction
of battery consumption can be achieved (up to 45%) while keeping a perfect detection
accuracy of areas of interest in comparison with periodic polling techniques widely
used in current mobile applications, with error rates of up to 55%.},
booktitle = {Proceedings of the Second ACM SIGSPATIAL International Workshop on Mobile Geographic Information Systems},
pages = {51–58},
numpages = {8},
keywords = {open-data, walk model, battery saving, location based services, quad-trees, geofence},
location = {Orlando, Florida},
series = {MobiGIS '13}
}

@inproceedings{10.1145/3055624.3075950,
author = {Alatalo, Toni and Pouke, Matti and Koskela, Timo and Hurskainen, Tomi and Florea, Ciprian and Ojala, Timo},
title = {Two Real-World Case Studies on 3D Web Applications for Participatory Urban Planning},
year = {2017},
isbn = {9781450349550},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3055624.3075950},
doi = {10.1145/3055624.3075950},
abstract = {3D Web is a potential platform for publishing and distributing 3D visualizations that
have proven useful in enabling the participation of the general public in urban planning.
However, technical requirements imposed by detailed and rich real-world plans and
related functionalities are demanding for 3D web technologies. In this paper we explore
the maturity of modern 3D web technologies in participatory urban planning through
two real-world case studies. Applications built on Unity-based platform are published
on the web to allow the general public to create, browse and comment on urban plans.
The virtual models of seven urban development sites of different visual styles are
optimized in terms of download sizes and memory use to be feasible on browsers used
by the general public. We report qualitative feedback from users and present a technical
analysis of the applications in terms of download sizes, runtime performance and memory
use. We summarize the findings of the case studies into an assessment of the general
feasibility of modern 3D web technologies in web-based urban planning.},
booktitle = {Proceedings of the 22nd International Conference on 3D Web Technology},
articleno = {11},
numpages = {9},
keywords = {virtual city model, case study, performance, unity, emscripten},
location = {Brisbane, Queensland, Australia},
series = {Web3D '17}
}

@inbook{10.1145/2858036.2858371,
author = {Kogan, Marina and Anderson, Jennings and Palen, Leysia and Anderson, Kenneth M. and Soden, Robert},
title = {Finding the Way to OSM Mapping Practices: Bounding Large Crisis Datasets for Qualitative Investigation},
year = {2016},
isbn = {9781450333627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2858036.2858371},
abstract = {OpenStreetMap (OSM) is the most widely used volunteer geographic information system.
Although it is increasingly relied upon during humanitarian response as the most up-to-date,
accurate, or accessible map of affected areas, the behavior of the mappers who contribute
to it is not well understood. In this paper, we explore the work practices and interactions
of volunteer mappers operating in the high-tempo, high-volume context of disasters.
To do this, we built upon and expanded prior network analysis techniques to select
high-value portions of the vast OSM data for further qualitative analysis. We then
performed detailed content analysis of the identified activity and, where possible,
conducted interviews with the participants. This research allowed the identification
of seven distinct mapping practices that can be classified according to dimensions
of time, space, and interpersonal interaction. Our work represents a baseline for
future research about how OSM crisis mapping practices have evolved over time.},
booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
pages = {2783–2795},
numpages = {13}
}

@article{10.1145/2843889,
author = {Singh, Sukhpal and Chana, Inderveer},
title = {QoS-Aware Autonomic Resource Management in Cloud Computing: A Systematic Review},
year = {2015},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/2843889},
doi = {10.1145/2843889},
abstract = {As computing infrastructure expands, resource management in a large, heterogeneous,
and distributed environment becomes a challenging task. In a cloud environment, with
uncertainty and dispersion of resources, one encounters problems of allocation of
resources, which is caused by things such as heterogeneity, dynamism, and failures.
Unfortunately, existing resource management techniques, frameworks, and mechanisms
are insufficient to handle these environments, applications, and resource behaviors.
To provide efficient performance of workloads and applications, the aforementioned
characteristics should be addressed effectively. This research depicts a broad methodical
literature analysis of autonomic resource management in the area of the cloud in general
and QoS (Quality of Service)-aware autonomic resource management specifically. The
current status of autonomic resource management in cloud computing is distributed
into various categories. Methodical analysis of autonomic resource management in cloud
computing and its techniques are described as developed by various industry and academic
groups. Further, taxonomy of autonomic resource management in the cloud has been presented.
This research work will help researchers find the important characteristics of autonomic
resource management and will also help to select the most suitable technique for autonomic
resource management in a specific application along with significant future research
directions.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {42},
numpages = {46},
keywords = {resource management, self-protecting, service-level agreement, self-optimizing, autonomic cloud computing, cloud computing, autonomic management, self-configuring, resource scheduling, quality of service, grid computing, self-management, self-healing, autonomic computing, Resource provisioning}
}

@inproceedings{10.1145/2247596.2247598,
author = {Borkar, Vinayak and Carey, Michael J. and Li, Chen},
title = {Inside "Big Data Management": Ogres, Onions, or Parfaits?},
year = {2012},
isbn = {9781450307901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2247596.2247598},
doi = {10.1145/2247596.2247598},
abstract = {In this paper we review the history of systems for managing "Big Data" as well as
today's activities and architectures from the (perhaps biased) perspective of three
"database guys" who have been watching this space for a number of years and are currently
working together on "Big Data" problems. Our focus is on architectural issues, and
particularly on the components and layers that have been developed recently (in open
source and elsewhere) and on how they are being used (or abused) to tackle challenges
posed by today's notion of "Big Data". Also covered is the approach we are taking
in the ASTERIX project at UC Irvine, where we are developing our own set of answers
to the questions of the "right" components and the "right" set of layers for taming
the "Big Data" beast. We close by sharing our opinions on what some of the important
open questions are in this area as well as our thoughts on how the dataintensive computing
community might best seek out answers.},
booktitle = {Proceedings of the 15th International Conference on Extending Database Technology},
pages = {3–14},
numpages = {12},
location = {Berlin, Germany},
series = {EDBT '12}
}

@inproceedings{10.1145/3293883.3295706,
author = {Awad, Muhammad A. and Ashkiani, Saman and Johnson, Rob and Farach-Colton, Mart\'{\i}n and Owens, John D.},
title = {Engineering a High-Performance GPU B-Tree},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3295706},
doi = {10.1145/3293883.3295706},
abstract = {We engineer a GPU implementation of a B-Tree that supports concurrent queries (point,
range, and successor) and updates (insertions and deletions). Our B-tree outperforms
the state of the art, a GPU log-structured merge tree (LSM) and a GPU sorted array.
In particular, point and range queries are significantly faster than in a GPU LSM
(the GPU LSM does not implement successor queries). Furthermore, B-Tree insertions
are also faster than LSM and sorted array insertions unless insertions come in batches
of more than roughly 100k. Because we cache the upper levels of the tree, we achieve
lookup throughput that exceeds the DRAM bandwidth of the GPU. We demonstrate that
the key limiter of performance on a GPU is contention and describe the design choices
that allow us to achieve this high performance.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {145–157},
numpages = {13},
keywords = {B-tree, data structures, mutable, GPU, dynamic},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

@article{10.1145/3161200,
author = {Zhang, Shuo and Alanezi, Khaled and Gartrell, Mike and Han, Richard and Lv, Qin and Mishra, Shivakant},
title = {Understanding Group Event Scheduling via the OutWithFriendz Mobile Application},
year = {2018},
issue_date = {December 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
url = {https://doi.org/10.1145/3161200},
doi = {10.1145/3161200},
abstract = {The wide adoption of smartphones and mobile applications has brought significant changes
to not only how individuals behave in the real world, but also how groups of users
interact with each other when organizing group events. Understanding how users make
event decisions as a group and identifying the contributing factors can offer important
insights for social group studies and more effective system and application design
for group event scheduling.In this work, we have designed a new mobile application
called OutWithFriendz, which enables users of our mobile app to organize group events,
invite friends, suggest and vote on event time and venue. We have deployed OutWithFriendz
at both Apple App Store and Google Play, and conducted a large-scale user study spanning
over 500 users and 300 group events. Our analysis has revealed several important observations
regarding group event planning process including the importance of user mobility,
individual preferences, host preferences, and group voting process.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = jan,
articleno = {175},
numpages = {19},
keywords = {Group Decision Making, Group Event Scheduling, OutWithFriendz, Collaboration}
}

@inproceedings{10.1145/2635868.2635901,
author = {Allamanis, Miltiadis and Sutton, Charles},
title = {Mining Idioms from Source Code},
year = {2014},
isbn = {9781450330565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2635868.2635901},
doi = {10.1145/2635868.2635901},
abstract = { We present the first method for automatically mining code idioms from a corpus of
previously written, idiomatic software projects. We take the view that a code idiom
is a syntactic fragment that recurs across projects and has a single semantic purpose.
Idioms may have metavariables, such as the body of a for loop. Modern IDEs commonly
provide facilities for manually defining idioms and inserting them on demand, but
this does not help programmers to write idiomatic code in languages or using libraries
with which they are unfamiliar. We present Haggis, a system for mining code idioms
that builds on recent advanced techniques from statistical natural language processing,
namely, nonparametric Bayesian probabilistic tree substitution grammars. We apply
Haggis to several of the most popular open source projects from GitHub. We present
a wide range of evidence that the resulting idioms are semantically meaningful, demonstrating
that they do indeed recur across software projects and that they occur more frequently
in illustrative code examples collected from a Q&amp;A site. Manual examination of the
most common idioms indicate that they describe important program concepts, including
object creation, exception handling, and resource management. },
booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {472–483},
numpages = {12},
keywords = {naturalness of source code, code idioms, syntactic code patterns},
location = {Hong Kong, China},
series = {FSE 2014}
}

@article{10.1145/3204947,
author = {Siow, Eugene and Tiropanis, Thanassis and Hall, Wendy},
title = {Analytics for the Internet of Things: A Survey},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3204947},
doi = {10.1145/3204947},
abstract = {The Internet of Things (IoT) envisions a world-wide, interconnected network of smart
physical entities. These physical entities generate a large amount of data in operation,
and as the IoT gains momentum in terms of deployment, the combined scale of those
data seems destined to continue to grow. Increasingly, applications for the IoT involve
analytics. Data analytics is the process of deriving knowledge from data, generating
value like actionable insights from them. This article reviews work in the IoT and
big data analytics from the perspective of their utility in creating efficient, effective,
and innovative applications and services for a wide spectrum of domains. We review
the broad vision for the IoT as it is shaped in various communities, examine the application
of data analytics across IoT domains, provide a categorisation of analytic approaches,
and propose a layered taxonomy from IoT data to analytics. This taxonomy provides
us with insights on the appropriateness of analytical techniques, which in turn shapes
a survey of enabling technology and infrastructure for IoT analytics. Finally, we
look at some tradeoffs for analytics in the IoT that can shape future research.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {74},
numpages = {36},
keywords = {big data, cyber-physical networks, Internet of things, data analytics}
}

@proceedings{10.1145/2723372,
title = {SIGMOD '15: Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to SIGMOD 2015 -- officially, the 2015 ACM SIGMOD International Conference
on the Management of Data! This year's conference is being held in the beautiful cultural
capital of Australia, Melbourne. During the Gold Rush period of the 19th Century,
Melbourne was the richest city in the world, and as a result it is filled with many
unique neighborhoods and distinctive buildings. In addition to wonderful neighborhoods
to explore, the city has great museums and other cultural attractions, as well as
a fine multi-cultural atmosphere. For those who would like to explore the outdoors,
popular highlights are the Phillip Island Nature Park (90 minutes away), which features
wild penguins who return in a parade each day at sunset, and the Great Ocean Road,
one of the world's most scenic coastal drives, including the famous towering 12 Apostles.SIGMOD
2015's exciting technical program reflects not only traditional topics, but the database
community's role in broader data science and data analytics. The keynote from Laura
Haas, "The Power Behind the Throne: Information Integration in the Age of Data-Driven
Discovery" highlights the role of database and data integration techniques in the
growing field of data science. Jignesh Patel's talk, "From Data to Insights @ Bare
Metal Speed," explains how hardware and software need to be co-evolved to support
the needs of scalable data analytics. Jennifer Widom, winner of the 2015 ACM-W Athena
Lecturer Award for fundamental contributions to computer science, will give her award
talk, "Three Favorite Results," on Tuesday. Christopher R\'{e} will lead a panel on "Machine
Learning and Databases: The Sound of Things to Come or a Cacophony of Hype?," with
participants Divyakant Agrawal, Magdalena Balazinska, Michael Cafarella, Michael Jordan,
Tim Kraska, and Raghu Ramakrishnan. Of course, there are also 106 research paper presentations,
4 tutorials, 30 demonstrations, and 18 industrial papers. Papers will be presented
both as talks during the research sessions, and as part of plenary Poster Sessions.SIGMOD
2015 is preceded by the PhD Workshop, as well as workshops on leading-edge topics
like data analytics (DanaC), databases and the Web (WebDB), exploratory search (ExploreDB),
managing and mining spatial data (GeoRich), and graph data (GRADES); the New Researcher
Symposium will take place on Wednesday. The banquet will be held in a Melbourne landmark,
the Town Hall.As in recent years, we had two submission deadlines for SIGMOD this
year, one in August and one in November. The review process was journal-style, with
multiple rounds of reviews coordinated by the Group Leaders. We accepted 34 of 137
papers from the first deadline and 72 of 278 from the second deadline. The total acceptance
rate was about 25.5%, and we believe that the revision processhas improved the quality
of the technical program.},
location = {Melbourne, Victoria, Australia}
}

@book{10.1145/3226595,
editor = {Brodie, Michael L.},
title = {Making Databases Work: The Pragmatic Wisdom of Michael Stonebraker},
year = {2018},
isbn = {9781947487192},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
volume = {22},
abstract = {At the ACM Awards banquet in June 2017, during the 50th anniversary celebration of
the A.M. Turing Award, ACM announced the launch of the ACM A.M. Turing Book Series,
a sub-series of ACM Books, to celebrate the winners of the A.M. Turing Award, computing's
highest honor, the "Nobel Prize" for computing. This series aims to highlight the
accomplishments of awardees, explaining their major contributions of lasting importance
in computing."Making Databases Work: The Pragmatic Wisdom of Michael Stonebraker,"
the first book in the series, celebrates Mike's contributions and impact. What accomplishments
warranted computing's highest honor? How did Stonebraker do it? Who is Mike Stonebraker---researcher,
professor, CTO, lecturer, innovative product developer, serial entrepreneur, and decades-long
leader, and research evangelist for the database community. This book describes Mike's
many contributions and evaluates them in light of the Turing Award.The book describes,
in 36 chapters, the unique nature, significance, and impact of Mike's achievements
in advancing modern database systems over more than 40 years. The stories involve
technical concepts, projects, people, prototype systems, failures, lucky accidents,
crazy risks, startups, products, venture capital, and lots of applications that drove
Mike Stonebraker's achievements and career. Even if you have no interest in databases
at all, you'll gain insights into the birth and evolution of Turing Award-worthy achievements
from the perspectives of 39 remarkable computer scientists and professionals.Today,
data is considered the world's most valuable resource ("The Economist," May 6, 2017),
whether it is in the tens of millions of databases used to manage the world's businesses
and governments, in the billions of databases in our smartphones and watches, or residing
elsewhere, as yet unmanaged, awaiting the elusive next generation of database systems.
Every one of the millions or billions of databases includes features that are celebrated
by the 2014 A.M. Turing Award and are described in this book.}
}

@article{10.1145/3170432,
author = {Dayarathna, Miyuru and Perera, Srinath},
title = {Recent Advancements in Event Processing},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3170432},
doi = {10.1145/3170432},
abstract = {Event processing (EP) is a data processing technology that conducts online processing
of event information. In this survey, we summarize the latest cutting-edge work done
on EP from both industrial and academic research community viewpoints. We divide the
entire field of EP into three subareas: EP system architectures, EP use cases, and
EP open research topics. Then we deep dive into the details of each subsection. We
investigate the system architecture characteristics of novel EP platforms, such as
Apache Storm, Apache Spark, and Apache Flink. We found significant advancements made
on novel application areas, such as the Internet of Things; streaming machine learning
(ML); and processing of complex data types such as text, video data streams, and graphs.
Furthermore, there has been significant body of contributions made on event ordering,
system scalability, development of EP languages and exploration of use of heterogeneous
devices for EP, which we investigate in the latter half of this article. Through our
study, we found key areas that require significant attention from the EP community,
such as Streaming ML, EP system benchmarking, and graph stream processing.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {33},
numpages = {36},
keywords = {data stream processing, Event processing, complex event processing}
}

@proceedings{10.1145/3308560,
title = {WWW '19: Companion Proceedings of The 2019 World Wide Web Conference},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to <i>The Web Conference 2019</i>. The Web
Conference is the premier venue focused on understanding the current state and the
evolution of the Web through the lens of computer science, computational social science,
economics, policy, and many other disciplines. The 2019 edition of the conference
is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, USA}
}

@inproceedings{10.1145/3437800.3439203,
author = {Raj, Rajendra K. and Romanowski, Carol J. and Impagliazzo, John and Aly, Sherif G. and Becker, Brett A. and Chen, Juan and Ghafoor, Sheikh and Giacaman, Nasser and Gordon, Steven I. and Izu, Cruz and Rahimi, Shahram and Robson, Michael P. and Thota, Neena},
title = {High Performance Computing Education: Current Challenges and Future Directions},
year = {2020},
isbn = {9781450382939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437800.3439203},
doi = {10.1145/3437800.3439203},
abstract = {High Performance Computing (HPC) is the ability to process data and perform complex
calculations at extremely high speeds. Current HPC platforms can achieve calculations
on the order of quadrillions of calculations per second, with quintillions on the
horizon. The past three decades witnessed a vast increase in the use of HPC across
different scientific, engineering, and business communities on problems such as sequencing
the genome, predicting climate changes, designing modern aerodynamics, or establishing
customer preferences. Although HPC has been well incorporated into science curricula
such as bioinformatics, the same cannot be said for most computing programs. Computing
educators are only now beginning to recognize the need for HPC Education (HPCEd).
Building on earlier work, this working group explored how HPCEd can make inroads into
computing education, focusing on the undergraduate level. This paper presents the
background of HPC and HPCEd, identifies several of the needed core HPC competencies
for students, identifies the support needed by educators for HPCEd, and explores the
symbiosis between HPCEd and computing education in contemporary areas such as artificial
intelligence and data science, as well as how HPCEd can be applied to benefit diverse
non-computing domains such as atmospheric science, biological sciences and critical
infrastructure protection. Finally, the report makes several recommendations to improve
and facilitate HPC education in the future.},
booktitle = {Proceedings of the Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {51–74},
numpages = {24},
keywords = {contemporary computing education, hpc education, high performance computing, computer science education, iticse working group, high-performance computing curricula},
location = {Trondheim, Norway},
series = {ITiCSE-WGR '20}
}

@article{10.1145/3309545,
author = {Habibzadeh, Hadi and Kaptan, Cem and Soyata, Tolga and Kantarci, Burak and Boukerche, Azzedine},
title = {Smart City System Design: A Comprehensive Study of the Application and Data Planes},
year = {2019},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3309545},
doi = {10.1145/3309545},
abstract = {Recent global smart city efforts resemble the establishment of electricity networks
when electricity was first invented, which meant the start of a new era to sell electricity
as a utility. A century later, in the smart era, the network to deliver services goes
far beyond a single entity like electricity. Supplemented by a well-established Internet
infrastructure that can run an endless number of applications, abundant processing
and storage capabilities of clouds, resilient edge computing, and sophisticated data
analysis like machine learning and deep learning, an already-booming Internet of Things
movement makes this new era far more exciting.In this article, we present a multi-faceted
survey of machine intelligence in modern implementations. We partition smart city
infrastructure into application, sensing, communication, security, and data planes
and put an emphasis on the data plane as the mainstay of computing and data storage.
We investigate (i) a centralized and distributed implementation of data plane’s physical
infrastructure and (ii) a complementary application of data analytics, machine learning,
deep learning, and data visualization to implement robust machine intelligence in
a smart city software core. We finalize our article with pointers to open issues and
challenges.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {41},
numpages = {38},
keywords = {big data, supervised learning, cybersecurity, unsupervised learning, diagnostic, Crowdsensing, deep learning, predictive, and prescriptive analytics, smart sustainable cities, data science, edge-computing, mobile computing}
}

@proceedings{10.1145/2998181,
title = {CSCW '17: Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to CSCW 2017, the ACM 2017 Conference on Computer Supported Cooperative Work
and Social Computing! We are excited to welcome the CSCW community back to Portland,
Oregon, where the second CSCW conference was held in 1988. Both Portland and CSCW
have matured a great deal during the intervening 29 years. We hope that you will find
that Portland provides a stimulating environment for our conference.CSCW is the premier
venue for presenting research in the design and use of technologies that affect groups,
organizations, communities, and networks. Bringing together top researchers and practitioners
from academia and industry, CSCW explores the technical, social, material, and theoretical
challenges of designing technology to support collaborative work and life activities.
CSCW welcomes a diverse range of topics and research methodologies. Studies often
involve the development and application of novel technologies and/or ethnographic
studies that inform design practice or theory. The mission of the conference is to
share research that advances the state of human knowledge and improves both the design
of systems and the ways they are used. The diversity of work in our conference program
reflects the diversity of technology use in people's work, social, and civic lives
as well as the geographic and cultural diversity of contributors.As many of you know,
CSCW follows a rigorous "revise and resubmit" review process that uses peer review
to improve submitted papers while maintaining a high-quality threshold for final acceptance.
We also help prepare the next generation of reviewers with a mentorship program in
which students review papers under the guidance of an experienced reviewer. This year
we have the largest CSCW program ever. We had 530 submitted papers and 183 were accepted
for presentation at the conference. The program also includes 4 papers published in
ACM Transactions on Human- Computer Interaction (TOCHI). In addition, we will feature
14 workshops, 56 posters, 12 demos, and 3 panels.Lili Cheng of Microsoft Research
will open the conference, speaking on "Conversational AI &amp; Lessons Learned." Our closing
plenary will feature Jorge Cham, the creator of PhD Comics, who will talk about, "The
Science Gap." We also welcome Paul Luff and Christian Heath from King's College as
the recipients of this year's CSCW Lasting Impact award for their influential 1998
paper, "Mobility in Collaboration."},
location = {Portland, Oregon, USA}
}

@proceedings{10.1145/2897839,
title = {SIGGRAPH '16: ACM SIGGRAPH 2016 Talks},
year = {2016},
isbn = {9781450342827},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Talks highlight the latest developments before publication, present ideas that are
still in progress, or showcase how computer graphics and interactive techniques are
actually implemented and used, in graphics production or other fields. Talks take
you behind the scenes and into the minds of SIGGRAPH 2016 creators in all areas of
computer graphics and interactive techniques, including art, design, animation, visual
effects, interactivity, research, and engineering.},
location = {Anaheim, California}
}

